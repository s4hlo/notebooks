{"cells":[{"cell_type":"markdown","id":"8441f52a","metadata":{"id":"8441f52a"},"source":["# Laboratório 1: Iteração de valor"]},{"cell_type":"markdown","id":"9dd0b12a","metadata":{"id":"9dd0b12a"},"source":["## Importações"]},{"cell_type":"code","execution_count":1,"id":"4786f23b","metadata":{"executionInfo":{"elapsed":1868,"status":"ok","timestamp":1756855224658,"user":{"displayName":"Leonardo Brito","userId":"11465166609065312365"},"user_tz":180},"id":"4786f23b"},"outputs":[],"source":["# Importações\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from matplotlib.colors import ListedColormap\n","import seaborn as sns\n","from typing import Dict, Tuple, List, Union, Optional, Set"]},{"cell_type":"markdown","id":"7f85d886","metadata":{"id":"7f85d886"},"source":["## Ambiente: Navegação no Labirinto (gridworld)"]},{"cell_type":"code","execution_count":2,"id":"2766bd79","metadata":{"executionInfo":{"elapsed":165,"status":"ok","timestamp":1756855224827,"user":{"displayName":"Leonardo Brito","userId":"11465166609065312365"},"user_tz":180},"id":"2766bd79"},"outputs":[],"source":["# Ambiente: Navegação no Labirinto (gridworld)\n","class AmbienteNavegacaoLabirinto:\n","    \"\"\"\n","    Navegação no Labirinto (gridworld) determinístico para experimentos de aprendizado por reforço.\n","\n","    Política de recompensa:\n","    - A recompensa é calculada com base na tentativa de movimento (s, a -> posição proposta), antes de aplicar rebote por sair da grade ou por entrada proibida em bad_states.\n","    - Se a tentativa mira fora da grade: r_boundary\n","    - Se mira um bad_state: r_bad\n","    - Se mira um target_state: r_target\n","    - Caso contrário: r_other\n","\n","    Terminação:\n","    - Por padrão, o ambiente não encerra episódios automaticamente (target_states/bad_states não são estados terminais).\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        world_size: Tuple[int, int],\n","        bad_states: List[Tuple[int, int]],\n","        target_states: List[Tuple[int, int]],\n","        allow_bad_entry: bool = False,\n","        rewards: Optional[List[float]] = None\n","    ) -> None:\n","        \"\"\"\n","        Inicializa o ambiente de navegação em labirinto.\n","\n","        Parâmetros\n","        ----------\n","        world_size : (n_linhas, n_colunas)\n","            Dimensões da grade.\n","        bad_states : lista de (linha, coluna)\n","            Coordenadas dos estados com penalidade.\n","        target_states : lista de (linha, coluna)\n","            Coordenadas dos estados-alvo.\n","        allow_bad_entry : bool\n","            Se False, impede entrada em bad_states (rebote para o estado atual).\n","        rewards : [r_boundary, r_bad, r_target, r_other]\n","            Recompensas possíveis. Se None, usa [-1, -1, 1, 0].\n","        \"\"\"\n","        if rewards is None:\n","            rewards = [-1, -1, 1, 0]\n","\n","        self.n_rows, self.n_cols = world_size   # dimensões da grade do labirinto\n","        self.bad_states = set(bad_states)       # estados com penalidade alta\n","        self.target_states = set(target_states) # estados com recompensa alta\n","        self.allow_bad_entry = allow_bad_entry  # se o agente pode entrar em estados ruins\n","\n","        # Validações\n","        for st in self.bad_states | self.target_states:\n","            if not (0 <= st[0] < self.n_rows and 0 <= st[1] < self.n_cols):\n","                raise ValueError(f\"Estado {st} fora dos limites.\")\n","        if self.bad_states & self.target_states:\n","            raise ValueError(\"bad_states e target_states devem ser disjuntos.\")\n","\n","        # Recompensas definidas para cada tipo de transição\n","        self.r_boundary = rewards[0]    # tentar sair da grade\n","        self.r_bad      = rewards[1]    # transição para estado ruim\n","        self.r_target   = rewards[2]    # transição para estado alvo\n","        self.r_other    = rewards[3]    # demais transições\n","\n","        # Espaço de ações: dicionário com deslocamentos (linha, coluna)\n","        self.action_space = {\n","            0: (-1, 0),  # cima\n","            1: (1, 0),   # baixo\n","            2: (0, -1),  # esquerda\n","            3: (0, 1),   # direita\n","            4: (0, 0)    # permanecer no mesmo estado\n","        }\n","\n","        # Espaço de recompensas: lista de recompensas possíveis\n","        self.recompensas_possiveis = np.array(sorted(set(rewards)))\n","        self.reward_map = {r: i for i, r in enumerate(self.recompensas_possiveis)}\n","\n","        # número total de estados\n","        self.n_states = self.n_rows * self.n_cols\n","\n","        # número total de ações\n","        self.n_actions = len(self.action_space)\n","\n","        # número total de recompensas possíveis\n","        self.n_rewards = self.recompensas_possiveis.shape[0]\n","\n","        # Tensor de probabilidades de transição: P(s'|s,a)\n","        self.state_transition_probabilities = np.zeros((self.n_states, self.n_states, self.n_actions))\n","\n","        # Tensor de probabilidade de recompensas: P(r|s,a)\n","        self.reward_probabilities = np.zeros((self.n_rewards, self.n_states, self.n_actions))\n","\n","        # Matriz de recompensa imediata (determinística): recompensa[s, a] = r\n","        self.recompensas_imediatas = np.zeros((self.n_states, self.n_actions))\n","\n","        # Matriz de transição de estados (determinística): transicao[s, a] = s'\n","        self.transicao_de_estados = np.zeros((self.n_states, self.n_actions), dtype=int)\n","\n","        self.agent_pos = (0, 0)  # posição inicial do agente\n","\n","        self._init_dynamics()  # inicializa as dinâmicas de transição e recompensa\n","\n","\n","    def __repr__(self) -> str:\n","        return (f\"AmbienteNavegacaoLabirinto({self.n_rows}x{self.n_cols}, \"\n","                f\"bad={len(self.bad_states)}, target={len(self.target_states)}, \"\n","                f\"allow_bad_entry={self.allow_bad_entry}, agent_pos={self.agent_pos})\")\n","\n","\n","    def __str__(self) -> str:\n","        return self.render(as_string=True)\n","\n","\n","    def reset(self) -> Tuple[int, int]:\n","        \"\"\"\n","        Reinicia a posição do agente para o estado inicial (0, 0).\n","\n","        Retorna\n","        -------\n","        (linha, coluna) : posição inicial do agente.\n","        \"\"\"\n","        self.agent_pos = (0, 0)\n","        return self.agent_pos\n","\n","\n","    def step(self, acao: int, *, linear: bool = False) -> Tuple[Union[int, Tuple[int, int]], float]:\n","        \"\"\"\n","        Executa uma ação, atualiza a posição do agente e devolve o próximo estado.\n","\n","        Parâmetros\n","        ----------\n","        acao : int\n","            Índice da ação (0-4).\n","        linear : bool, keyword-only (default=False)\n","            Se True, retorna o estado como índice linear {0,...,self.n_states-1}; caso contrário, retorna tupla (linha, coluna).\n","\n","        Retorna\n","        -------\n","        proximo_estado : int | (int, int)\n","            Estado observado após a transição (com possível rebote).\n","        recompensa : float\n","            Recompensa imediata recebida.\n","        \"\"\"\n","        estado_atual = self.agent_pos                                           # armazena o estado atual do agente\n","        proposta = self._proposta(estado_atual, acao)                           # calcula a posição proposta pela ação\n","        recompensa = self._compute_reward(proposta)                             # avalia a recompensa com base na tentativa\n","        destino = self._destino_final(estado_atual, acao)                       # aplica regras e obtém o destino após possíveis rebotes\n","        self.agent_pos = destino                                                # atualiza a posição interna do agente\n","        proximo_estado = self.state_to_index(destino) if linear else destino    # escolhe o formato da observação de saída\n","        return proximo_estado, recompensa                                       # retorna observação e recompensa\n","\n","\n","    def reset_to_state(self, estado: Union[Tuple[int, int], int], verificar_validade_estado: bool = True) -> Tuple[int, int]:\n","        \"\"\"\n","        Teleporta o agente para `estado` sem reiniciar o episódio completo.\n","\n","        Parâmetros\n","        ----------\n","        estado : (linha, coluna) | int\n","            Tupla de coordenadas (linha, coluna) ou índice linear (int).\n","        verificar_validade_estado : bool\n","            Se True, lança ValueError se o estado for inválido.\n","\n","        Retorna\n","        -------\n","        (linha, coluna) : nova posição do agente.\n","        \"\"\"\n","        # Converte índice -> tupla, se necessário\n","        if isinstance(estado, int):\n","            estado = self.index_to_state(estado)\n","\n","        if verificar_validade_estado and not self._in_bounds(estado):\n","            raise ValueError(f\"Estado {estado} fora dos limites do labirinto.\")\n","\n","        self.agent_pos = tuple(estado)      # mantém tupla imutável\n","\n","        return self.agent_pos\n","\n","\n","    def is_bad(self, state: Union[int, Tuple[int, int]]) -> bool:\n","        \"\"\"Retorna True se o estado for um bad_state.\"\"\"\n","        if isinstance(state, int):\n","            state = self.index_to_state(state)\n","        return state in self.bad_states\n","\n","\n","    def is_target(self, state: Union[int, Tuple[int, int]]) -> bool:\n","        \"\"\"Retorna True se o estado for um target_state.\"\"\"\n","        if isinstance(state, int):\n","            state = self.index_to_state(state)\n","        return state in self.target_states\n","\n","\n","    def state_to_index(self, estado: Tuple[int, int]) -> int:\n","        \"\"\"\n","        Converte coordenada (linha, coluna) para índice linear no intervalo [0, n_states-1].\n","        \"\"\"\n","        linha, coluna = estado\n","        return linha * self.n_cols + coluna\n","\n","\n","    def index_to_state(self, indice: int) -> Tuple[int, int]:\n","        \"\"\"\n","        Converte índice linear  no intervalo [0, n_states-1] para coordenada (linha, coluna).\n","        \"\"\"\n","        return divmod(indice, self.n_cols)  # (linha, coluna) = (indice // self.n_cols, indice % self.n_cols)\n","\n","\n","    def enumerate_states(self) -> List[int]:\n","        \"\"\"Retorna a lista de índices lineares de todos os estados [0, ..., n_states - 1].\"\"\"\n","        return list(range(self.n_states))\n","\n","\n","    def enumerate_actions(self) -> List[int]:\n","        \"\"\"Retorna a lista de índices das ações disponíveis [0, ..., n_actions - 1].\"\"\"\n","        return list(self.action_space.keys())\n","\n","\n","    def render(\n","        self,\n","        *,\n","        as_string: bool = True,\n","        show_coords: bool = False,\n","        legend: bool = True,\n","        chars: dict | None = None\n","    ) -> str:\n","        \"\"\"\n","        Renderização ASCII do grid.\n","        - A: agente, B: bad, T: target, .: vazio\n","        - show_coords: mostra índices de linha/coluna\n","        - legend: inclui legenda ao final\n","        - chars: permite customizar símbolos (keys: 'agent','bad','target','empty')\n","        \"\"\"\n","        if chars is None:\n","            chars = {\"agent\": \"A\", \"bad\": \"B\", \"target\": \"T\", \"empty\": \".\"}\n","\n","        linhas = []\n","\n","        # cabeçalho de colunas\n","        if show_coords:\n","            header = \"    \" + \" \".join(f\"{c:2d}\" for c in range(self.n_cols))\n","            linhas.append(header)\n","            linhas.append(\"    \" + \"--\" * self.n_cols)\n","\n","        # monta o grid linha a linha\n","        for r in range(self.n_rows):\n","            row_syms = []\n","            for c in range(self.n_cols):\n","                sym = chars[\"empty\"]\n","                if (r, c) in self.bad_states:\n","                    sym = chars[\"bad\"]\n","                if (r, c) in self.target_states:\n","                    sym = chars[\"target\"]\n","                if self.agent_pos == (r, c):\n","                    sym = chars[\"agent\"]\n","                row_syms.append(sym)\n","\n","            linha_str = \" \".join(row_syms)\n","            if show_coords:\n","                linhas.append(f\"{r:2d} | {linha_str}\")\n","            else:\n","                linhas.append(linha_str)\n","\n","        # legenda\n","        if legend:\n","            linhas.append(\"\")\n","            linhas.append(f\"Legenda: {chars['agent']}=agente, {chars['bad']}=bad, \"\n","                        f\"{chars['target']}=target, {chars['empty']}=vazio\")\n","\n","        out = \"\\n\".join(linhas)\n","        return out if as_string else print(out)\n","\n","\n","    def plot_labirinto(self, ax=None, titulo: str = \"Visualização do Labirinto\", cbar: bool = False):\n","        \"\"\"\n","        Visualiza o labirinto.\n","\n","        Representa:\n","        - Estado neutro: branco\n","        - Estado ruim: vermelho\n","        - Estado alvo: verde\n","\n","        Parâmetros\n","        ----------\n","        ax : matplotlib.axes.Axes, opcional\n","            Eixo onde desenhar. Se None, cria uma nova figura.\n","        titulo : str, opcional\n","            Título do gráfico.\n","        cbar : bool, opcional (default=False)\n","            Exibe (True) ou oculta (False) a barra de cores.\n","\n","        Retorna\n","        -------\n","        ax : matplotlib.axes.Axes\n","            Eixo com o heatmap.\n","        \"\"\"\n","        # Cria matriz com valores padrão (0 = neutro)\n","        matriz = np.zeros((self.n_rows, self.n_cols), dtype=int)\n","\n","        # marca estados\n","        for (r, c) in self.bad_states:\n","            matriz[r, c] = 1   # ruim\n","        for (r, c) in self.target_states:\n","            matriz[r, c] = 2   # alvo\n","\n","        # cores: branco=neutro, vermelho=ruim, verde=alvo\n","        cmap = ListedColormap([\"white\", \"red\", \"green\"])\n","\n","        fig = None\n","        if ax is None:\n","            fig, ax = plt.subplots(figsize=(self.n_cols, self.n_rows))\n","\n","        ax = sns.heatmap(\n","            matriz,\n","            cmap=cmap,\n","            cbar=cbar,\n","            linewidths=0.5,\n","            linecolor=\"gray\",\n","            square=True,\n","            ax=ax\n","        )\n","\n","        # remove ticks/labels\n","        ax.set_xticks([]); ax.set_yticks([])\n","        ax.set_xticklabels([]); ax.set_yticklabels([])\n","\n","        # bordas externas\n","        for side in (\"left\", \"right\", \"top\", \"bottom\"):\n","            ax.spines[side].set_visible(True)\n","            ax.spines[side].set_linewidth(0.5)\n","            ax.spines[side].set_edgecolor(\"gray\")\n","\n","        ax.set_title(titulo)\n","\n","        if fig is not None:\n","            plt.tight_layout()\n","            plt.show()\n","\n","        return\n","\n","\n","    def _init_dynamics(self):\n","        \"\"\"\n","        Preenche as matrizes de transição e recompensa com base na estrutura do ambiente e regras de movimentação.\n","        \"\"\"\n","\n","        self.recompensas_imediatas.fill(0.0)\n","        self.transicao_de_estados.fill(0)\n","        self.state_transition_probabilities.fill(0.0)\n","        self.reward_probabilities.fill(0.0)\n","\n","        for s in self.enumerate_states():                                   # percorre todos os estados (índices lineares)\n","            estado_atual = self.index_to_state(s)                           # converte índice para (linha, coluna)\n","            for a in self.enumerate_actions():                              # percorre todas as ações disponíveis\n","                proposta = self._proposta(estado_atual, a)                  # calcula a posição proposta pela ação\n","                r = self._compute_reward(proposta)                          # avalia a recompensa da tentativa de movimento\n","                destino = self._destino_final(estado_atual, a)              # obtém destino após aplicar regras de rebote\n","                s_next = self.state_to_index(destino)                       # converte destino para índice linear\n","\n","                self.recompensas_imediatas[s, a] = r                        # registra r(s, a) na matriz de recompensas imediatas\n","                self.transicao_de_estados[s, a] = s_next                    # registra T(s, a) = s' na matriz de transições\n","\n","                self.state_transition_probabilities[s_next, s, a] = 1.0     # define P(s'|s, a) = 1 (ambiente determinístico)\n","                self.reward_probabilities[self.reward_map[r], s, a] = 1.0   # define P(r |s, a) = 1 (ambiente determinístico)\n","\n","\n","    def _proposta(self, state: Tuple[int, int], acao: int) -> Tuple[int, int]:\n","        \"\"\"Retorna a posição proposta (antes de qualquer rebote).\"\"\"\n","        dl, dc = self.action_space[acao]\n","        return (state[0] + dl, state[1] + dc)\n","\n","\n","    def _destino_final(self, state: Tuple[int, int], acao: int) -> Tuple[int, int]:\n","        \"\"\"\n","        Aplica as regras de rebote: fora da grade => rebote;\n","        bad_state com allow_bad_entry=False => rebote; caso contrário segue para a proposta.\n","        \"\"\"\n","        proposta = self._proposta(state, acao)\n","        if not self._in_bounds(proposta):\n","            return state\n","        if (not self.allow_bad_entry) and self.is_bad(proposta):\n","            return state\n","        return proposta\n","\n","\n","    def _in_bounds(self, posicao: Tuple[int, int]) -> bool:\n","        \"\"\"\n","        Verifica se uma posição está dentro dos limites do labirinto.\n","\n","        Parâmetros\n","        ----------\n","        posicao : (linha, coluna)\n","\n","        Retorna\n","        -------\n","        bool : True se dentro da grade, False caso contrário.\n","        \"\"\"\n","        linha, coluna = posicao\n","        return 0 <= linha < self.n_rows and 0 <= coluna < self.n_cols\n","\n","\n","    def _compute_reward(self, destino: Tuple[int, int]) -> float:\n","        \"\"\"\n","        Calcula a recompensa da tentativa de transição para 'destino'.\n","\n","        Regras\n","        ------\n","        - Se 'destino' está fora da grade: r_boundary\n","        - Se 'destino' é bad_state: r_bad\n","        - Se 'destino' é target_state: r_target\n","        - Caso contrário: r_other\n","        \"\"\"\n","        if not self._in_bounds(destino):\n","            return self.r_boundary\n","        elif self.is_bad(destino):\n","            return self.r_bad\n","        elif self.is_target(destino):\n","            return self.r_target\n","        else:\n","            return self.r_other"]},{"cell_type":"markdown","id":"541999ef","metadata":{"id":"541999ef"},"source":["## Funções auxiliares para visualização"]},{"cell_type":"code","execution_count":3,"id":"eedf6090","metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1756855224845,"user":{"displayName":"Leonardo Brito","userId":"11465166609065312365"},"user_tz":180},"id":"eedf6090"},"outputs":[],"source":["def _prepare_grid(env, ax=None, draw_cells=True):\n","    \"\"\"\n","    Configura o grid. Se 'ax' não for passado, cria 'fig, ax'; caso contrário retorna 'fig=None, ax'.\n","    \"\"\"\n","    fig = None\n","    if ax is None:\n","        fig, ax = plt.subplots(figsize=(env.n_cols, env.n_rows))\n","\n","    ax.set_xlim(0, env.n_cols)\n","    ax.set_ylim(0, env.n_rows)\n","    ax.set_xticks(np.arange(0, env.n_cols + 1, 1))\n","    ax.set_yticks(np.arange(0, env.n_rows + 1, 1))\n","    ax.grid(True)\n","    ax.set_aspect('equal')\n","    ax.invert_yaxis()\n","\n","    if draw_cells:\n","        for r in range(env.n_rows):\n","            for c in range(env.n_cols):\n","                cell = (r, c)\n","                if cell in env.bad_states:\n","                    color = 'red'\n","                elif cell in env.target_states:\n","                    color = 'green'\n","                else:\n","                    color = 'white'\n","                rect = patches.Rectangle((c, r), 1, 1, facecolor=color, edgecolor='gray')\n","                ax.add_patch(rect)\n","\n","    return fig, ax\n","\n","def _coerce_policy(env, policy):\n","    \"\"\"\n","    Normaliza a política para o formato dict[(r,c)] -> ação (int).\n","    Aceita:\n","      - dict[(r,c)] -> ação\n","      - dict[(r,c)] -> vetor de probabilidades\n","      - Pi ndarray (n_estados, n_acoes)\n","    \"\"\"\n","    # caso 1: matriz Pi (ndarray)\n","    if isinstance(policy, np.ndarray):\n","        a_star = np.argmax(policy, axis=1)\n","        return {env.index_to_state(s): int(a_star[s]) for s in range(env.n_states)}\n","\n","    # caso 2: dicionário\n","    sample_val = next(iter(policy.values()))\n","    if isinstance(sample_val, np.ndarray):\n","        return {pos: int(np.argmax(probs)) for pos, probs in policy.items()}\n","    else:\n","        return policy\n","\n","def plot_policy(env, policy, ax=None, titulo=\"Política\"):\n","    \"\"\"\n","    Desenha setas/círculos de uma política. 'policy' pode ser:\n","      - dict[(r,c)] -> ação\n","      - dict[(r,c)] -> vetor de probabilidades\n","      - ndarray com shape (n_estados, n_acoes)\n","    \"\"\"\n","    fig, ax = _prepare_grid(env, ax=ax)\n","\n","    policy_dict = _coerce_policy(env, policy)\n","    color = 'black'\n","    lw = 1.5\n","\n","    for (r, c), action in policy_dict.items():\n","        x, y = c + 0.5, r + 0.5\n","        if action == 0:      # cima\n","            ax.arrow(x, y, dx=0, dy=-0.3, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n","        elif action == 1:    # baixo\n","            ax.arrow(x, y, dx=0, dy=0.3, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n","        elif action == 2:    # esquerda\n","            ax.arrow(x, y, dx=-0.3, dy=0, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n","        elif action == 3:    # direita\n","            ax.arrow(x, y, dx=0.3, dy=0, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n","        elif action == 4:    # ficar\n","            circ = patches.Circle((x, y), 0.1, edgecolor=color, facecolor='none', linewidth=lw)\n","            ax.add_patch(circ)\n","\n","    ax.set_title(titulo)\n","    if fig is not None:\n","        plt.tight_layout()\n","        plt.show()\n","    return ax\n","\n","def plot_tabular(\n","    data,\n","    kind: str = \"Q\",          # \"Q\" (valores de ação), \"Pi\" (política), \"V\" (valores de estado)\n","    ambiente=None,            # necessário quando kind=\"V\" para reshape\n","    ax=None,\n","    cbar: bool = True,\n","    fmt: str = \".1f\",\n","    center_zero: bool = True  # só relevante para \"Q\" e \"V\"\n","):\n","    \"\"\"\n","    Plota matrizes tabulares de RL em formato de heatmaps (mapas de calor).\n","    Esta função cobre 3 casos:\n","    1. kind=\"Q\": heatmap de Q(s, a) com ações nas linhas e estados nas colunas.\n","    2. kind=\"Pi\": heatmap de Pi(a|s) (probabilidades) com ações nas linhas e estados nas colunas.\n","    3. kind=\"V\": heatmap de V(s) no grid (n_rows x n_cols) do ambiente .\n","\n","    Parameters\n","    ----------\n","    data : ndarray\n","        Dados a serem plotados.\n","        - Para kind=\"Q\" ou \"Pi\": array 2D com shape (n_estados, n_acoes).\n","        - Para kind=\"V\": array 1D com shape (n_estados,) que será remodelado para (ambiente.n_rows, ambiente.n_cols).\n","    kind : {\"Q\", \"Pi\", \"V\"}, default=\"Q\"\n","        Tipo do plot:\n","        - \"Q\" usa paleta divergente centrada em zero.\n","        - \"Pi\" usa paleta sequencial no intervalo [0, 1].\n","        - \"V\" plota o valor de estado no grid do ambiente.\n","    ambiente : object, optional\n","        Necessário quando kind=\"V\". Deve expor n_rows e n_cols para o reshape.\n","    ax : matplotlib.axes.Axes, optional\n","        Eixo onde o heatmap será desenhado. Se None, uma nova figura/eixo é criado.\n","    cbar : bool, default=True\n","        Se True, exibe a barra de cores (colorbar).\n","    fmt : str, default=\".1f\"\n","        Formatação dos valores anotados em cada célula do heatmap.\n","    center_zero : bool, default=True\n","        Quando kind é \"Q\" ou \"V\", centraliza a escala de cores em zero (vmin=-absmax, vmax=absmax). Ignorado para \"Pi\".\n","\n","    Returns\n","    -------\n","    ax : matplotlib.axes.Axes\n","        Eixo contendo o heatmap resultante.\n","    \"\"\"\n","    kind = kind.upper()\n","\n","    xlabel = {\"V\": \"Colunas\", \"PI\": \"Estados\", \"Q\": \"Estados\"}\n","    ylabel = {\"V\": \"Linhas\", \"PI\": \"Ações\", \"Q\": \"Ações\" }\n","    title  = {\"V\": \"Valores de Estado (V(s))\", \"PI\": r\"Política ($\\pi(a|s)$ transposta)\", \"Q\": \"Valores de ação (Q(s, a) transposta)\"}\n","\n","    fig = None\n","\n","    #  V(s): precisa do shape do grid\n","    match kind:\n","        case \"V\":\n","\n","            if ambiente is None:\n","                raise ValueError(\"Para kind='V', passe 'ambiente' para reshape (n_rows, n_cols).\")\n","\n","            M = data.reshape(ambiente.n_rows, ambiente.n_cols)\n","\n","            if ax is None:\n","                fig, ax = plt.subplots(figsize=(ambiente.n_cols, ambiente.n_rows))\n","\n","            if center_zero:\n","                vmax = float(np.abs(M).max())\n","                vmin = -vmax\n","            else:\n","                vmin = float(M.min())\n","                vmax = float(M.max())\n","\n","            cmap, square = \"bwr\", True\n","\n","        case \"PI\" | \"Q\":\n","\n","            # Q(s,a) e Pi(a|s): ações nas linhas, estados nas colunas\n","            M = data.T  # data: (n_estados, n_acoes) -> transposto para (n_acoes, n_estados)\n","            n_acoes, n_estados = M.shape\n","\n","            if ax is None:\n","                fig, ax = plt.subplots(figsize=(n_estados, n_acoes))\n","\n","            if kind == \"PI\":\n","                cmap = \"Blues\";\n","                vmin, vmax = 0.0, 1.0\n","            else:  # \"Q\"\n","                cmap = \"bwr\"\n","                if center_zero:\n","                    vmax = float(np.abs(M).max())\n","                    vmin = -vmax\n","                else:\n","                    vmin = float(M.min())\n","                    vmax = float(M.max())\n","\n","            square = False\n","\n","        case _:\n","            raise ValueError(f\"kind desconhecido: {kind!r} (use 'Q', 'Pi' ou 'V').\")\n","\n","\n","    ax = sns.heatmap(\n","        data=M,\n","        annot=True,\n","        fmt=fmt,\n","        cmap=cmap,\n","        vmin=vmin,\n","        vmax=vmax,\n","        cbar=cbar,\n","        square=square,\n","        linewidths=0.5,\n","        linecolor=\"gray\",\n","        ax=ax\n","    )\n","\n","    ax.set_xlabel(xlabel[kind])\n","    ax.set_ylabel(ylabel[kind])\n","    ax.set_title(title[kind])\n","\n","    # bordas externas\n","    for side in (\"left\", \"right\", \"top\", \"bottom\"):\n","        ax.spines[side].set_visible(True)\n","        ax.spines[side].set_linewidth(0.5)\n","        ax.spines[side].set_edgecolor(\"gray\")\n","\n","    # rótulos\n","    if kind in (\"Q\", \"PI\"):\n","        ax.set_xticks(np.arange(n_estados) + 0.5)\n","        ax.set_xticklabels([f\"s{i}\" for i in range(n_estados)], rotation=0)\n","        ax.set_yticks(np.arange(n_acoes) + 0.5)\n","        ax.set_yticklabels([f\"a{i}\" for i in range(n_acoes)], rotation=0)\n","\n","    if fig is not None:\n","        plt.tight_layout()\n","        plt.show()\n","\n","    return"]},{"cell_type":"markdown","id":"81df4148","metadata":{"id":"81df4148"},"source":["## Ambiente: nova instância"]},{"cell_type":"code","execution_count":4,"id":"d196b06c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"elapsed":113,"status":"ok","timestamp":1756855224962,"user":{"displayName":"Leonardo Brito","userId":"11465166609065312365"},"user_tz":180},"id":"d196b06c","outputId":"fca4d707-c677-49e2-b060-cbda0f72fb20"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 500x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["ambiente = AmbienteNavegacaoLabirinto(\n","        world_size=(5, 5),\n","        bad_states=[(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)],\n","        target_states=[(3, 2)],\n","        allow_bad_entry=True,\n","        rewards=[-1, -10, 1, 0]\n","    )\n","ambiente.plot_labirinto()"]},{"cell_type":"markdown","id":"d10ce035","metadata":{"id":"d10ce035"},"source":["## Iteração de valor"]},{"cell_type":"code","execution_count":5,"id":"5ae2f49a","metadata":{"id":"5ae2f49a","executionInfo":{"status":"ok","timestamp":1756855224966,"user_tz":180,"elapsed":2,"user":{"displayName":"Leonardo Brito","userId":"11465166609065312365"}}},"outputs":[],"source":["def iteracao_de_valor(\n","    ambiente: \"AmbienteNavegacaoLabirinto\",\n","    gamma: float = 0.9,\n","    theta: float = 1e-6,\n","    max_iteracoes: int = 1000,\n",") -> Tuple[np.ndarray, np.ndarray, np.ndarray, int]:\n","    \"\"\"\n","    Implementa o algoritmo de Iteração de Valor com atualização síncrona (Jacobi) para encontrar a política ótima.\n","    Usa os modelos probabilísticos p(r|s,a) e p(s'|s,a) fornecidos pelo ambiente.\n","\n","    Parameters\n","    ----------\n","    ambiente : AmbienteNavegacaoLabirinto\n","        Ambiente tabular que expõe:\n","        - n_states (int), n_actions (int)\n","        - state_transition_probabilities : ndarray, shape (n_states', n_states, n_actions)\n","          Probabilidade de transição p(s'|s,a) com eixo 0 = s', 1 = s, 2 = a.\n","        - reward_probabilities : ndarray, shape (n_rewards, n_states, n_actions)\n","          Probabilidade de recompensa p(r|s,a) com eixo 0 = r, 1 = s, 2 = a.\n","        - recompensas_possiveis : ndarray, shape (n_rewards,)\n","          Suporte ordenado dos valores de recompensa.\n","    gamma : float, default=0.9\n","        Fator de desconto. Em horizonte infinito sem estados terminais, recomenda-se 0 <= gamma < 1 para convergência.\n","    theta : float, default=1e-6\n","        Tolerância de convergência na norma infinita aplicada a ||v_{k+1} - v_k||.\n","    max_iteracoes : int, default=1000\n","        Número máximo de iterações.\n","\n","    Returns\n","    -------\n","    V : ndarray, shape (n_states,)\n","        Valor de estado.\n","    Q : ndarray, shape (n_states, n_actions)\n","        Valores de ação calculados na última iteração.\n","    Pi : ndarray, shape (n_states, n_actions)\n","        Política gulosa determinística.\n","    num_iter : int\n","        Número de iterações efetivamente realizadas.\n","    \"\"\"\n","    if not (0.0 <= gamma < 1.0):\n","        raise ValueError(\"Sem estados terminais, use 0 <= gamma < 1 para garantir convergência.\")\n","\n","    # Atalhos (utilize as variáveis que forem necessárias)\n","    n_estados       = ambiente.n_states\n","    n_acoes         = ambiente.n_actions\n","    n_recompensas   = ambiente.n_rewards\n","    R = ambiente.recompensas_imediatas              # r(s,a) -> r\n","    T = ambiente.transicao_de_estados               # T(s,a) -> s'\n","    Ps = ambiente.state_transition_probabilities    # shape (n_estados, n_estados, n_acoes)\n","    Pr = ambiente.reward_probabilities              # shape (n_recompensas,  n_estados, n_acoes)\n","    r_vector  = ambiente.recompensas_possiveis      # shape (n_recompensas,)\n","\n","    # TODO: inicialize V, Q, Pi\n","    for k in range(max_iteracoes):\n","        # TODO: armazenar V_old\n","        # TODO: laços sobre estados e ações\n","        # TODO: atualizar Q, Pi e V\n","        # TODO: critério de parada\n","        pass\n","    # TODO: retorne V, Q, Pi, k+1\n","    raise NotImplementedError"]},{"cell_type":"markdown","id":"c18cd948","metadata":{"id":"c18cd948"},"source":["## Experimento"]},{"cell_type":"markdown","id":"2048d94c","metadata":{"id":"2048d94c"},"source":["### Simulação"]},{"cell_type":"code","execution_count":6,"id":"b04c506b","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":298},"executionInfo":{"elapsed":29,"status":"error","timestamp":1756855224997,"user":{"displayName":"Leonardo Brito","userId":"11465166609065312365"},"user_tz":180},"id":"b04c506b","outputId":"4ed788fd-a7c8-4e56-9596-b2a83d5de1f2"},"outputs":[{"output_type":"error","ename":"NotImplementedError","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1421647119.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m V, Q, Pi, k = iteracao_de_valor(ambiente,           # gridworld\n\u001b[0m\u001b[1;32m      2\u001b[0m                                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m          \u001b[0;31m# fator de desconto (0 <= gamma < 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 \u001b[0mtheta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0;31m# tolerância de convergência (norma infinita ||v_{k+1} - v_k||_inf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 \u001b[0mmax_iteracoes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m  \u001b[0;31m# número máximo de iterações\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 )\n","\u001b[0;32m/tmp/ipython-input-2551238174.py\u001b[0m in \u001b[0;36miteracao_de_valor\u001b[0;34m(ambiente, gamma, theta, max_iteracoes)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# TODO: retorne V, Q, Pi, k+1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m: "]}],"source":["V, Q, Pi, k = iteracao_de_valor(ambiente,           # gridworld\n","                                gamma=0.9,          # fator de desconto (0 <= gamma < 1)\n","                                theta=1e-6,         # tolerância de convergência (norma infinita ||v_{k+1} - v_k||_inf)\n","                                max_iteracoes=1000  # número máximo de iterações\n","                                )"]},{"cell_type":"markdown","id":"67579889","metadata":{"id":"67579889"},"source":["### Visualização"]},{"cell_type":"code","execution_count":null,"id":"51d58a86","metadata":{"id":"51d58a86","executionInfo":{"status":"aborted","timestamp":1756855225120,"user_tz":180,"elapsed":37,"user":{"displayName":"Leonardo Brito","userId":"11465166609065312365"}}},"outputs":[],"source":["# Q: ndarray (n_estados, n_acoes)\n","plot_tabular(Q, kind=\"Q\")\n","\n","# Pi: ndarray (n_estados, n_acoes)\n","plot_tabular(Pi, kind=\"Pi\")\n","\n","# V: ndarray (n_estados,)\n","plot_tabular(V, kind=\"V\", ambiente=ambiente, center_zero=False)\n","\n","# Política (setas) sobre o ambiente\n","_ = plot_policy(ambiente, Pi)"]},{"cell_type":"markdown","id":"20c98c1b","metadata":{"id":"20c98c1b"},"source":["# Tarefa:\n","\n","1. Variação do fator de desconto\n","- Observar e reportar o efeito de diferentes valores da taxa de desconto (por exemplo: $\\gamma \\in \\{\\,0.0,\\ 0.5,\\ 0.9\\,\\}$)\n","2. Penalidade de estados ruins mais branda\n","- Observar e reportar o efeito de trocar $r_{\\text{bad}}=-10$ para $r_{\\text{bad}}=-1$.\n","3. Transformação afim nas recompensas\n","- Observar e reportar o efeito de uma transformação afim ($r' = a\\,r + b$, com $a>0$) em todas as recompensas, isto é, em todos os elementos de $[\\,r_{\\text{boundary}}, r_{\\text{bad}}, r_{\\text{target}}, r_{\\text{other}}\\,]$.\n","\n","**Configuração base (baseline)**\n","\n","- `world_size = (5, 5)`\n","- `target_states = [(3, 2)]`\n","- `bad_states = [(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)]`\n","- `allow_bad_entry = True`\n","- recompensas base: $[\\,r_{\\text{boundary}},\\ r_{\\text{bad}},\\ r_{\\text{target}},\\ r_{\\text{other}}\\,] = [-1,\\ -10,\\ 1,\\ 0]$\n","- tolerância e limite: $\\theta = 10^{-6}$, `max_iteracoes = 1000`\n","\n","> Se alterar qualquer parâmetro do setup, **documente explicitamente** no relatório.\n","\n","**Em todos os experimentos mostrar:**\n","\n","1. **Figuras**:\n","   - heatmap de $V(s)$ no grid $(n_{\\text{rows}}\\times n_{\\text{cols}})$;\n","   - heatmap de $Q(s,a)$ (ações nas linhas, estados nas colunas);\n","   - heatmap de $\\pi(a\\mid s)$ (probabilidades).\n","2. **Convergência**: número de iterações até $\\lVert v_{k+1}-v_k\\rVert_\\infty < \\theta$.\n","3. **Discussão**: texto breve (3–6 linhas) por experimento.\n","\n","**Entregáveis:**\n","\n","2. **Código** (notebook `.ipynb`)\n","1. **Relatório** (`.pdf`).\n","- O PDF deve conter:\n","  - **Setup** (parâmetros usados).\n","  - **Resultados** (figuras e tabelas organizadas por experimento).\n","  - **Análises curtas** por experimento.\n","- O PDF **NÃO** deve conter:\n","    - Códigos."]}],"metadata":{"colab":{"provenance":[{"file_id":"14i1cNr2ZU6wlWBuhPnXVRW4ZnDtHpzLZ","timestamp":1747073070383}],"collapsed_sections":["9dd0b12a","541999ef","d10ce035","c18cd948"]},"kernelspec":{"display_name":"pytorch","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.5"}},"nbformat":4,"nbformat_minor":5}