{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "518b7bbc",
   "metadata": {},
   "source": [
    "# Representação Vetorial de Texto\n",
    "\n",
    "Modelos de aprendizado de máquina, incluindo redes neurais, não operam sobre texto puro. Eles requerem que a entrada de dados seja representada como vetores numéricos. O campo do Processamento de Linguagem Natural (PLN) desenvolveu diversas técnicas para realizar essa conversão, um processo conhecido como *feature extraction* ou vetorização. A qualidade dessa representação vetorial é fundamental para o desempenho do modelo final, pois ela deve capturar as propriedades sintáticas e semânticas da linguagem de forma que o modelo possa generalizar o conhecimento adquirido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5b8d18",
   "metadata": {},
   "source": [
    "# 1. Tokenização\n",
    "\n",
    "Antes que qualquer representação vetorial possa ser criada, o texto bruto deve ser segmentado em suas unidades constituintes. Esse processo é chamado de **tokenização**. As unidades resultantes são chamadas de **tokens**. A escolha da estratégia de tokenização é um passo fundamental que impacta todo o pipeline de PLN.\n",
    "\n",
    "Após a tokenização, o próximo passo é construir um **vocabulário**, que é o conjunto de todos os tokens únicos presentes em um corpus de texto. A finalidade do vocabulário é criar um mapeamento determinístico entre cada token e um índice numérico (um inteiro). Esse mapeamento é a base para todas as formas de representação vetorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659592d3",
   "metadata": {},
   "source": [
    "## Tokenização por Caractere\n",
    "\n",
    "A forma mais granular de tokenização é tratar cada caractere como um token individual.\n",
    "\n",
    "* **Vantagens**: O vocabulário é muito pequeno e controlado (letras, números, símbolos). O modelo nunca encontrará um token \"fora do vocabulário\" (out-of-vocabulary, OOV).\n",
    "* **Desvantagens**: A noção de palavra é perdida no nível de entrada, e o modelo precisa aprender a compor palavras a partir de caracteres. As sequências de entrada tornam-se muito longas, aumentando o custo computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2bfc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = \"IA Generativa.\"\n",
    "\n",
    "# Tokenização por caractere\n",
    "char_tokens = list(text_sample)\n",
    "char_vocab = sorted(list(set(char_tokens)))\n",
    "char_to_ix = {ch: i for i, ch in enumerate(char_vocab)}\n",
    "\n",
    "# Mapeando tokens para inteiros\n",
    "encoded_chars = [char_to_ix[ch] for ch in char_tokens]\n",
    "\n",
    "print(f\"Texto original: '{text_sample}'\")\n",
    "print(f\"Tokens de caractere: {char_tokens}\")\n",
    "print(f\"Vocabulário de caracteres: {char_vocab}\")\n",
    "print(f\"Sequência codificada: {encoded_chars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88450da2",
   "metadata": {},
   "source": [
    "## Tokenização por Palavra\n",
    "\n",
    "Esta é a abordagem mais intuitiva, onde o texto é dividido com base em espaços em branco e pontuação.\n",
    "\n",
    "* **Vantagens**: Preserva a unidade semântica da palavra. É computacionalmente eficiente.\n",
    "* **Desvantagens**: O tamanho do vocabulário pode se tornar extremamente grande. O modelo é incapaz de lidar com palavras que não viu durante o treinamento (problema OOV). Palavras com flexões morfológicas (e.g., \"aprende\", \"aprendendo\", \"aprendeu\") são tratadas como tokens distintos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text_sample = \"IA Generativa é um campo fascinante, talvez o mais importante da inteligência artificial!\"\n",
    "\n",
    "# Regex para encontrar sequências de caracteres alfanuméricos (incluindo acentos) ou pontuações isoladas.\n",
    "word_tokens = re.findall(r'[\\w]+|[.,!?]', text_sample.lower())\n",
    "word_vocab = sorted(list(set(word_tokens)))\n",
    "word_to_ix_map = {word: i for i, word in enumerate(word_vocab)}\n",
    "\n",
    "print(f\"Texto original: '{text_sample}'\")\n",
    "print(f\"Tokens de palavra: {word_tokens}\")\n",
    "print(f\"Vocabulário de palavras: {word_vocab}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd1350",
   "metadata": {},
   "source": [
    "## Tokenização por Subpalavra\n",
    "\n",
    "A tokenização por subpalavra busca um meio-termo, segmentando palavras raras em unidades menores e mais frequentes, enquanto mantém palavras comuns como tokens únicos.\n",
    "\n",
    "* **Vantagens**: Controla o tamanho do vocabulário, elimina o problema de OOV e pode capturar relações morfológicas (e.g., \"generativa\" e \"geração\" podem compartilhar o subtoken \"gera\").\n",
    "* **Desvantagens**: A implementação é mais complexa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a02c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulário hipotético de subpalavras\n",
    "subword_vocab = {\"ia\", \"generativa\", \"aprend\", \"##izado\", \"##endo\", \"supervision\", \"##ado\"}\n",
    "\n",
    "# Palavra a ser tokenizada\n",
    "word = \"aprendizado\"\n",
    "\n",
    "# Lógica de tokenização simplificada\n",
    "tokens = []\n",
    "# Encontra a maior subpalavra do vocabulário que corresponde ao início da palavra\n",
    "if word.startswith(\"aprend\"):\n",
    "    tokens.append(\"aprend\")\n",
    "    remaining_part = word[len(\"aprend\"):] # \"izado\"\n",
    "    # Adiciona o prefixo '##' para buscar a continuação\n",
    "    if \"##\" + remaining_part in subword_vocab:\n",
    "        tokens.append(\"##\" + remaining_part)\n",
    "\n",
    "print(f\"A palavra '{word}' é tokenizada em: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b46082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavra desconhecida\n",
    "word = \"supervisionado\"\n",
    "tokens = []\n",
    "if word.startswith(\"supervision\"):\n",
    "    tokens.append(\"supervision\")\n",
    "    remaining_part = word[len(\"supervision\"):]\n",
    "    if \"##\" + remaining_part in subword_vocab:\n",
    "        tokens.append(\"##\" + remaining_part)\n",
    "        \n",
    "print(f\"A palavra '{word}' é tokenizada em: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a6044f",
   "metadata": {},
   "source": [
    "# 2. One-Hot Encoding\n",
    "\n",
    "Com um vocabulário de tokens definido, a abordagem mais simples de vetorização é o One-Hot Encoding (OHE). Para um vocabulário de tamanho $V$, cada token $w$ com índice $i_w$ é representado por um vetor $v_w$ de dimensão $V$. Formalmente, a j-ésima componente do vetor $v_w$ é dada pela função delta de Kronecker:\n",
    "\n",
    "$$(v_w)_j = \\delta_{i_w, j} = \\begin{cases} 1 & \\text{se } j = i_w \\\\ 0 & \\text{se } j \\neq i_w \\end{cases}$$\n",
    "\n",
    "Por exemplo, considere o vocabulário `{\"o\": 0, \"gato\": 1, \"bebe\": 2}`.\n",
    "\n",
    "* O token \"gato\" tem índice 1. Seu vetor OHE é: `[0, 1, 0]`\n",
    "* O token \"o\" tem índice 0. Seu vetor OHE é: `[1, 0, 0]`\n",
    "\n",
    "A sequência \"o gato\" seria representada por uma lista de vetores: `[[1, 0, 0], [0, 1, 0]]`.\n",
    "\n",
    "A principal limitação é a **ortogonalidade** ($v_i^T v_j = 0$ para $i \\neq j$), que impede o modelo de capturar qualquer noção de similaridade entre os tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e8b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Vocabulário e mapeamento\n",
    "vocab = {\"o\": 0, \"gato\": 1, \"bebe\": 2}\n",
    "vocab_size = len(vocab)\n",
    "word_to_ix = vocab\n",
    "\n",
    "def one_hot_encode(word, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    if word in word_to_ix:\n",
    "        vec[word_to_ix[word]] = 1\n",
    "    return vec\n",
    "\n",
    "# Exemplo\n",
    "word_ohe = one_hot_encode(\"gato\", word_to_ix)\n",
    "print(f\"Vocabulário: {vocab}\")\n",
    "print(f\"Representação One-Hot: {word_ohe}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9b97cb",
   "metadata": {},
   "source": [
    "# 3. Representações Densas: Word Embeddings\n",
    "\n",
    "As representações esparsas como o One-Hot Encoding apresentam duas desvantagens críticas:\n",
    "1.  **A Maldição da Dimensionalidade**: Para um vocabulário realista (dezenas de milhares de palavras), os vetores se tornam extremamente grandes e esparsos, tornando o processamento computacionalmente caro e exigindo uma quantidade massiva de dados para que um modelo possa aprender padrões significativos.\n",
    "2.  **Ausência de Relação Semântica**: A ortogonalidade dos vetores implica que não há uma noção de similaridade. O modelo não consegue inferir que \"cão\" e \"gato\" são mais parecidos entre si do que com a palavra \"algoritmo\".\n",
    "\n",
    "Para superar isso, foram desenvolvidas as **representações densas**, conhecidas como **word embeddings**. A ideia é representar cada palavra por um vetor de baixa dimensão (tipicamente de 50 a 300 dimensões) com valores de ponto flutuante (números reais). Em vez de cada dimensão corresponder a uma palavra específica, as dimensões representam *features latentes* ou atributos do significado da palavra, que são aprendidos automaticamente a partir dos dados.\n",
    "\n",
    "O princípio fundamental por trás do aprendizado desses embeddings é a **Hipótese Distribucional**: \"uma palavra é caracterizada pela companhia que mantém\" (Firth, 1957). Isso significa que palavras que aparecem em contextos textuais similares tendem a ter significados similares. O objetivo, portanto, é aprender uma função de mapeamento $E: w \\rightarrow \\mathbb{R}^d$ (onde $d \\ll |V|$) tal que, se duas palavras $w_1$ e $w_2$ são semanticamente similares, seus vetores $E(w_1)$ e $E(w_2)$ estarão próximos no espaço vetorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0640e12",
   "metadata": {},
   "source": [
    "## O Conceito de Espaço Vetorial Semântico\n",
    "\n",
    "Esse processo de aprendizado cria um **espaço vetorial semântico**, onde a geometria (distância e direção) entre os vetores captura relações de significado. Por exemplo, a relação vetorial entre \"rei\" e \"rainha\" pode ser similar à relação entre \"homem\" e \"mulher\", permitindo raciocínios por analogia como:\n",
    "\n",
    "$$ \\vec{v}_{\\text{rei}} - \\vec{v}_{\\text{homem}} + \\vec{v}_{\\text{mulher}} \\approx \\vec{v}_{\\text{rainha}} $$\n",
    "\n",
    "Modelos como o Word2Vec são projetados para construir esse espaço a partir de grandes volumes de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e4660",
   "metadata": {},
   "source": [
    "## Word2Vec e a Arquitetura Skip-gram\n",
    "\n",
    "Word2Vec é uma família de modelos que aprende esses embeddings de forma eficiente. Vamos focar na sua arquitetura mais popular, o **Skip-gram**.\n",
    "\n",
    "### Objetivo e Estrutura do Skip-gram\n",
    "\n",
    "O objetivo do Skip-gram é, dada uma palavra central $w_c$, prever as palavras que aparecem em sua janela de contexto, $w_o$. O modelo é uma rede neural simples com uma única camada oculta.\n",
    "\n",
    "1.  **Entrada**: A palavra central $w_c$ é representada como um vetor one-hot de tamanho $|V|$.\n",
    "2.  **Matriz de Embedding ($W$)**: Uma matriz de pesos de dimensão $|V| \\times d$. Multiplicar o vetor one-hot da palavra de entrada por esta matriz é equivalente a selecionar a linha correspondente ao índice da palavra, obtendo seu vetor de embedding $v_{w_c}$ (a camada oculta).\n",
    "3.  **Matriz de Saída ($W'$)**: Uma segunda matriz de pesos, de dimensão $d \\times |V|$.\n",
    "4.  **Cálculo dos Scores**: O embedding da palavra central $v_{w_c}$ é multiplicado pela matriz de saída $W'$ para produzir um vetor de scores $u$ de dimensão $|V|$. Cada elemento $u_j$ deste vetor é o produto escalar $u_j = v'_{w_j} \\cdot v_{w_c}$, onde $v'_{w_j}$ é o \"embedding de saída\" da j-ésima palavra do vocabulário. Este score mede a similaridade entre a palavra central $w_c$ e cada outra palavra $w_j$ como um potencial contexto.\n",
    "5.  **Probabilidades de Saída**: Os scores são convertidos em probabilidades usando a função softmax. A probabilidade de uma palavra $w_o$ ser uma palavra de contexto de $w_c$ é dada por:\n",
    "\n",
    "$$P(w_o | w_c) = \\frac{\\exp(v'_{w_o} \\cdot v_{w_c})}{\\sum_{j=1}^{|V|} \\exp(v'_{w_j} \\cdot v_{w_c})}$$\n",
    "\n",
    "### Função Objetivo (Loss)\n",
    "\n",
    "O objetivo do treinamento é ajustar os pesos das matrizes $W$ (os embeddings de entrada) e $W'$ (os embeddings de saída) para maximizar a probabilidade de prever corretamente as palavras de contexto reais. Isso é feito minimizando a função de perda (loss), que é o log-likelihood negativo médio sobre todo o corpus. Para uma única palavra central $w_t$, a perda para seu contexto $C_t$ é:\n",
    "\n",
    "$$L = - \\sum_{w_j \\in C_t} \\log P(w_j | w_t)$$\n",
    "\n",
    "Ao final do treinamento, a matriz $W$ contém os vetores de embedding densos e semanticamente ricos que procuramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524edeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus temático sobre IA\n",
    "corpus_text = \"\"\"\n",
    "o modelo de aprendizado supervisionado utiliza dados rotulados. \n",
    "um algoritmo de aprendizado profundo treina uma rede neural artificial. \n",
    "a rede neural aprende com os dados de entrada. \n",
    "o modelo de clusterizacao é um algoritmo de aprendizado nao supervisionado. \n",
    "o aprendizado por reforco otimiza uma recompensa.\n",
    "o modelo generativo cria novos dados.\n",
    "um bom algoritmo precisa de bons dados para o treinamento.\n",
    "a rede convolucional é um tipo de rede neural para imagens.\n",
    "o treinamento de um modelo profundo exige muito poder computacional.\n",
    "\"\"\"\n",
    "\n",
    "# Pré-processamento e criação do vocabulário\n",
    "corpus_words = re.findall(r'[\\w]+', corpus_text.lower())\n",
    "vocab = sorted(list(set(corpus_words)))\n",
    "vocab_size = len(vocab)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geração de pares de treinamento\n",
    "window_size = 2\n",
    "skipgram_data = []\n",
    "for i in range(window_size, len(corpus_words) - window_size):\n",
    "    center_word = corpus_words[i]\n",
    "    context_indices = list(range(i - window_size, i)) + list(range(i + 1, i + window_size + 1))\n",
    "    for j in context_indices:\n",
    "        skipgram_data.append((center_word, corpus_words[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8fca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    def forward(self, center_word_idx):\n",
    "        embeds = self.embeddings(center_word_idx)\n",
    "        out = self.linear(embeds)\n",
    "        return nn.functional.log_softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do modelo\n",
    "embedding_dim = 8\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(250):\n",
    "    for center_word_str, context_word_str in skipgram_data:\n",
    "        center_word_idx = torch.tensor([word_to_ix[center_word_str]], dtype=torch.long)\n",
    "        context_word_idx = torch.tensor([word_to_ix[context_word_str]], dtype=torch.long)\n",
    "        model.zero_grad()\n",
    "        log_probs = model(center_word_idx)\n",
    "        loss = loss_function(log_probs, context_word_idx)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7133ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização\n",
    "learned_embeddings = model.embeddings.weight.data.numpy()\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(10, vocab_size - 1))\n",
    "embeddings_2d = tsne.fit_transform(learned_embeddings)\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i, word in enumerate(vocab):\n",
    "    x, y = embeddings_2d[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, (x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "plt.title(\"Visualização de Embeddings\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc419ec7",
   "metadata": {},
   "source": [
    "# 4. Utilizando Modelos Word2Vec Pré-treinados\n",
    "\n",
    "Treinar embeddings do zero é custoso e exige um corpus massivo. Na prática, é comum utilizar modelos pré-treinados em bilhões de palavras (e.g., Wikipedia, Google News). Esses modelos fornecem representações semânticas robustas e de alta qualidade que podem ser usadas diretamente em tarefas de PLN.\n",
    "\n",
    "A biblioteca `gensim` facilita o download e o uso desses modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "# Carregando um modelo pré-treinado (GloVe treinado na Wikipedia, 100 dimensões)\n",
    "# Modelos disponíveis: print(list(api.info()['models'].keys()))\n",
    "print(\"Carregando modelo GloVe\")\n",
    "glove_model = api.load('glove-wiki-gigaword-100')\n",
    "print(\"Modelo carregado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f0c125",
   "metadata": {},
   "source": [
    "### Explorando o Modelo Pré-treinado\n",
    "\n",
    "Com o modelo carregado, podemos realizar operações semânticas, como encontrar as palavras mais similares a um dado termo ou resolver analogias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f10ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Palavras mais similares a 'king':\")\n",
    "print(glove_model.most_similar('king'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d929940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolvendo a analogia: king - man + woman = queen\n",
    "print(\"Resolvendo a analogia 'king - man + woman':\")\n",
    "print(glove_model.most_similar(positive=['woman', 'king'], negative=['man']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a5dda",
   "metadata": {},
   "source": [
    "### Visualizando Embeddings Pré-treinados\n",
    "\n",
    "Podemos extrair os vetores de um conjunto de palavras e visualizá-los com t-SNE para confirmar que o espaço semântico é coerente. Note como palavras de categorias similares (países, animais, verbos) se agrupam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd338400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando um conjunto de palavras para visualização\n",
    "words_to_plot = [\n",
    "    'king', 'queen', 'prince', 'princess', # Realeza\n",
    "    'dog', 'cat', 'lion', 'tiger',       # Animais\n",
    "    'brazil', 'france', 'japan', 'egypt',  # Países\n",
    "    'run', 'walk', 'swim', 'fly'         # Verbos\n",
    "]\n",
    "\n",
    "# Filtrando palavras que existem no vocabulário do modelo\n",
    "valid_words = [word for word in words_to_plot if word in glove_model]\n",
    "word_vectors = np.array([glove_model[word] for word in valid_words])\n",
    "\n",
    "# Redução de dimensionalidade com t-SNE\n",
    "tsne_plot = TSNE(n_components=2, random_state=42, perplexity=min(10, len(valid_words) - 1))\n",
    "embeddings_2d_plot = tsne_plot.fit_transform(word_vectors)\n",
    "\n",
    "# Plotando\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i, word in enumerate(valid_words):\n",
    "    x, y = embeddings_2d_plot[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(word, (x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "\n",
    "plt.title(\"Visualização de Embeddings Pré-treinados (GloVe)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a95df2",
   "metadata": {},
   "source": [
    "# Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4540954",
   "metadata": {},
   "source": [
    "## Exercício 1: Construindo um Tokenizador de Palavras\n",
    "\n",
    "Crie uma função Python chamada `tokenizador(texto)` que receba uma string como entrada e retorne uma lista de tokens.\n",
    "\n",
    "Sua função deve atender aos seguintes critérios:\n",
    "1.  Converter todo o texto para letras minúsculas.\n",
    "2.  Separar pontuações comuns (como `.`, `,`, `!`, `?`) das palavras. Por exemplo, \"Olá!\" deve se tornar `['olá', '!']`.\n",
    "3.  Dividir o texto em tokens com base nos espaços em branco."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d95f998",
   "metadata": {},
   "source": [
    "## Exercício 2: Criando Representações One-Hot\n",
    "\n",
    "Utilizando a função `tokenizador` do exercício anterior, crie um pipeline para gerar vetores One-Hot.\n",
    "\n",
    "Você precisará de duas funções:\n",
    "1.  `construir_vocabulario(texto, tokenizador)`: Recebe um texto e sua função de tokenização, e retorna um dicionário que mapeia cada token único a um índice (ex: `{'mundo': 0, 'olá': 1, ...}`).\n",
    "2.  `codificar_one_hot(palavra, vocabulario)`: Recebe uma palavra e o mapa de vocabulário, e retorna seu vetor One-Hot correspondente como um `torch.Tensor`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4aadfc",
   "metadata": {},
   "source": [
    "## Exercício 3: Analogias de Capitais com Embeddings Pré-treinados\n",
    "\n",
    "Embeddings de palavras capturam relações semânticas. Uma das analogias mais famosas é a de capitais: \"Atenas está para a Grécia assim como Oslo está para a Noruega\". Matematicamente, isso se traduz em:\n",
    "\n",
    "$$ \\vec{v}_{\\text{Atenas}} - \\vec{v}_{\\text{Grécia}} + \\vec{v}_{\\text{Noruega}} \\approx \\vec{v}_{\\text{Oslo}} $$\n",
    "\n",
    "Use o modelo `glove_model` carregado anteriormente para resolver a seguinte analogia: **\"Alemanha está para Berlim assim como Brasil está para ?\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd1930a",
   "metadata": {},
   "source": [
    "## Exercício 4: O Desafio da Polissemia\n",
    "\n",
    "Um dos problemas de modelos de embedding estáticos como Word2Vec e GloVe é a **polissemia**: uma única palavra pode ter múltiplos significados, mas recebe apenas um único vetor.\n",
    "\n",
    "O seu desafio é investigar a palavra \"bank\", que em inglês pode significar uma instituição financeira ou a margem de um rio.\n",
    "\n",
    "1.  Use o `glove_model` para encontrar os 10 vizinhos mais próximos da palavra \"bank\".\n",
    "2.  Analise a lista de vizinhos. Eles se referem ao contexto financeiro, ao geográfico, ou a uma mistura de ambos?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
