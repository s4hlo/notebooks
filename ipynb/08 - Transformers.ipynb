{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 - Transformer Encoder-Decoder\n",
    "\n",
    "Neste notebook, vamos implementar um modelo encoder-decoder baseado no **Transformer**, uma arquitetura que substituiu as RNNs em muitas tarefas de NLP e se tornou o padrão em tradução automática e modelos de linguagem modernos.\n",
    "\n",
    "## Objetivos de Aprendizado\n",
    "- Revisar a arquitetura do Transformer e seus principais componentes (Self-Attention, Encoder, Decoder)\n",
    "- Implementar o Encoder e o Decoder com PyTorch\n",
    "- Construir um modelo completo de tradução português-inglês usando Transformer\n",
    "- Treinar o modelo em um conjunto de dados de exemplo\n",
    "- Avaliar a qualidade das traduções geradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vaRzxrPXyIQR"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCMIS7t82sou"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "Transformers não possuem mecanismos recorrentes ou convolucionais, o que significa que eles não têm uma noção implícita da ordem dos tokens em uma sequência. Para incorporar essa informação, é adicionada uma codificação posicional aos vetores de embedding. Essa codificação é determinística e baseada em funções senoidais de diferentes frequências.\n",
    "\n",
    "A codificação posicional utilizada segue a formulação original do paper *\"Attention is All You Need\"*:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "- $pos$ representa a posição do token na sequência,\n",
    "- $i$ é o índice da dimensão do embedding,\n",
    "- $d_{\\text{model}}$ é a dimensionalidade do embedding.\n",
    "\n",
    "O resultado é uma matriz de codificação com forma $(1, \\text{max\\_len}, d_{\\text{model}})$ que é somada diretamente aos embeddings de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # matriz (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # seno nas posições pares, cosseno nas ímpares\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # pares\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # ímpares\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model) -> broadcast no batch\n",
    "        self.register_buffer(\"pe\", pe)  # não é parâmetro treinável\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        x = x + self.pe[:, :T, :]  # (B, T, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 16\n",
    "num_heads = 4\n",
    "B, T = 2, 5\n",
    "\n",
    "# embeddings simulados\n",
    "x = torch.randn(B, T, d_model)\n",
    "\n",
    "# positional encoding\n",
    "pos_enc = PositionalEncoding(d_model)\n",
    "x = pos_enc(x)  # adiciona posições\n",
    "\n",
    "print(\"Positional Encoding:\", x.shape)  # (B, T, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "O mecanismo de **multi-head attention** é um dos blocos centrais dos Transformers. Ele permite que o modelo foque em diferentes partes da sequência em paralelo, usando múltiplas \"cabeças\" de atenção. Cada cabeça realiza uma atenção com projeções diferentes dos vetores de entrada, e seus resultados são combinados ao final.\n",
    "\n",
    "A atenção é baseada no mecanismo de **Scaled Dot-Product Attention**, que recebe três vetores: $Q$ (query), $K$ (key) e $V$ (value). O cálculo da atenção segue a fórmula:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "- $Q$, $K$ e $V$ são tensores projetados a partir da entrada,\n",
    "- $d_k$ é a dimensionalidade das chaves (key),\n",
    "- A divisão por $\\sqrt{d_k}$ serve para normalizar os scores e evitar valores muito grandes que podem saturar a softmax.\n",
    "\n",
    "No caso de múltiplas cabeças, os vetores $Q$, $K$ e $V$ são divididos em $h$ partes (cabeças), cada uma com dimensionalidade reduzida $d_k = d_{\\text{model}} / h$, aplicando a atenção de forma independente em cada cabeça. Os resultados são então concatenados e projetados novamente com uma camada linear:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O\n",
    "$$\n",
    "\n",
    "Cada cabeça é computada como:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)\n",
    "$$\n",
    "\n",
    "Esse paralelismo permite que diferentes aspectos contextuais da sequência sejam aprendidos simultaneamente. Essa implementação define todas as projeções lineares necessárias, faz o `split_heads`, aplica a atenção escalada, combina os resultados com `combine_heads`, e projeta de volta para o espaço original com uma camada linear $W^O$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model deve ser divisível por num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        # Projeções lineares\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B, Tq, _ = q.size()\n",
    "        Tk = k.size(1)\n",
    "        Tv = v.size(1)\n",
    "\n",
    "        # Projeções\n",
    "        Q = self.q_proj(q)\n",
    "        K = self.k_proj(k)\n",
    "        V = self.v_proj(v)\n",
    "\n",
    "        # Split heads\n",
    "        Q = Q.view(B, Tq, self.num_heads, self.head_dim).transpose(1, 2)  # (B, h, Tq, d_head)\n",
    "        K = K.view(B, Tk, self.num_heads, self.head_dim).transpose(1, 2)  # (B, h, Tk, d_head)\n",
    "        V = V.view(B, Tv, self.num_heads, self.head_dim).transpose(1, 2)  # (B, h, Tv, d_head)\n",
    "\n",
    "        # Atenção\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (B, h, Tq, Tk)\n",
    "\n",
    "        if mask is not None:\n",
    "            # mask: (B, 1, 1, Tk) ou (B, 1, Tq, Tk)\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)  # (B, h, Tq, d_head)\n",
    "\n",
    "        # Junta os heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, Tq, self.d_model)\n",
    "\n",
    "        return self.out_proj(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(seq_len, device=None):\n",
    "    \"\"\"\n",
    "    Cria máscara causal triangular inferior.\n",
    "    shape: (1, 1, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "    return mask.unsqueeze(0).unsqueeze(0)  # (1, 1, T, T)\n",
    "\n",
    "\n",
    "def padding_mask(pad_tokens, device=None):\n",
    "    \"\"\"\n",
    "    Cria máscara de padding.\n",
    "    pad_tokens: tensor (B, T) com 1 onde é token válido e 0 onde é padding\n",
    "    retorna shape: (B, 1, 1, T) -> broadcast em atenção\n",
    "    \"\"\"\n",
    "    return pad_tokens.unsqueeze(1).unsqueeze(2).to(device)  # (B,1,1,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso\n",
    "d_model = 16\n",
    "num_heads = 4\n",
    "B, T = 2, 5\n",
    "\n",
    "x = torch.randn(B, T, d_model)\n",
    "mask = causal_mask(T, device=x.device)\n",
    "attn = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "out = attn(x, x, x, mask=mask)\n",
    "print(\"Self-Attention:\", out.shape)  # (B, T, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Layer\n",
    "\n",
    "Em Transformers, cada bloco contém uma **camada feed-forward totalmente conectada** que é aplicada de forma independente a cada posição da sequência. Essa camada é responsável por aprender transformações não-lineares locais após o mecanismo de atenção.\n",
    "\n",
    "A arquitetura típica de uma feed-forward layer é composta por duas camadas lineares com uma função de ativação não-linear (geralmente ReLU) no meio:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = W_2 \\cdot \\text{ReLU}(W_1 \\cdot x + b_1) + b_2\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "- $x$ é o vetor de entrada de dimensão $d_{\\text{model}}$,\n",
    "- $W_1 \\in \\mathbb{R}^{d_{\\text{ff}} \\times d_{\\text{model}}}$ e $W_2 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{ff}}}$ são pesos aprendidos,\n",
    "- $d_{\\text{ff}}$ é a dimensionalidade intermediária (maior que $d_{\\text{model}}$ para aumentar a capacidade do modelo),\n",
    "- $\\text{ReLU}(x) = \\max(0, x)$ é a função de ativação não-linear.\n",
    "\n",
    "Essa camada é aplicada posição a posição (de forma independente em cada token), e introduz não-linearidades e capacidade de transformação mais complexa ao modelo, além de expandir e comprimir a dimensionalidade, o que funciona como um \"bottleneck\" informativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kc5gpf2V6cSY",
    "outputId": "59ba54cf-46dd-4d06-e279-aebfd30ce57d"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(self.relu(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "B, T = 2, 5\n",
    "\n",
    "feed_forward = FeedForward(d_model, d_ff)\n",
    "\n",
    "query = torch.randn(B, T, d_model)  # (B, T, d_model)\n",
    "output = feed_forward(query)\n",
    "\n",
    "print(f'Input shape: {query.shape}')  # Input shape: (B, T, d_model)\n",
    "print(f'Output shape: {output.shape}')  # Output shape: (B, T, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EncoderLayer e Encoder\n",
    "\n",
    "Cada `EncoderLayer` é composta por dois blocos: atenção multi-cabeça seguida de normalização, e uma feed-forward seguida de outra normalização. Em ambos os casos, há conexões residuais e dropout:\n",
    "\n",
    "$$\n",
    "x_1 = \\text{LayerNorm}(x + \\text{Dropout}(\\text{MultiHead}(x, x, x)))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{LayerNorm}(x_1 + \\text{Dropout}(\\text{FFN}(x_1)))\n",
    "$$\n",
    "\n",
    "A classe `Encoder` empilha múltiplas `EncoderLayer`s após converter os tokens com `Embedding` e adicionar codificações posicionais. O fluxo é:\n",
    "\n",
    "$$\n",
    "x = \\text{Embedding}(x) + \\text{PositionalEncoding}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x = \\text{EncoderLayer}_N \\circ \\cdots \\circ \\text{EncoderLayer}_1 (x)\n",
    "$$\n",
    "\n",
    "Esse processo transforma a sequência de entrada em uma representação contextualizada, onde cada posição é influenciada pelas demais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce-MVk6t6eSl",
    "outputId": "020d261d-0074-4cff-9896-bf68c5e15e69"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward  = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        # Self-Attention\n",
    "        attn_out = self.self_attn(x, x, x, mask=src_mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # FeedForward\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = x + self.dropout(ff_out)\n",
    "        x = self.norm2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "d_ff = 64\n",
    "\n",
    "encoder_layer = EncoderLayer(d_model, num_heads, d_ff)\n",
    "\n",
    "# Exemplo\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "out = encoder_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")    # (N, T, d_model)\n",
    "print(f\"Output shape: {out.shape}\") # (N, T, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o5K1EaNm6gLT",
    "outputId": "0659c9ed-7e2e-4d9a-cd03-dbe174fb2c2b"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "d_ff = 64\n",
    "num_layers = 3\n",
    "\n",
    "encoder = Encoder(d_model, num_heads, d_ff, num_layers)\n",
    "\n",
    "# Exemplo\n",
    "batch_size = 4\n",
    "seq_len = 12\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "out = encoder(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")     # (N, T, d_model)\n",
    "print(f\"Encoder output: {out.shape}\") # (N, T, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecoderLayer e Decoder\n",
    "\n",
    "Cada `DecoderLayer` possui três blocos com conexões residuais e normalização:\n",
    "\n",
    "1. **Self-attention mascarada**: impede que o token atual veja os futuros.\n",
    "2. **Cross-attention**: permite que o decoder atenda à saída do encoder.\n",
    "3. **Feed-forward**: transformação não linear local.\n",
    "\n",
    "As operações são:\n",
    "\n",
    "$$\n",
    "x_1 = \\text{LayerNorm}(x + \\text{Dropout}(\\text{SelfAttn}(x)))\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_2 = \\text{LayerNorm}(x_1 + \\text{Dropout}(\\text{CrossAttn}(x_1, \\text{enc\\_out})))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{LayerNorm}(x_2 + \\text{Dropout}(\\text{FFN}(x_2)))\n",
    "$$\n",
    "\n",
    "O `Decoder` empilha múltiplas `DecoderLayer`s após aplicar embedding e codificação posicional, e gera uma distribuição sobre o vocabulário via uma camada linear final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1, cross_attention=True):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attention = cross_attention\n",
    "        if cross_attention:\n",
    "            self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model) if cross_attention else None\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out=None, tgt_mask=None, memory_mask=None):\n",
    "        # Masked Self-Attention\n",
    "        attn_out = self.self_attn(x, x, x, mask=tgt_mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Cross-Attention (se habilitado)\n",
    "        if self.cross_attention and enc_out is not None:\n",
    "            attn_out = self.cross_attn(x, enc_out, enc_out, mask=memory_mask)\n",
    "            x = x + self.dropout(attn_out)\n",
    "            x = self.norm2(x)\n",
    "\n",
    "        # FeedForward\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = x + self.dropout(ff_out)\n",
    "        x = self.norm3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "d_ff = 64\n",
    "\n",
    "decoder_layer = DecoderLayer(d_model, num_heads, d_ff)\n",
    "\n",
    "# Exemplo\n",
    "batch_size = 4\n",
    "src_len = 15\n",
    "tgt_len = 7\n",
    "\n",
    "enc_out = torch.randn(batch_size, src_len, d_model)  # saída do encoder\n",
    "tgt = torch.randn(batch_size, tgt_len, d_model)      # entrada do decoder\n",
    "\n",
    "tgt_mask = causal_mask(tgt_len)\n",
    "\n",
    "out = decoder_layer(tgt, enc_out, tgt_mask=tgt_mask)\n",
    "\n",
    "print(f\"Target input: {tgt.shape}\")        # (N, T_tgt, d_model)\n",
    "print(f\"DecoderLayer output: {out.shape}\") # (N, T_tgt, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1, cross_attention=True):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout, cross_attention)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc_out=None, tgt_mask=None, memory_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, tgt_mask, memory_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "d_ff = 64\n",
    "num_layers = 2\n",
    "\n",
    "decoder = Decoder(d_model, num_heads, d_ff, num_layers)\n",
    "\n",
    "# Exemplo\n",
    "batch_size = 4\n",
    "src_len = 15\n",
    "tgt_len = 7\n",
    "\n",
    "enc_out = torch.randn(batch_size, src_len, d_model)\n",
    "tgt = torch.randn(batch_size, tgt_len, d_model)\n",
    "\n",
    "tgt_mask = causal_mask(tgt_len)\n",
    "\n",
    "out = decoder(tgt, enc_out, tgt_mask=tgt_mask)\n",
    "\n",
    "print(f\"Decoder input: {tgt.shape}\")     # (N, T_tgt, d_model)\n",
    "print(f\"Decoder output: {out.shape}\")    # (N, T_tgt, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "\n",
    "A classe `Transformer` combina o encoder e o decoder em uma arquitetura completa de tradução seq2seq. Ela segue o formato proposto por Vaswani et al. (2017), onde:\n",
    "\n",
    "- O **encoder** processa a sequência de entrada e gera representações contextuais.\n",
    "- O **decoder** gera a saída passo a passo, utilizando essas representações.\n",
    "\n",
    "#### Máscaras\n",
    "\n",
    "Durante o `forward`, são geradas duas máscaras:\n",
    "- **Máscara de padding**: impede atenção a tokens vazios (`src == 0` ou `trg == 0`).\n",
    "- **Máscara causal (no-peak)**: impede que a atenção no decoder veja posições futuras, garantindo autoregressividade. Ela é definida por:\n",
    "\n",
    "$$\n",
    "\\text{nopeak}_{i,j} = \\begin{cases}\n",
    "1, & \\text{se } j \\leq i \\\\\n",
    "0, & \\text{caso contrário}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "O fluxo geral é:\n",
    "\n",
    "$$\n",
    "\\text{EncoderOutput} = \\text{Encoder}(src, src\\_mask)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{Decoder}(trg, \\text{EncoderOutput}, src\\_mask, trg\\_mask)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_encoder_layers, num_decoder_layers,\n",
    "                 src_vocab_size, tgt_vocab_size, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # embeddings separados\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        # positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "\n",
    "        # encoder e decoder\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_encoder_layers, dropout)\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_decoder_layers, dropout)\n",
    "\n",
    "        # projeção final para o vocabulário de saída\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None):\n",
    "        \"\"\"\n",
    "        src: (N, T_src) índices dos tokens da entrada\n",
    "        tgt: (N, T_tgt) índices dos tokens da saída\n",
    "        \"\"\"\n",
    "        # embeddings + posições\n",
    "        src_emb = self.src_embedding(src) * (self.src_embedding.embedding_dim ** 0.5)\n",
    "        src_emb = self.pos_encoding(src_emb)\n",
    "\n",
    "        tgt_emb = self.tgt_embedding(tgt) * (self.tgt_embedding.embedding_dim ** 0.5)\n",
    "        tgt_emb = self.pos_encoding(tgt_emb)\n",
    "\n",
    "        # encoder\n",
    "        memory = self.encoder(src_emb, src_mask)\n",
    "\n",
    "        # decoder\n",
    "        out = self.decoder(tgt_emb, memory, tgt_mask, memory_mask)\n",
    "\n",
    "        # projeção final para vocabulário alvo\n",
    "        logits = self.fc_out(out)  # (N, T_tgt, tgt_vocab_size)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "d_ff = 64\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "src_vocab_size = 120   # ex: português\n",
    "tgt_vocab_size = 150   # ex: inglês\n",
    "max_len = 50\n",
    "\n",
    "model = Transformer(d_model, num_heads, d_ff, num_encoder_layers, num_decoder_layers, src_vocab_size, tgt_vocab_size, max_len)\n",
    "\n",
    "# Exemplo\n",
    "batch_size = 4\n",
    "src_len = 12\n",
    "tgt_len = 8\n",
    "\n",
    "src = torch.randint(0, src_vocab_size, (batch_size, src_len))  # tokens de entrada\n",
    "tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_len))  # tokens de saída\n",
    "\n",
    "tgt_mask = causal_mask(tgt_len)\n",
    "\n",
    "out = model(src, tgt, tgt_mask=tgt_mask)\n",
    "\n",
    "print(f\"Source input shape: {src.shape}\")   # (N, T_src)\n",
    "print(f\"Target input shape: {tgt.shape}\")   # (N, T_tgt)\n",
    "print(f\"Output shape: {out.shape}\")         # (N, T_tgt, tgt_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tradução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    (\"olá\", \"hello\"),\n",
    "    (\"bom dia\", \"good morning\"),\n",
    "    (\"boa noite\", \"good night\"),\n",
    "    (\"como vai?\", \"how are you?\"),\n",
    "    (\"estou bem\", \"i am fine\"),\n",
    "    (\"obrigado\", \"thank you\"),\n",
    "    (\"até logo\", \"see you later\"),\n",
    "    (\"sim\", \"yes\"),\n",
    "    (\"não\", \"no\"),\n",
    "    (\"eu gosto de café\", \"i like coffee\"),\n",
    "    (\"ela gosta de música\", \"she likes music\"),\n",
    "    (\"nós vamos para a escola\", \"we go to school\"),\n",
    "    (\"ele está em casa\", \"he is at home\"),\n",
    "    (\"onde você está?\", \"where are you?\"),\n",
    "    (\"o gato está na cadeira\", \"the cat is on the chair\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    tokens = set()\n",
    "    for s in sentences:\n",
    "        tokens.update(s.lower().split())\n",
    "    stoi = {tok: i+4 for i, tok in enumerate(sorted(tokens))}\n",
    "    stoi[\"<pad>\"] = 0\n",
    "    stoi[\"<sos>\"] = 1\n",
    "    stoi[\"<eos>\"] = 2\n",
    "    stoi[\"<unk>\"] = 3\n",
    "    itos = {i: t for t, i in stoi.items()}\n",
    "    return stoi, itos\n",
    "\n",
    "# constrói vocabulários\n",
    "src_sentences = [pt for pt, en in pairs]\n",
    "tgt_sentences = [en for pt, en in pairs]\n",
    "\n",
    "src_stoi, src_itos = build_vocab(src_sentences)\n",
    "tgt_stoi, tgt_itos = build_vocab(tgt_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence, stoi, max_len=10):\n",
    "    tokens = sentence.lower().split()\n",
    "    ids = [stoi.get(tok, stoi[\"<unk>\"]) for tok in tokens]\n",
    "    ids = [stoi[\"<sos>\"]] + ids + [stoi[\"<eos>\"]]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [stoi[\"<pad>\"]] * (max_len - len(ids))\n",
    "    return ids[:max_len]\n",
    "\n",
    "max_len = 10\n",
    "data = [\n",
    "    (encode_sentence(pt, src_stoi, max_len), encode_sentence(en, tgt_stoi, max_len))\n",
    "    for pt, en in pairs\n",
    "]\n",
    "\n",
    "src_data = torch.tensor([pt for pt, en in data])\n",
    "tgt_data = torch.tensor([en for pt, en in data])\n",
    "\n",
    "print(\"src_data:\", src_data.shape)  # (N, max_len)\n",
    "print(\"tgt_data:\", tgt_data.shape)  # (N, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(src_stoi)\n",
    "tgt_vocab_size = len(tgt_stoi)\n",
    "\n",
    "d_model = 32\n",
    "num_heads = 4\n",
    "d_ff = 64\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "\n",
    "model = Transformer(\n",
    "    d_model, num_heads, d_ff,\n",
    "    num_encoder_layers, num_decoder_layers,\n",
    "    src_vocab_size, tgt_vocab_size, max_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_stoi[\"<pad>\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(0, len(src_data), batch_size):\n",
    "        src_batch = src_data[i:i+batch_size]\n",
    "        tgt_batch = tgt_data[i:i+batch_size]\n",
    "\n",
    "        # entrada do decoder é sem o último token\n",
    "        tgt_in = tgt_batch[:, :-1]\n",
    "        # alvo é sem o primeiro token\n",
    "        tgt_out = tgt_batch[:, 1:]\n",
    "\n",
    "        tgt_mask = causal_mask(tgt_in.size(1))\n",
    "\n",
    "        logits = model(src_batch, tgt_in, tgt_mask=tgt_mask)\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.reshape(-1, tgt_vocab_size),\n",
    "            tgt_out.reshape(-1)\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_translate(model, src_sentence, max_len=10):\n",
    "    model.eval()\n",
    "    src_ids = torch.tensor([encode_sentence(src_sentence, src_stoi, max_len)])\n",
    "    tgt_ids = torch.tensor([[tgt_stoi[\"<sos>\"]]])\n",
    "\n",
    "    for _ in range(max_len-1):\n",
    "        tgt_mask = causal_mask(tgt_ids.size(1))\n",
    "        logits = model(src_ids, tgt_ids, tgt_mask=tgt_mask)\n",
    "        next_token = logits[:, -1, :].argmax(-1).unsqueeze(0)\n",
    "        tgt_ids = torch.cat([tgt_ids, next_token], dim=1)\n",
    "        if next_token.item() == tgt_stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    return \" \".join([tgt_itos[i.item()] for i in tgt_ids[0]])\n",
    "\n",
    "print(greedy_translate(model, \"eu gosto de cafe\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 1: Classificação de Notícias com Transformer Encoder\n",
    "\n",
    "Implemente um módulo de classificação de texto utilizando **apenas o Encoder do Transformer**.  \n",
    "\n",
    "O modelo deve:  \n",
    "- Receber uma sequência de índices de tokens como entrada.  \n",
    "- Passar os embeddings pela pilha de camadas do Encoder.  \n",
    "- Agregar a informação da sequência por meio de um **pooling de média na dimensão temporal** (`seq_len`).  \n",
    "- Passar o vetor resultante por uma camada linear para prever a classe.  \n",
    "\n",
    "O dataset a ser utilizado é o **20 Newsgroups**, filtrado em algumas categorias de notícias.  \n",
    "Seu objetivo é treinar o classificador para prever a qual categoria pertence cada texto.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['sci.electronics', 'comp.graphics', 'sci.med', 'rec.motorcycles']\n",
    "max_len = 100\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# === Carregamento dos dados ===\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', categories=categories)\n",
    "texts = newsgroups_data.data[:5000]\n",
    "labels = newsgroups_data.target[:5000]\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# === Pré-processamento ===\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def build_vocab(texts, min_freq=1):\n",
    "    word_freq = {}\n",
    "    for text in texts:\n",
    "        tokens = preprocess_text(text)\n",
    "        for token in tokens:\n",
    "            word_freq[token] = word_freq.get(token, 0) + 1\n",
    "\n",
    "    vocab = {'<pad>': 0, '<unk>': 1}\n",
    "    index = 2\n",
    "    for word, freq in word_freq.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "# === Vocabulário ===\n",
    "vocab = build_vocab(train_texts)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "# === Dataset ===\n",
    "class NewsGroupsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        tokens = preprocess_text(text)\n",
    "        token_ids = [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n",
    "        if len(token_ids) > self.max_len:\n",
    "            token_ids = token_ids[:self.max_len]\n",
    "        else:\n",
    "            token_ids += [self.vocab['<pad>']] * (self.max_len - len(token_ids))\n",
    "        return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        token_ids = self.encode_text(text)\n",
    "        return token_ids, label\n",
    "\n",
    "\n",
    "# === DataLoaders ===\n",
    "train_dataset = NewsGroupsDataset(train_texts, train_labels, vocab, max_len)\n",
    "val_dataset = NewsGroupsDataset(val_texts, val_labels, vocab, max_len)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sua classe aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss e otimizador\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# # Loop de treino\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "#     for inputs, labels in train_dataloader:\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         # Forward\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Métricas\n",
    "#         total_loss += loss.item()\n",
    "#         preds = outputs.argmax(dim=1)\n",
    "#         correct += (preds == labels).sum().item()\n",
    "#         total += labels.size(0)\n",
    "\n",
    "#     train_acc = correct / total\n",
    "\n",
    "#     # Validação\n",
    "#     model.eval()\n",
    "#     val_loss, val_correct, val_total = 0, 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in val_dataloader:\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "\n",
    "#             val_loss += loss.item()\n",
    "#             preds = outputs.argmax(dim=1)\n",
    "#             val_correct += (preds == labels).sum().item()\n",
    "#             val_total += labels.size(0)\n",
    "\n",
    "#     val_acc = val_correct / val_total\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "#           f\"Train Loss: {total_loss:.3f}, Train Acc: {train_acc:.3f} | \"\n",
    "#           f\"Val Loss: {val_loss:.3f}, Val Acc: {val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício 2: Modelo de Linguagem com Transformer Decoder\n",
    "\n",
    "Implemente um **modelo de linguagem baseado apenas no Decoder do Transformer** (estilo *decoder-only*).  \n",
    "\n",
    "O modelo deve:  \n",
    "- Receber uma sequência de tokens como entrada.  \n",
    "- Utilizar máscara **causal** na auto-atenção, garantindo que cada posição só acesse os tokens anteriores e o próprio token.  \n",
    "- Prever o **próximo token em cada posição** (treinamento por *next token prediction*).  \n",
    "\n",
    "O corpus de treino será o fornecido na variável `corpus`.  \n",
    "Seu objetivo é treinar o modelo para gerar frases coerentes em português a partir de um prompt inicial.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def load_corpus_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # dispara erro se a requisição falhar\n",
    "    text = response.text\n",
    "    return text\n",
    "\n",
    "# Exemplo de uso\n",
    "url = \"https://raw.githubusercontent.com/wess/iotr/master/lotr.txt\"\n",
    "corpus = load_corpus_from_url(url)[:30000]\n",
    "\n",
    "print(\"Tamanho do corpus:\", len(corpus))\n",
    "\n",
    "# Tokenize o texto\n",
    "tokens = re.findall(r'\\b\\w+\\b', corpus.lower())\n",
    "\n",
    "# Constrói o vocabulário\n",
    "word_counts = Counter(tokens)\n",
    "vocab = sorted(word_counts.keys())\n",
    "\n",
    "special_tokens = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
    "word2idx = {tok: idx for idx, tok in enumerate(special_tokens, start=0)}\n",
    "\n",
    "for word in vocab:\n",
    "    if word not in word2idx:  # evita colisão\n",
    "        word2idx[word] = len(word2idx)\n",
    "\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "# Converte tokens para índices\n",
    "indices = [word2idx.get(w, word2idx[\"<unk>\"]) for w in tokens]\n",
    "\n",
    "# Gera as sequências\n",
    "sequence_length = 10\n",
    "inputs, targets = [], []\n",
    "\n",
    "for i in range(len(indices) - seq_len):\n",
    "    seq = indices[i:i+seq_len]\n",
    "    tgt = indices[i+1:i+seq_len+1]\n",
    "\n",
    "    # insere <sos> no início do input, <eos> no fim do target\n",
    "    seq = [word2idx[\"<sos>\"]] + seq\n",
    "    tgt = tgt + [word2idx[\"<eos>\"]]\n",
    "\n",
    "    inputs.append(seq)\n",
    "    targets.append(tgt)\n",
    "\n",
    "inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# Cria o dataset e o dataloader\n",
    "batch_size = 32\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seu modelo de linguagem aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# epochs = 20\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = 0\n",
    "#     for x, y in dataloader:\n",
    "#         x, y = x.to(device), y.to(device)\n",
    "#         mask = causal_mask(x.size(1)).to(device)\n",
    "#         logits = model(x, mask=mask)\n",
    "#         loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {total_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(model, prompt, word2idx, idx2word, max_new_tokens=20, device=\"cpu\"):\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Converte prompt em índices\n",
    "#     tokens = re.findall(r'\\b\\w+\\b', prompt.lower())\n",
    "#     ids = torch.tensor([[word2idx.get(tok, word2idx[\"<unk>\"]) for tok in tokens]], device=device)\n",
    "\n",
    "#     for _ in range(max_new_tokens):\n",
    "#         # Máscara causal\n",
    "#         mask = causal_mask(ids.size(1)).to(device)\n",
    "\n",
    "#         # Forward\n",
    "#         with torch.no_grad():\n",
    "#             logits = model(ids, mask=mask)  # (1, T, vocab_size)\n",
    "\n",
    "#         # Pega último token previsto (greedy)\n",
    "#         next_id = logits[:, -1, :].argmax(-1).unsqueeze(0)\n",
    "\n",
    "#         # Concatena ao input\n",
    "#         ids = torch.cat([ids, next_id], dim=1)\n",
    "\n",
    "#     # Decodifica para palavras\n",
    "#     out_tokens = [idx2word[i.item()] for i in ids[0]]\n",
    "#     return \" \".join(out_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"the\"\n",
    "# generated = generate_text(model, prompt, word2idx, idx2word, max_new_tokens=10, device=device)\n",
    "# print(\"Generated:\", generated)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
