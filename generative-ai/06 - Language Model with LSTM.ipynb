{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Modelos de Linguagem com LSTM\n",
    "\n",
    "Este notebook introduz os **Modelos de Linguagem** (*Language Models*), uma das aplicações mais fundamentais e poderosas do processamento de linguagem natural. Exploraremos a teoria matemática por trás desses modelos e implementaremos uma versão prática usando **Long Short-Term Memory (LSTM)** networks com PyTorch.\n",
    "\n",
    "## Conteúdos Abordados\n",
    "\n",
    "1. **Fundamentos Teóricos de Modelos de Linguagem**: A matemática da modelagem probabilística da linguagem\n",
    "2. **Long Short-Term Memory (LSTM)**: Arquitetura avançada para sequências longas\n",
    "3. **Implementação de um Modelo de Linguagem com LSTM**: Construção completa do pipeline\n",
    "4. **Geração Autorregressiva**: Como gerar texto de forma probabilística\n",
    "5. **Análise de Perplexidade**: Métrica de avaliação de modelos de linguagem\n",
    "6. **Análise dos Embeddings Aprendidos**: Visualização do espaço semântico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4",
   "metadata": {},
   "source": [
    "## 1. Fundamentos Teóricos de Modelos de Linguagem\n",
    "\n",
    "Um **modelo de linguagem** é um modelo probabilístico que atribui probabilidades a sequências de palavras. Formalmente, dado um vocabulário $V$ e uma sequência de palavras $w_1, w_2, \\ldots, w_T$, um modelo de linguagem estima a probabilidade conjunta:\n",
    "\n",
    "$$P(w_1, w_2, \\ldots, w_T)$$\n",
    "\n",
    "### Decomposição Autorregressiva\n",
    "\n",
    "Pela regra da cadeia da probabilidade, podemos decompor essa probabilidade conjunta como:\n",
    "\n",
    "$$P(w_1, w_2, \\ldots, w_T) = \\prod_{t=1}^{T} P(w_t | w_1, w_2, \\ldots, w_{t-1})$$\n",
    "\n",
    "Onde $P(w_t | w_1, \\ldots, w_{t-1})$ é a **probabilidade condicional** da palavra $w_t$ dado todo o contexto anterior. Esta decomposição é chamada de **autorregressiva** porque cada palavra é modelada condicionalmente em relação às palavras anteriores.\n",
    "\n",
    "### O Problema da Maldição da Dimensionalidade\n",
    "\n",
    "Estimar $P(w_t | w_1, \\ldots, w_{t-1})$ diretamente é impraticável, pois:\n",
    "1. O número de possíveis contextos cresce exponencialmente com o comprimento\n",
    "2. A maioria das sequências específicas nunca aparece nos dados de treino\n",
    "\n",
    "### Aproximação Neural\n",
    "\n",
    "Modelos neurais de linguagem resolvem este problema aprendendo uma função paramétrica $f_\\theta$:\n",
    "\n",
    "$$P(w_t | w_1, \\ldots, w_{t-1}) \\approx f_\\theta(w_1, \\ldots, w_{t-1})$$\n",
    "\n",
    "Onde $\\theta$ representa os parâmetros da rede neural (pesos e biases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4",
   "metadata": {},
   "source": [
    "## 2. Long Short-Term Memory (LSTM)\n",
    "\n",
    "Enquanto RNNs tradicionais sofrem do problema de **desvanecimento do gradiente** (*vanishing gradient*), as **LSTMs** foram projetadas especificamente para capturar dependências de longo prazo em sequências.\n",
    "\n",
    "### Arquitetura da Célula LSTM\n",
    "\n",
    "Uma célula LSTM possui três **gates** (portões) que controlam o fluxo de informação:\n",
    "\n",
    "1. **Forget Gate** ($f_t$): Decide quanta informação descartar do estado da célula\n",
    "2. **Input Gate** ($i_t$): Decide quais valores atualizar no estado da célula\n",
    "3. **Output Gate** ($o_t$): Controla quais partes do estado da célula serão outputadas\n",
    "\n",
    "### Equações Matemáticas da LSTM\n",
    "\n",
    "Para o tempo $t$, com entrada $x_t$ e estado oculto anterior $h_{t-1}$:\n",
    "\n",
    "**Gates:**\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "\n",
    "**Candidato a novo estado:**\n",
    "$$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "**Estado da célula:**\n",
    "$$C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t$$\n",
    "\n",
    "**Estado oculto:**\n",
    "$$h_t = o_t * \\tanh(C_t)$$\n",
    "\n",
    "Onde $\\sigma$ é a função sigmoid, $*$ denota multiplicação elemento-a-elemento, e $W$, $b$ são as matrizes de pesos e vetores de bias, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e2f3g4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2g3h4",
   "metadata": {},
   "source": [
    "## 3. Preparação dos Dados\n",
    "\n",
    "Para treinar nosso modelo de linguagem, utilizaremos um corpus de texto em português. O processo de preparação envolve:\n",
    "\n",
    "1. **Limpeza e tokenização** do texto\n",
    "2. **Construção do vocabulário** com as palavras mais frequentes\n",
    "3. **Criação de sequências** de treinamento com janela deslizante\n",
    "4. **Mapeamento** de tokens para índices numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1g2h3i4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus expandido em português sobre inteligência artificial\n",
    "corpus_text = \"\"\"\n",
    "A inteligencia artificial e uma area da ciencia da computacao que se concentra no desenvolvimento de sistemas capazes de realizar tarefas que normalmente requerem inteligencia humana.\n",
    "O aprendizado de maquina e um subcampo da inteligencia artificial que permite aos computadores aprender e melhorar automaticamente atraves da experiencia.\n",
    "As redes neurais artificiais sao inspiradas no funcionamento do cerebro humano e consistem em nos interconectados que processam informacoes.\n",
    "O aprendizado profundo utiliza redes neurais com multiplas camadas para modelar e compreender dados complexos.\n",
    "Os algoritmos de aprendizado supervisionado aprendem a partir de exemplos rotulados para fazer predicoes sobre novos dados.\n",
    "O aprendizado nao supervisionado descobre padroes ocultos em dados sem rotulos explicitos.\n",
    "O processamento de linguagem natural permite que os computadores compreendam e gerem texto em linguagem humana.\n",
    "A visao computacional capacita as maquinas a interpretar e analisar conteudo visual do mundo real.\n",
    "Os sistemas de recomendacao utilizam algoritmos de aprendizado de maquina para sugerir produtos ou conteudo aos usuarios.\n",
    "A robotica integra inteligencia artificial com engenharia mecanica para criar maquinas autonomas.\n",
    "O reconhecimento de padroes e fundamental para identificar estruturas e regularidades nos dados.\n",
    "Os modelos generativos podem criar novos dados que se assemelham aos dados de treinamento originais.\n",
    "A otimizacao e crucial para encontrar os melhores parametros em algoritmos de aprendizado de maquina.\n",
    "O overfitting ocorre quando um modelo aprende muito especificamente os dados de treino perdendo capacidade de generalizacao.\n",
    "A validacao cruzada e uma tecnica para avaliar o desempenho de modelos de forma mais robusta.\n",
    "As funcoes de ativacao introduzem nao linearidade nas redes neurais permitindo modelar relacoes complexas.\n",
    "O gradiente descendente e um algoritmo de otimizacao usado para treinar modelos de aprendizado de maquina.\n",
    "A regularizacao ajuda a prevenir o overfitting adicionando penalidades aos parametros do modelo.\n",
    "Os transformadores revolucionaram o processamento de linguagem natural com mecanismos de atencao.\n",
    "A inteligencia artificial generativa pode criar texto imagens audio e outros tipos de conteudo criativo.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Tamanho do corpus: {len(corpus_text)} caracteres\")\n",
    "print(f\"Primeiros 200 caracteres:\")\n",
    "print(corpus_text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g1h2i3j4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Limpa e tokeniza o texto.\"\"\"\n",
    "    # Converter para minúsculas e remover caracteres especiais\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-záàâãéèêíìîóòôõúùûç\\s]', '', text)\n",
    "    \n",
    "    # Tokenizar por palavras\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def build_vocabulary(tokens, vocab_size=1000):\n",
    "    \"\"\"Constrói vocabulário com as palavras mais frequentes.\"\"\"\n",
    "    counter = Counter(tokens)\n",
    "    \n",
    "    # Tokens especiais\n",
    "    special_tokens = ['<UNK>', '<START>', '<END>']\n",
    "    \n",
    "    # Palavras mais frequentes (excluindo tokens especiais)\n",
    "    frequent_words = [word for word, _ in counter.most_common(vocab_size - len(special_tokens))]\n",
    "    \n",
    "    # Vocabulário completo\n",
    "    vocabulary = special_tokens + frequent_words\n",
    "    \n",
    "    # Mapeamentos\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    \n",
    "    return vocabulary, word_to_idx, idx_to_word\n",
    "\n",
    "# Processar o corpus\n",
    "tokens = preprocess_text(corpus_text)\n",
    "vocabulary, word_to_idx, idx_to_word = build_vocabulary(tokens, vocab_size=200)\n",
    "\n",
    "print(f\"Total de tokens: {len(tokens)}\")\n",
    "print(f\"Vocabulário único: {len(vocabulary)}\")\n",
    "print(f\"Primeiras 20 palavras do vocabulário: {vocabulary[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h1i2j3k4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(tokens, word_to_idx):\n",
    "    \"\"\"Converte tokens em índices numéricos.\"\"\"\n",
    "    unk_idx = word_to_idx['<UNK>']\n",
    "    return [word_to_idx.get(token, unk_idx) for token in tokens]\n",
    "\n",
    "def create_training_sequences(encoded_tokens, seq_length):\n",
    "    \"\"\"Cria sequências de entrada e alvos para treinamento.\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for i in range(len(encoded_tokens) - seq_length):\n",
    "        seq = encoded_tokens[i:i + seq_length]\n",
    "        target = encoded_tokens[i + seq_length]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    \n",
    "    return sequences, targets\n",
    "\n",
    "# Codificar tokens\n",
    "encoded_tokens = encode_tokens(tokens, word_to_idx)\n",
    "\n",
    "# Criar sequências de treinamento\n",
    "seq_length = 10  # Comprimento da sequência de contexto\n",
    "sequences, targets = create_training_sequences(encoded_tokens, seq_length)\n",
    "\n",
    "print(f\"Número total de sequências de treinamento: {len(sequences)}\")\n",
    "print(f\"Exemplo de sequência: {sequences[0]}\")\n",
    "print(f\"Alvo correspondente: {targets[0]}\")\n",
    "\n",
    "# Decodificar para verificar\n",
    "example_seq_words = [idx_to_word[idx] for idx in sequences[0]]\n",
    "example_target_word = idx_to_word[targets[0]]\n",
    "print(f\"Sequência em palavras: {example_seq_words}\")\n",
    "print(f\"Alvo em palavra: {example_target_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1j2k3l4",
   "metadata": {},
   "source": [
    "## 4. Dataset Personalizado para PyTorch\n",
    "\n",
    "Criaremos uma classe Dataset personalizada para facilitar o carregamento dos dados em lotes (*batches*) durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j1k2l3m4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.LongTensor(self.sequences[idx]),\n",
    "            torch.LongTensor([self.targets[idx]])\n",
    "        )\n",
    "\n",
    "# Criar dataset e dataloader\n",
    "dataset = LanguageModelDataset(sequences, targets)\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Dataset criado com {len(dataset)} exemplos\")\n",
    "print(f\"Número de lotes: {len(dataloader)}\")\n",
    "\n",
    "# Testar um lote\n",
    "for batch_seq, batch_target in dataloader:\n",
    "    print(f\"Formato do lote de sequências: {batch_seq.shape}\")\n",
    "    print(f\"Formato do lote de alvos: {batch_target.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n4",
   "metadata": {},
   "source": [
    "## 5. Implementação do Modelo de Linguagem com LSTM\n",
    "\n",
    "Nossa arquitetura consistirá em:\n",
    "\n",
    "1. **Camada de Embedding**: Converte índices de palavras em vetores densos\n",
    "2. **Camada LSTM**: Processa a sequência e mantém memória de longo prazo\n",
    "3. **Camada Linear de Saída**: Projeta do espaço oculto para o vocabulário\n",
    "4. **Dropout**: Regularização para prevenir overfitting\n",
    "\n",
    "A saída final são **logits** sobre todo o vocabulário, que são convertidos em probabilidades via **softmax**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l1m2n3o4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=2, dropout=0.3):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Camadas da rede\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, \n",
    "                           dropout=dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass do modelo.\n",
    "        \n",
    "        Args:\n",
    "            x: tensor de entrada [batch_size, seq_length]\n",
    "            hidden: estado oculto inicial (opcional)\n",
    "        \n",
    "        Returns:\n",
    "            output: logits sobre vocabulário [batch_size, seq_length, vocab_size]\n",
    "            hidden: estado oculto final\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Embedding: [batch_size, seq_length] -> [batch_size, seq_length, embedding_dim]\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # LSTM: [batch_size, seq_length, embedding_dim] -> [batch_size, seq_length, hidden_dim]\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # Dropout para regularização\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Projeção para vocabulário: [batch_size, seq_length, hidden_dim] -> [batch_size, seq_length, vocab_size]\n",
    "        output = self.fc_out(lstm_out)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"Inicializa estado oculto com zeros.\"\"\"\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))\n",
    "\n",
    "# Hiperparâmetros do modelo\n",
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "dropout = 0.3\n",
    "\n",
    "# Instanciar modelo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Modelo criado com {sum(p.numel() for p in model.parameters())} parâmetros\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1n2o3p4",
   "metadata": {},
   "source": [
    "## 6. Treinamento do Modelo\n",
    "\n",
    "O treinamento de um modelo de linguagem usa **Cross-Entropy Loss** entre as probabilidades preditas e as palavras reais. A cada época, passamos todas as sequências pelo modelo e atualizamos os parâmetros via **backpropagation**.\n",
    "\n",
    "### Função de Loss\n",
    "\n",
    "Para uma sequência de entrada $x_1, \\ldots, x_T$ e alvo $y$, a loss é:\n",
    "\n",
    "$$\\mathcal{L} = -\\log P(y | x_1, \\ldots, x_T) = -\\log \\frac{\\exp(\\text{logit}_y)}{\\sum_{i=1}^{|V|} \\exp(\\text{logit}_i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n1o2p3q4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração de treinamento\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 50\n",
    "\n",
    "# Lista para armazenar perdas\n",
    "training_losses = []\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Treina o modelo por uma época.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_seq, batch_target in dataloader:\n",
    "        batch_seq = batch_seq.to(device)\n",
    "        batch_target = batch_target.squeeze(-1).to(device)  # Remove dimensão extra\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output, _ = model(batch_seq)\n",
    "        \n",
    "        # Pegar apenas o último token de saída para cada sequência\n",
    "        # output shape: [batch_size, seq_length, vocab_size]\n",
    "        # Queremos: [batch_size, vocab_size]\n",
    "        last_output = output[:, -1, :]\n",
    "        \n",
    "        # Calcular loss\n",
    "        loss = criterion(last_output, batch_target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (importante para LSTMs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Atualizar parâmetros\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "print(\"Iniciando treinamento...\")\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    epoch_loss = train_epoch(model, dataloader, criterion, optimizer, device)\n",
    "    training_losses.append(epoch_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Época [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Treinamento concluído!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o1p2q3r4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar curva de perda\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(training_losses, linewidth=2)\n",
    "plt.title('Curva de Perda Durante o Treinamento', fontsize=16)\n",
    "plt.xlabel('Época', fontsize=14)\n",
    "plt.ylabel('Cross-Entropy Loss', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Perda inicial: {training_losses[0]:.4f}\")\n",
    "print(f\"Perda final: {training_losses[-1]:.4f}\")\n",
    "print(f\"Redução: {((training_losses[0] - training_losses[-1]) / training_losses[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p1q2r3s4",
   "metadata": {},
   "source": [
    "## 7. Geração Autorregressiva de Texto\n",
    "\n",
    "Agora implementaremos a **geração autorregressiva**, onde o modelo gera texto palavra por palavra, usando suas próprias predições anteriores como entrada para as próximas predições.\n",
    "\n",
    "### Processo de Geração\n",
    "\n",
    "1. Começamos com uma sequência *seed* (semente)\n",
    "2. O modelo prediz a próxima palavra mais provável\n",
    "3. Adicionamos essa palavra à sequência\n",
    "4. Repetimos o processo usando a nova sequência como entrada\n",
    "\n",
    "### Estratégias de Amostragem\n",
    "\n",
    "- **Greedy**: Sempre escolher a palavra mais provável\n",
    "- **Random Sampling**: Amostrar aleatoriamente da distribuição de probabilidade\n",
    "- **Temperature Sampling**: Controlar a \"criatividade\" com um parâmetro de temperatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q1r2s3t4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_text, max_length=50, temperature=1.0, device='cpu'):\n",
    "    \"\"\"\n",
    "    Gera texto usando o modelo treinado.\n",
    "    \n",
    "    Args:\n",
    "        model: modelo LSTM treinado\n",
    "        seed_text: texto inicial (string)\n",
    "        max_length: número máximo de palavras a gerar\n",
    "        temperature: controla aleatoriedade (1.0 = normal, >1.0 = mais criativo, <1.0 = mais conservador)\n",
    "        device: dispositivo (cpu ou cuda)\n",
    "    \n",
    "    Returns:\n",
    "        texto gerado (string)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenizar e codificar texto seed\n",
    "    seed_tokens = preprocess_text(seed_text)\n",
    "    if len(seed_tokens) == 0:\n",
    "        seed_tokens = ['inteligencia']  # Fallback\n",
    "    \n",
    "    # Codificar tokens\n",
    "    seed_indices = encode_tokens(seed_tokens, word_to_idx)\n",
    "    \n",
    "    # Garantir que temos pelo menos seq_length tokens\n",
    "    if len(seed_indices) < seq_length:\n",
    "        # Pad com tokens <START>\n",
    "        start_idx = word_to_idx['<START>']\n",
    "        seed_indices = [start_idx] * (seq_length - len(seed_indices)) + seed_indices\n",
    "    else:\n",
    "        # Usar apenas os últimos seq_length tokens\n",
    "        seed_indices = seed_indices[-seq_length:]\n",
    "    \n",
    "    generated_indices = seed_indices.copy()\n",
    "    current_sequence = seed_indices.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Converter para tensor\n",
    "            input_tensor = torch.LongTensor([current_sequence]).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output, _ = model(input_tensor)\n",
    "            \n",
    "            # Pegar logits da última posição\n",
    "            last_logits = output[0, -1, :]\n",
    "            \n",
    "            # Aplicar temperature\n",
    "            if temperature != 1.0:\n",
    "                last_logits = last_logits / temperature\n",
    "            \n",
    "            # Converter para probabilidades\n",
    "            probabilities = F.softmax(last_logits, dim=0)\n",
    "            \n",
    "            # Amostrar próxima palavra\n",
    "            if temperature == 0.0:  # Greedy\n",
    "                next_token_idx = torch.argmax(probabilities).item()\n",
    "            else:  # Sampling\n",
    "                next_token_idx = torch.multinomial(probabilities, 1).item()\n",
    "            \n",
    "            # Parar se gerar token de fim\n",
    "            if next_token_idx == word_to_idx.get('<END>', -1):\n",
    "                break\n",
    "            \n",
    "            # Adicionar nova palavra\n",
    "            generated_indices.append(next_token_idx)\n",
    "            \n",
    "            # Atualizar sequência atual (janela deslizante)\n",
    "            current_sequence = current_sequence[1:] + [next_token_idx]\n",
    "    \n",
    "    # Decodificar de volta para texto\n",
    "    generated_words = []\n",
    "    for idx in generated_indices:\n",
    "        word = idx_to_word.get(idx, '<UNK>')\n",
    "        if word not in ['<START>', '<END>', '<UNK>']:\n",
    "            generated_words.append(word)\n",
    "    \n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "# Testar geração com diferentes seeds e temperaturas\n",
    "seeds = [\n",
    "    \"inteligencia artificial\",\n",
    "    \"aprendizado de maquina\",\n",
    "    \"redes neurais\"\n",
    "]\n",
    "\n",
    "temperatures = [0.5, 1.0, 1.5]\n",
    "\n",
    "print(\"=== EXEMPLOS DE GERAÇÃO DE TEXTO ===\")\n",
    "print()\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"Seed: '{seed}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        generated_text = generate_text(model, seed, max_length=30, temperature=temp, device=device)\n",
    "        print(f\"Temperature {temp}: {generated_text}\")\n",
    "    \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r1s2t3u4",
   "metadata": {},
   "source": [
    "## 8. Avaliação: Perplexidade do Modelo\n",
    "\n",
    "A **perplexidade** é a métrica padrão para avaliar modelos de linguagem. Ela mede quão \"surpreso\" o modelo fica com uma sequência de palavras. Matematicamente:\n",
    "\n",
    "$$\\text{Perplexidade} = \\exp\\left(-\\frac{1}{T} \\sum_{t=1}^{T} \\log P(w_t | w_1, \\ldots, w_{t-1})\\right)$$\n",
    "\n",
    "- **Perplexidade menor** = modelo melhor\n",
    "- **Perplexidade = tamanho do vocabulário** = modelo aleatório\n",
    "- **Perplexidade = 1** = modelo perfeito (impossível na prática)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s1t2u3v4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Calcula perplexidade do modelo nos dados de teste.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_tokens = 0\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_seq, batch_target in dataloader:\n",
    "            batch_seq = batch_seq.to(device)\n",
    "            batch_target = batch_target.squeeze(-1).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output, _ = model(batch_seq)\n",
    "            \n",
    "            # Última saída de cada sequência\n",
    "            last_output = output[:, -1, :]\n",
    "            \n",
    "            # Calcular loss\n",
    "            loss = criterion(last_output, batch_target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_tokens += batch_target.size(0)\n",
    "    \n",
    "    # Perplexidade é exp da loss média\n",
    "    average_loss = total_loss / num_tokens\n",
    "    perplexity = math.exp(average_loss)\n",
    "    \n",
    "    return perplexity, average_loss\n",
    "\n",
    "# Calcular perplexidade no conjunto de dados\n",
    "perplexity, average_loss = calculate_perplexity(model, dataloader, device)\n",
    "\n",
    "print(f\"=== AVALIAÇÃO DO MODELO ===\")\n",
    "print(f\"Perda média: {average_loss:.4f}\")\n",
    "print(f\"Perplexidade: {perplexity:.2f}\")\n",
    "print(f\"Tamanho do vocabulário: {vocab_size}\")\n",
    "print(f\"Perplexidade de modelo aleatório: {vocab_size}\")\n",
    "print(f\"Melhoria sobre modelo aleatório: {(vocab_size - perplexity):.2f} pontos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t1u2v3w4",
   "metadata": {},
   "source": [
    "## 9. Análise dos Embeddings Aprendidos\n",
    "\n",
    "Vamos visualizar os embeddings de palavras que o modelo aprendeu durante o treinamento. Palavras semanticamente similares devem ter embeddings próximos no espaço vetorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u1v2w3x4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def visualize_embeddings(model, vocabulary, word_to_idx, num_words=30):\n",
    "    \"\"\"\n",
    "    Visualiza embeddings usando t-SNE.\n",
    "    \"\"\"\n",
    "    # Extrair embeddings do modelo\n",
    "    embeddings = model.embedding.weight.data.cpu().numpy()\n",
    "    \n",
    "    # Selecionar palavras mais interessantes (evitar tokens especiais)\n",
    "    interesting_words = []\n",
    "    interesting_indices = []\n",
    "    \n",
    "    for word in vocabulary[3:num_words+3]:  # Pular tokens especiais\n",
    "        if word in word_to_idx:\n",
    "            interesting_words.append(word)\n",
    "            interesting_indices.append(word_to_idx[word])\n",
    "    \n",
    "    # Embeddings selecionados\n",
    "    selected_embeddings = embeddings[interesting_indices]\n",
    "    \n",
    "    # Reduzir dimensionalidade com t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(15, len(interesting_words)-1))\n",
    "    embeddings_2d = tsne.fit_transform(selected_embeddings)\n",
    "    \n",
    "    # Plotar\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                         alpha=0.7, s=100, c=range(len(interesting_words)), \n",
    "                         cmap='tab20')\n",
    "    \n",
    "    # Adicionar labels\n",
    "    for i, word in enumerate(interesting_words):\n",
    "        plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), \n",
    "                    xytext=(5, 5), textcoords='offset points', \n",
    "                    fontsize=10, alpha=0.8)\n",
    "    \n",
    "    plt.title('Visualização dos Embeddings Aprendidos (t-SNE)', fontsize=16)\n",
    "    plt.xlabel('Componente 1', fontsize=14)\n",
    "    plt.ylabel('Componente 2', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def find_similar_words(model, word, word_to_idx, idx_to_word, top_k=5):\n",
    "    \"\"\"\n",
    "    Encontra palavras mais similares usando similaridade do cosseno.\n",
    "    \"\"\"\n",
    "    if word not in word_to_idx:\n",
    "        return f\"Palavra '{word}' não encontrada no vocabulário.\"\n",
    "    \n",
    "    # Embedding da palavra alvo\n",
    "    word_idx = word_to_idx[word]\n",
    "    embeddings = model.embedding.weight.data.cpu().numpy()\n",
    "    word_embedding = embeddings[word_idx].reshape(1, -1)\n",
    "    \n",
    "    # Calcular similaridades\n",
    "    similarities = cosine_similarity(word_embedding, embeddings)[0]\n",
    "    \n",
    "    # Encontrar índices das palavras mais similares\n",
    "    similar_indices = np.argsort(similarities)[::-1][1:top_k+1]  # Excluir a própria palavra\n",
    "    \n",
    "    similar_words = [(idx_to_word[idx], similarities[idx]) \n",
    "                     for idx in similar_indices if idx in idx_to_word]\n",
    "    \n",
    "    return similar_words\n",
    "\n",
    "# Visualizar embeddings\n",
    "visualize_embeddings(model, vocabulary, word_to_idx, num_words=25)\n",
    "\n",
    "# Testar similaridades\n",
    "print(\"=== PALAVRAS SIMILARES ===\")\n",
    "test_words = ['inteligencia', 'aprendizado', 'redes', 'dados']\n",
    "\n",
    "for word in test_words:\n",
    "    if word in word_to_idx:\n",
    "        similar = find_similar_words(model, word, word_to_idx, idx_to_word)\n",
    "        print(f\"\\nPalavras similares a '{word}':\")\n",
    "        for similar_word, score in similar:\n",
    "            print(f\"  {similar_word}: {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v1w2x3y4",
   "metadata": {},
   "source": [
    "## Exercícios\n",
    "\n",
    "### Exercício 1: Experimentando com Hiperparâmetros\n",
    "\n",
    "Modifique os hiperparâmetros (`embedding_dim`, `hidden_dim`, camadas, dropout...) do modelo LSTM e observe como afetam o desempenho.\n",
    "\n",
    "Para cada configuração, registre:\n",
    "- Perplexidade final\n",
    "- Tempo de treinamento\n",
    "- Qualidade subjetiva da geração de texto\n",
    "\n",
    "Qual configuração oferece o melhor equilíbrio entre desempenho e eficiência?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z1a2b3c4",
   "metadata": {},
   "source": [
    "### Exercício 2: Expandindo o Corpus\n",
    "\n",
    "O corpus atual é relativamente pequeno. Sua tarefa é:\n",
    "\n",
    "1. **Expandir o corpus**: Adicione mais texto sobre IA em português (pode usar artigos da Wikipedia, livros em domínio público, etc.)\n",
    "2. **Pré-processar adequadamente**: Implemente limpeza mais robusta (remoção de URLs, normalização de acentos, etc.)\n",
    "3. **Aumentar o vocabulário**: Teste com vocabulários maiores\n",
    "4. **Retreinar o modelo**: Use o corpus expandido e compare os resultados\n",
    "\n",
    "Como o tamanho do corpus afeta a qualidade da geração?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d5",
   "metadata": {},
   "source": [
    "### Exercício 3: Implementando Top-k e Nucleus Sampling\n",
    "\n",
    "A função `generate_text` atual usa apenas temperature sampling. Implemente duas estratégias mais avançadas:\n",
    "\n",
    "1. **Top-k Sampling**: Considere apenas as k palavras mais prováveis\n",
    "2. **Top-p Sampling**: Considere o menor conjunto de palavras cuja probabilidade acumulada seja ≥ p\n",
    "\n",
    "Compare a qualidade da geração entre as diferentes estratégias."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
