{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eccc2dab",
   "metadata": {},
   "source": [
    "# Redes Neurais Recorrentes com PyTorch\n",
    "\n",
    "Este notebook explora a teoria e a implementação de Redes Neurais Recorrentes (RNNs) utilizando PyTorch. As RNNs são uma classe de redes neurais especializadas no processamento de dados sequenciais, como séries temporais ou texto.\n",
    "\n",
    "## Conteúdos Abordados\n",
    "\n",
    "1.  **Fundamentos Teóricos das RNNs**: O conceito de recorrência e o estado oculto.\n",
    "2.  **Implementando uma RNN**: Construção de uma célula RNN.\n",
    "3.  **O Módulo `torch.nn.RNN`**: Utilizando a implementação otimizada do PyTorch.\n",
    "4.  **Exemplo 1: Predição de Séries Temporais**: Um modelo autorregressivo para prever o próximo valor de uma sequência numérica.\n",
    "5.  **Exemplo 2: Predição do Próximo Caractere**: Um modelo de linguagem em nível de caractere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7007b974",
   "metadata": {},
   "source": [
    "## 1. Fundamentos Teóricos das RNNs\n",
    "\n",
    "Diferentemente das redes neurais *feedforward*, as RNNs possuem um \"laço\" em sua arquitetura que permite que a informação persista. Essa característica é fundamental para o processamento de sequências, pois a rede pode manter um \"estado\" ou \"memória\" das informações vistas em passos de tempo anteriores.\n",
    "\n",
    "A recorrência é matematicamente definida pela seguinte relação, onde em cada passo de tempo $t$, a saída do estado oculto $h_t$ é uma função do estado oculto anterior $h_{t-1}$ e da entrada atual $x_t$:\n",
    "\n",
    "$$\n",
    "h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h)\n",
    "$$\n",
    "\n",
    "A saída da rede em um determinado passo de tempo, $y_t$, é tipicamente uma função do estado oculto $h_t$:\n",
    "\n",
    "$$\n",
    "y_t = W_{hy}h_t + b_y\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "-   $x_t$: Vetor de entrada no tempo $t$.\n",
    "-   $h_t$: Vetor do estado oculto (memória) no tempo $t$.\n",
    "-   $y_t$: Vetor de saída no tempo $t$.\n",
    "-   $W_{xh}, W_{hh}, W_{hy}$: Matrizes de pesos (entrada-oculto, oculto-oculto, oculto-saída).\n",
    "-   $b_h, b_y$: Vetores de viés (bias).\n",
    "-   $f$: Função de ativação não linear, comumente `tanh` ou `ReLU`.\n",
    "\n",
    "Essa estrutura permite que a RNN compartilhe os mesmos parâmetros ($W, b$) ao longo de toda a sequência, tornando-a eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c37eb14",
   "metadata": {},
   "source": [
    "## 2. Implementando uma RNN\n",
    "\n",
    "Para solidificar o entendimento, podemos implementar uma célula RNN simples usando apenas as operações fundamentais do PyTorch. Uma célula RNN processa um único passo de tempo da sequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c5df62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato da entrada (x_t): torch.Size([5, 10])\n",
      "Formato do estado oculto anterior (h_prev): torch.Size([5, 20])\n",
      "Formato do estado oculto de saída (h_next): torch.Size([5, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SimpleRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleRNNCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Camada para entrada -> hidden\n",
    "        self.input2hidden = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # Camada para hidden -> hidden (recorrência)\n",
    "        self.hidden2hidden = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        # Função de ativação\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x_t, h_prev):\n",
    "        # h_t = tanh(W_xh * x_t + W_hh * h_prev + b)\n",
    "        h_t = self.activation(\n",
    "            self.input2hidden(x_t) + self.hidden2hidden(h_prev)\n",
    "        )\n",
    "        return h_t\n",
    "\n",
    "# Exemplo de uso\n",
    "batch_size = 5\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "\n",
    "cell = SimpleRNNCell(input_size, hidden_size)\n",
    "x_t = torch.randn(batch_size, input_size)\n",
    "h_prev = torch.randn(batch_size, hidden_size)\n",
    "\n",
    "h_next = cell(x_t, h_prev)\n",
    "print(f\"Formato da entrada (x_t): {x_t.shape}\")\n",
    "print(f\"Formato do estado oculto anterior (h_prev): {h_prev.shape}\")\n",
    "print(f\"Formato do estado oculto de saída (h_next): {h_next.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "321dfe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formato da sequência de entrada: torch.Size([5, 7, 10])\n",
      "Formato da sequência de saída: torch.Size([5, 7, 20])\n"
     ]
    }
   ],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = SimpleRNNCell(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, x_sequence):\n",
    "        # x_sequence: [batch_size, sequence_length, input_size]\n",
    "        batch_size = x_sequence.size(0)\n",
    "        \n",
    "        # Inicializa o estado oculto com zeros\n",
    "        h_t = torch.zeros(batch_size, self.hidden_size)\n",
    "        \n",
    "        # Lista para armazenar as saídas de cada passo de tempo\n",
    "        outputs = []\n",
    "        \n",
    "        # Itera sobre cada passo de tempo na sequência\n",
    "        for t in range(x_sequence.size(1)):\n",
    "            x_t = x_sequence[:, t, :] # Pega a entrada no tempo t\n",
    "            h_t = self.cell(x_t, h_t)\n",
    "            outputs.append(h_t)\n",
    "            \n",
    "        # Empilha as saídas\n",
    "        # A saída final terá formato [batch_size, sequence_length, hidden_size]\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "# Exemplo de uso com uma sequência\n",
    "seq_length = 7\n",
    "rnn_from_scratch = SimpleRNN(input_size, hidden_size)\n",
    "x_sequence = torch.randn(batch_size, seq_length, input_size)\n",
    "\n",
    "output_sequence = rnn_from_scratch(x_sequence)\n",
    "print(f\"Formato da sequência de entrada: {x_sequence.shape}\")\n",
    "print(f\"Formato da sequência de saída: {output_sequence.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b07b1",
   "metadata": {},
   "source": [
    "## 3. O Módulo `torch.nn.RNN`\n",
    "\n",
    "Na prática utilizamos o módulo `torch.nn.RNN`, que é altamente otimizado e mais robusto.\n",
    "\n",
    "Seus parâmetros principais são:\n",
    "-   `input_size`: A dimensionalidade da entrada $x_t$.\n",
    "-   `hidden_size`: A dimensionalidade do estado oculto $h_t$.\n",
    "-   `num_layers`: Número de camadas recorrentes empilhadas.\n",
    "-   `batch_first`: Se `True`, o tensor de entrada e saída tem o formato `(batch, seq, feature)`. Caso contrário, `(seq, batch, feature)`. É altamente recomendado usar `True`.\n",
    "\n",
    "Ele retorna dois tensores:\n",
    "1.  `output`: Contém o estado oculto de saída para **cada** passo de tempo da sequência.\n",
    "2.  `h_n`: Contém o estado oculto **final** do último passo de tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f530a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando o módulo nativo do PyTorch\n",
    "# É importante usar batch_first=True para consistência\n",
    "rnn_pytorch = nn.RNN(input_size, hidden_size, num_layers=1, batch_first=True)\n",
    "\n",
    "# A entrada é a mesma sequência de antes\n",
    "output_pytorch, h_n_pytorch = rnn_pytorch(x_sequence)\n",
    "\n",
    "print(f\"Formato da sequência de entrada: {x_sequence.shape}\")\n",
    "print(f\"Formato da sequência de saída (output): {output_pytorch.shape}\")\n",
    "print(f\"Formato do estado oculto final (h_n): {h_n_pytorch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d3b2f0",
   "metadata": {},
   "source": [
    "## 4. Exemplo 1: Predição de Séries Temporais\n",
    "\n",
    "Neste exemplo, treinaremos uma RNN para prever o próximo valor de uma onda senoidal, baseando-se nos valores anteriores. Esta é uma tarefa de regressão autorregressiva.\n",
    "\n",
    "Primeiro, geramos os dados e os estruturamos em sequências de entrada e seus respectivos alvos. Se a nossa sequência de entrada tem tamanho `L`, o modelo usará `data[0:L]` para prever `data[L]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Parâmetros\n",
    "sequence_length = 20\n",
    "noise_factor = 0.3 # Fator de ruído para simular medições\n",
    "\n",
    "# Gera os dados: uma onda senoidal já está em uma boa escala (~[-1, 1])\n",
    "time_steps = np.linspace(0, 6*np.pi, 100)\n",
    "data = np.sin(time_steps) + np.random.randn(len(time_steps)) * noise_factor\n",
    "\n",
    "# Converte para tensor do PyTorch\n",
    "data_tensor = torch.FloatTensor(data).view(-1, 1)\n",
    "\n",
    "# Plota os dados de entrada\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(time_steps, data, s=10)\n",
    "plt.title(\"Dados de Entrada\")\n",
    "plt.xlabel(\"Tempo\")\n",
    "plt.ylabel(\"Valor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar as sequências de entrada e os rótulos\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences, labels = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length]\n",
    "        label = data[i+seq_length]\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return torch.stack(sequences), torch.stack(labels)\n",
    "\n",
    "# Cria as sequências e divide em treino/teste\n",
    "X, y = create_sequences(data_tensor, sequence_length)\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Cria o DataLoader para o treinamento\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d6e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(TimeSeriesPredictor, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_length, input_size]\n",
    "        # Pegamos a saída do último passo de tempo da RNN\n",
    "        # rnn_out: [batch_size, seq_length, hidden_size]\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        last_time_step_out = rnn_out[:, -1, :]\n",
    "        \n",
    "        # Passamos pela camada linear\n",
    "        # out: [batch_size, output_size]\n",
    "        out = self.fc(last_time_step_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526bcc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o modelo\n",
    "model = TimeSeriesPredictor(input_size=1, hidden_size=4, output_size=1)\n",
    "criterion = nn.MSELoss() # Mean Squared Error para regressão\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef74e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Loop de Treinamento\n",
    "num_epochs = 500\n",
    "train_losses = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for sequences, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward e otimização\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "print(\"Treinamento concluído!\")\n",
    "plt.plot(train_losses, label='Perda de Treino')\n",
    "plt.title(\"Curva de Perda\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb6a81",
   "metadata": {},
   "source": [
    "### Inferência com Geração Autoregressiva\n",
    "\n",
    "Até agora, avaliamos o modelo fazendo previsões de um passo à frente (*one-step-ahead*), ou seja, usando sempre dados reais como entrada. Uma forma mais robusta de avaliar um modelo de série temporal é através da **geração autorregressiva**.\n",
    "\n",
    "Neste processo, utilizamos a saída do próprio modelo como entrada para a próxima predição. Começamos com uma sequência inicial de dados reais (a \"semente\" ou *seed*) e, a partir dela, geramos o restante da série.\n",
    "\n",
    "Matematicamente, se o modelo é uma função $f$ e $L$ é o tamanho da sequência de entrada:\n",
    "1.  A primeira predição $\\hat{y}_{t}$ é baseada em dados reais:\n",
    "    $$ \\hat{y}_{t} = f(x_{t-L}, \\ldots, x_{t-1}) $$\n",
    "2.  A segunda predição $\\hat{y}_{t+1}$ usa a primeira predição $\\hat{y}_{t}$ como parte da nova sequência de entrada:\n",
    "    $$ \\hat{y}_{t+1} = f(x_{t-L+1}, \\ldots, x_{t-1}, \\hat{y}_{t}) $$\n",
    "3.  O processo continua, com cada nova predição sendo adicionada à janela de entrada para a predição subsequente:\n",
    "    $$ \\hat{y}_{t+k} = f(\\ldots, \\hat{y}_{t+k-2}, \\hat{y}_{t+k-1}) $$\n",
    "\n",
    "Este método é um teste mais rigoroso, pois os erros do modelo podem se acumular ao longo do tempo. Uma boa previsão autorregressiva indica que o modelo aprendeu bem a dinâmica subjacente da série."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d667d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geração autorregressiva\n",
    "model.eval()\n",
    "\n",
    "# Pega a primeira sequência do conjunto de teste como semente inicial\n",
    "current_seq = X_test[0]\n",
    "# Número de pontos a serem gerados\n",
    "generation_steps = 100\n",
    "# Lista para armazenar as previsões\n",
    "generated_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(generation_steps):\n",
    "        # Adiciona a dimensão do batch (1, seq_len, input_size)\n",
    "        input_tensor = current_seq.unsqueeze(0)\n",
    "        \n",
    "        # Faz a predição\n",
    "        prediction = model(input_tensor)\n",
    "        \n",
    "        # Armazena a predição\n",
    "        generated_predictions.append(prediction.item())\n",
    "        \n",
    "        # Atualiza a sequência de entrada para a próxima iteração\n",
    "        current_seq = torch.cat((current_seq[1:], prediction), dim=0)\n",
    "\n",
    "# Converte listas e tensores para numpy para plotagem\n",
    "generated_predictions = np.array(generated_predictions)\n",
    "original_data = data_tensor.numpy().flatten()\n",
    "seed_data = X_test[0].numpy().flatten()\n",
    "\n",
    "seed_start_index = train_size\n",
    "seed_end_index = train_size + sequence_length\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 7))\n",
    "\n",
    "# Plot dos dados originais\n",
    "plt.scatter(range(original_data.shape[0]), original_data, color='gray', alpha=0.7)\n",
    "\n",
    "# Plot da geração autorregressiva\n",
    "generated_indices = range(seed_end_index, seed_end_index + generation_steps)\n",
    "plt.plot(generated_indices, generated_predictions, label='Geração Autoregressiva', color='red', linestyle='--')\n",
    "\n",
    "plt.title(\"Geração Autoregressiva com RNN\")\n",
    "plt.xlabel(\"Tempo\")\n",
    "plt.ylabel(\"Valor\")\n",
    "plt.legend()\n",
    "plt.axvline(x=seed_end_index, color='black', linestyle=':', label='Início da Geração')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e04b7ff",
   "metadata": {},
   "source": [
    "## 5. Exemplo 2: Predição do Próximo Caractere\n",
    "\n",
    "Neste exemplo, treinaremos uma RNN para prever o próximo caractere em uma sequência de texto. Esta é uma tarefa de classificação, onde as \"classes\" são todos os caracteres possíveis no nosso vocabulário.\n",
    "\n",
    "O processo envolve:\n",
    "1.  Criar um vocabulário de caracteres únicos.\n",
    "2.  Mapear cada caractere para um número inteiro.\n",
    "3.  Representar a entrada como `one-hot vectors`, que é uma forma de representar dados categóricos.\n",
    "4.  Treinar a rede para, dada uma sequência de caracteres, prever o próximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52960ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preparar os dados\n",
    "text = \"o rato roeu a roupa do rei de roma\"\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Tamanho do vocabulário: {vocab_size}\")\n",
    "print(f\"Vocabulário: {''.join(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a73077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Criar sequências e alvos como listas de inteiros\n",
    "seq_len = 5\n",
    "input_seqs_list = []\n",
    "target_seqs_list = []\n",
    "for i in range(len(text) - seq_len):\n",
    "    in_seq = text[i:i+seq_len]\n",
    "    target = text[i+seq_len]\n",
    "    input_seqs_list.append([char_to_int[ch] for ch in in_seq])\n",
    "    target_seqs_list.append(char_to_int[target])\n",
    "\n",
    "input_seqs_list[0], target_seqs_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d45b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Converter as listas em tensores de índices (não mais one-hot)\n",
    "# A entrada da camada Embedding deve ser do tipo LongTensor\n",
    "X_char_indices = torch.LongTensor(input_seqs_list)\n",
    "y_char_labels = torch.LongTensor(target_seqs_list)\n",
    "\n",
    "print(f\"Formato de X_char_indices (Entrada): {X_char_indices.shape}\")\n",
    "print(f\"Formato de y_char_labels (Alvo): {y_char_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f84d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(CharPredictor, self).__init__()\n",
    "        # Camada de Embedding: Mapeia cada índice de caractere para um vetor denso\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # A entrada da RNN agora é a dimensão do embedding\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "        \n",
    "        # Camada de saída mapeia do espaço oculto para o vocabulário\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len] (tensores de índices)\n",
    "        x = self.embedding(x)\n",
    "        # após embedding, x: [batch_size, seq_len, embedding_dim]\n",
    "        \n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        # rnn_out: [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # Usamos a saída do último passo de tempo para a previsão\n",
    "        out = self.fc(rnn_out[:, -1, :])\n",
    "        # out: [batch_size, vocab_size]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd170de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros\n",
    "embedding_dim = 4\n",
    "hidden_size = 8\n",
    "\n",
    "# Instanciando o modelo\n",
    "model_char = CharPredictor(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_size=hidden_size)\n",
    "criterion_char = nn.CrossEntropyLoss()\n",
    "optimizer_char = torch.optim.Adam(model_char.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0690c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop de Treinamento\n",
    "num_epochs_char = 2000\n",
    "for epoch in range(num_epochs_char):\n",
    "    model_char.train()\n",
    "    # Passa o tensor de índices diretamente para o modelo\n",
    "    outputs = model_char(X_char_indices)\n",
    "    loss = criterion_char(outputs, y_char_labels)\n",
    "    \n",
    "    optimizer_char.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_char.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Época [{epoch+1}/{num_epochs_char}], Perda: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c94af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando texto com o modelo treinado\n",
    "model_char.eval()\n",
    "\n",
    "# Começa com uma sequência aleatória do nosso dataset\n",
    "start_idx = np.random.randint(0, len(input_seqs_list) - 1)\n",
    "seed_seq_int = input_seqs_list[start_idx] # Usa a lista de inteiros\n",
    "seed_seq_text = ''.join([int_to_char[i] for i in seed_seq_int])\n",
    "generated_text = seed_seq_text\n",
    "\n",
    "print(f\"Semente inicial: '{seed_seq_text}'\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(100): # Gerar 50 novos caracteres\n",
    "        # Prepara a sequência de entrada como um LongTensor\n",
    "        input_tensor = torch.LongTensor([seed_seq_int]) # Batch size de 1\n",
    "            \n",
    "        # Obtém a previsão do modelo (logits)\n",
    "        output_logits = model_char(input_tensor)\n",
    "        \n",
    "        # Amostragem para obter o próximo caractere\n",
    "        probabilities = nn.functional.softmax(output_logits, dim=1)\n",
    "        predicted_idx = torch.multinomial(probabilities, 1).item()\n",
    "        \n",
    "        # Adiciona o caractere previsto ao texto gerado\n",
    "        generated_text += int_to_char[predicted_idx]\n",
    "        \n",
    "        # Atualiza a sequência de entrada para a próxima iteração\n",
    "        seed_seq_int = seed_seq_int[1:] + [predicted_idx]\n",
    "\n",
    "print(f\"\\nTexto gerado: '{generated_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcdaae0",
   "metadata": {},
   "source": [
    "### Análise da Inferência: Visualizando as Probabilidades\n",
    "\n",
    "Para entender o que o modelo \"pensa\" em um determinado passo, podemos realizar uma única inferência e visualizar a distribuição de probabilidade que ele atribui a cada caractere do vocabulário para ser o próximo. Isso é feito aplicando a função `softmax` aos *logits* de saída do modelo. O gráfico resultante nos mostrará quais caracteres o modelo considera mais prováveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ac6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para plotar a distribuição de probabilidades\n",
    "def plot_next_char_probabilities(model, seed_text):\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepara a sequência de entrada como um LongTensor\n",
    "    seed_seq_int = [char_to_int[c] for c in seed_text]\n",
    "    input_tensor = torch.LongTensor([seed_seq_int])\n",
    "\n",
    "    # Inferência\n",
    "    with torch.no_grad():\n",
    "        output_logits = model(input_tensor)\n",
    "        probabilities = nn.functional.softmax(output_logits, dim=1).squeeze()\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(chars, probabilities.numpy())\n",
    "    plt.title(f\"Probabilidade do Próximo Caractere após '{seed_text}'\")\n",
    "    plt.ylabel(\"Probabilidade\")\n",
    "    plt.xlabel(\"Caractere\")\n",
    "    plt.show()\n",
    "\n",
    "# Testando com uma sequência do texto\n",
    "seed = \"a roupa do \" \n",
    "plot_next_char_probabilities(model_char, seed)\n",
    "\n",
    "# Outro teste\n",
    "seed = \"pa do rei \" \n",
    "plot_next_char_probabilities(model_char, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3b4c00",
   "metadata": {},
   "source": [
    "### Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf055d",
   "metadata": {},
   "source": [
    "#### Exercício 1: Alterando a Função de Ativação\n",
    "\n",
    "Modifique a classe `SimpleRNNCell` (a nossa implementação \"do zero\") para utilizar a função de ativação `nn.ReLU` em vez de `nn.Tanh`.\n",
    "\n",
    "-   Depois da modificação, execute a célula novamente.\n",
    "-   **Pergunta:** Qual a principal diferença no intervalo de valores que o estado oculto (`h_t`) pode assumir ao usar ReLU em comparação com Tanh? Como você acha que isso poderia impactar o treinamento de uma rede mais profunda?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a5fd6",
   "metadata": {},
   "source": [
    "#### Exercício 2: Otimizando o Previsor de Série Temporal\n",
    "\n",
    "Experimente com os hiperparâmetros do modelo `TimeSeriesPredictor` para tentar obter uma geração autorregressiva mais precisa. Altere um de cada vez e analise o resultado:\n",
    "\n",
    "1.  **Tamanho da Camada Oculta (`hidden_size`):** Teste um valor menor e um valor maior.\n",
    "2.  **Tamanho da Sequência (`sequence_length`):** Altere o tamanho da janela de entrada para um valor menor e um maior. Você precisará recriar os dados para isso.\n",
    "3.  **Taxa de Aprendizado (`learning_rate`):** Teste um valor uma ordem de magnitude menor (ex: 0.001).\n",
    "\n",
    "-   **Análise:** Para cada mudança, observe a curva de perda e o gráfico de geração. Um modelo maior é sempre melhor? Qual o impacto de ver mais (ou menos) do passado da série?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd1cea",
   "metadata": {},
   "source": [
    "#### Exercício 3: Aprofundando o Modelo\n",
    "\n",
    "Modifique a arquitetura do `TimeSeriesPredictor` para usar duas camadas de RNN empilhadas.\n",
    "\n",
    "-   **Dica:** A classe `nn.RNN` possui um parâmetro `num_layers`. Basta ajustá-lo.\n",
    "-   **Pergunta:** O modelo com duas camadas converge mais rápido? A qualidade da geração autorregressiva melhora, piora ou permanece a mesma?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26d0e6f",
   "metadata": {},
   "source": [
    "#### Exercício 4: Controlando a Criatividade com Temperatura\n",
    "\n",
    "Modifique a célula de geração de texto para incluir um parâmetro de **temperatura** na amostragem, que controla a aleatoriedade da geração. A ideia é dividir os *logits* pela temperatura antes de aplicar a função `softmax`.\n",
    "\n",
    "-   **Explicação:**\n",
    "    -   `temperatura < 1.0`: Torna a distribuição de probabilidade mais \"afiada\", favorecendo os caracteres mais prováveis. O texto fica mais previsível e repetitivo.\n",
    "    -   `temperatura > 1.0`: Torna a distribuição mais \"suave\", aumentando a chance de caracteres menos prováveis serem escolhidos. O texto fica mais \"criativo\", mas também com mais erros.\n",
    "\n",
    "-   **Tarefa:** Gere textos com pelo menos 3 temperaturas diferentes e compare os resultados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
