{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8441f52a",
      "metadata": {
        "id": "8441f52a"
      },
      "source": [
        "# Laboratório 5: MC com inícios exploratórios"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4298bf0e",
      "metadata": {
        "id": "4298bf0e"
      },
      "source": [
        "## Importações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4786f23b",
      "metadata": {
        "id": "4786f23b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.colors import ListedColormap\n",
        "import seaborn as sns\n",
        "from typing import Dict, Tuple, List, Union, Optional, Set\n",
        "from numpy import linalg as LA\n",
        "from collections import defaultdict\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8610559",
      "metadata": {
        "id": "e8610559"
      },
      "source": [
        "## Ambiente: Navegação no Labirinto (gridworld)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2766bd79",
      "metadata": {
        "id": "2766bd79"
      },
      "outputs": [],
      "source": [
        "# Ambiente: Navegação no Labirinto (gridworld)\n",
        "class AmbienteNavegacaoLabirinto:\n",
        "    \"\"\"\n",
        "    Navegação no Labirinto (gridworld) determinístico para experimentos de aprendizado por reforço.\n",
        "\n",
        "    Política de recompensa:\n",
        "    - A recompensa é calculada com base na tentativa de movimento (s, a -> posição proposta), antes de aplicar rebote por sair da grade ou por entrada proibida em bad_states.\n",
        "    - Se a tentativa mira fora da grade: r_boundary\n",
        "    - Se mira um bad_state: r_bad\n",
        "    - Se mira um target_state: r_target\n",
        "    - Caso contrário: r_other\n",
        "\n",
        "    Terminação:\n",
        "    - Por padrão, o ambiente não encerra episódios automaticamente (target_states/bad_states não são estados terminais).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        world_size: Tuple[int, int],\n",
        "        bad_states: List[Tuple[int, int]],\n",
        "        target_states: List[Tuple[int, int]],\n",
        "        allow_bad_entry: bool = False,\n",
        "        rewards: Optional[List[float]] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Inicializa o ambiente de navegação em labirinto.\n",
        "\n",
        "        Parâmetros\n",
        "        ----------\n",
        "        world_size : (n_linhas, n_colunas)\n",
        "            Dimensões da grade.\n",
        "        bad_states : lista de (linha, coluna)\n",
        "            Coordenadas dos estados com penalidade.\n",
        "        target_states : lista de (linha, coluna)\n",
        "            Coordenadas dos estados-alvo.\n",
        "        allow_bad_entry : bool\n",
        "            Se False, impede entrada em bad_states (rebote para o estado atual).\n",
        "        rewards : [r_boundary, r_bad, r_target, r_other]\n",
        "            Recompensas possíveis. Se None, usa [-1, -1, 1, 0].\n",
        "        \"\"\"\n",
        "        if rewards is None:\n",
        "            rewards = [-1, -1, 1, 0]\n",
        "\n",
        "        self.n_rows, self.n_cols = world_size   # dimensões da grade do labirinto\n",
        "        self.bad_states = set(bad_states)       # estados com penalidade alta\n",
        "        self.target_states = set(target_states) # estados com recompensa alta\n",
        "        self.allow_bad_entry = allow_bad_entry  # se o agente pode entrar em estados ruins\n",
        "\n",
        "        # Validações\n",
        "        for st in self.bad_states | self.target_states:\n",
        "            if not (0 <= st[0] < self.n_rows and 0 <= st[1] < self.n_cols):\n",
        "                raise ValueError(f\"Estado {st} fora dos limites.\")\n",
        "        if self.bad_states & self.target_states:\n",
        "            raise ValueError(\"bad_states e target_states devem ser disjuntos.\")\n",
        "\n",
        "        # Recompensas definidas para cada tipo de transição\n",
        "        self.r_boundary = rewards[0]    # tentar sair da grade\n",
        "        self.r_bad      = rewards[1]    # transição para estado ruim\n",
        "        self.r_target   = rewards[2]    # transição para estado alvo\n",
        "        self.r_other    = rewards[3]    # demais transições\n",
        "\n",
        "        # Espaço de ações: dicionário com deslocamentos (linha, coluna)\n",
        "        self.action_space = {\n",
        "            0: (-1, 0),  # cima\n",
        "            1: (1, 0),   # baixo\n",
        "            2: (0, -1),  # esquerda\n",
        "            3: (0, 1),   # direita\n",
        "            4: (0, 0)    # permanecer no mesmo estado\n",
        "        }\n",
        "\n",
        "        # Espaço de recompensas: lista de recompensas possíveis\n",
        "        self.recompensas_possiveis = np.array(sorted(set(rewards)))\n",
        "        self.reward_map = {r: i for i, r in enumerate(self.recompensas_possiveis)}\n",
        "\n",
        "        # número total de estados\n",
        "        self.n_states = self.n_rows * self.n_cols\n",
        "\n",
        "        # número total de ações\n",
        "        self.n_actions = len(self.action_space)\n",
        "\n",
        "        # número total de recompensas possíveis\n",
        "        self.n_rewards = self.recompensas_possiveis.shape[0]\n",
        "\n",
        "        # Tensor de probabilidades de transição: P(s'|s,a)\n",
        "        self.state_transition_probabilities = np.zeros((self.n_states, self.n_states, self.n_actions))\n",
        "\n",
        "        # Tensor de probabilidade de recompensas: P(r|s,a)\n",
        "        self.reward_probabilities = np.zeros((self.n_rewards, self.n_states, self.n_actions))\n",
        "\n",
        "        # Matriz de recompensa imediata (determinística): recompensa[s, a] = r\n",
        "        self.recompensas_imediatas = np.zeros((self.n_states, self.n_actions))\n",
        "\n",
        "        # Matriz de transição de estados (determinística): transicao[s, a] = s'\n",
        "        self.transicao_de_estados = np.zeros((self.n_states, self.n_actions), dtype=int)\n",
        "\n",
        "        self.agent_pos = (0, 0)  # posição inicial do agente\n",
        "\n",
        "        self._init_dynamics()  # inicializa as dinâmicas de transição e recompensa\n",
        "\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (f\"AmbienteNavegacaoLabirinto({self.n_rows}x{self.n_cols}, \"\n",
        "                f\"bad={len(self.bad_states)}, target={len(self.target_states)}, \"\n",
        "                f\"allow_bad_entry={self.allow_bad_entry}, agent_pos={self.agent_pos})\")\n",
        "\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return self.render(as_string=True)\n",
        "\n",
        "\n",
        "    def reset(self) -> Tuple[int, int]:\n",
        "        \"\"\"\n",
        "        Reinicia a posição do agente para o estado inicial (0, 0).\n",
        "\n",
        "        Retorna\n",
        "        -------\n",
        "        (linha, coluna) : posição inicial do agente.\n",
        "        \"\"\"\n",
        "        self.agent_pos = (0, 0)\n",
        "        return self.agent_pos\n",
        "\n",
        "\n",
        "    def step(self, acao: int, *, linear: bool = False) -> Tuple[Union[int, Tuple[int, int]], float]:\n",
        "        \"\"\"\n",
        "        Executa uma ação, atualiza a posição do agente e devolve o próximo estado.\n",
        "\n",
        "        Parâmetros\n",
        "        ----------\n",
        "        acao : int\n",
        "            Índice da ação (0-4).\n",
        "        linear : bool, keyword-only (default=False)\n",
        "            Se True, retorna o estado como índice linear {0,...,self.n_states-1}; caso contrário, retorna tupla (linha, coluna).\n",
        "\n",
        "        Retorna\n",
        "        -------\n",
        "        proximo_estado : int | (int, int)\n",
        "            Estado observado após a transição (com possível rebote).\n",
        "        recompensa : float\n",
        "            Recompensa imediata recebida.\n",
        "        \"\"\"\n",
        "        estado_atual = self.agent_pos                                           # armazena o estado atual do agente\n",
        "        proposta = self._proposta(estado_atual, acao)                           # calcula a posição proposta pela ação\n",
        "        recompensa = self._compute_reward(proposta)                             # avalia a recompensa com base na tentativa\n",
        "        destino = self._destino_final(estado_atual, acao)                       # aplica regras e obtém o destino após possíveis rebotes\n",
        "        self.agent_pos = destino                                                # atualiza a posição interna do agente\n",
        "        proximo_estado = self.state_to_index(destino) if linear else destino    # escolhe o formato da observação de saída\n",
        "        return proximo_estado, recompensa                                       # retorna observação e recompensa\n",
        "\n",
        "\n",
        "    def reset_to_state(self, estado: Union[Tuple[int, int], int], verificar_validade_estado: bool = True) -> Tuple[int, int]:\n",
        "        \"\"\"\n",
        "        Teleporta o agente para `estado` sem reiniciar o episódio completo.\n",
        "\n",
        "        Parâmetros\n",
        "        ----------\n",
        "        estado : (linha, coluna) | int\n",
        "            Tupla de coordenadas (linha, coluna) ou índice linear (int).\n",
        "        verificar_validade_estado : bool\n",
        "            Se True, lança ValueError se o estado for inválido.\n",
        "\n",
        "        Retorna\n",
        "        -------\n",
        "        (linha, coluna) : nova posição do agente.\n",
        "        \"\"\"\n",
        "        # Converte índice -> tupla, se necessário\n",
        "        if isinstance(estado, int):\n",
        "            estado = self.index_to_state(estado)\n",
        "\n",
        "        if verificar_validade_estado and not self._in_bounds(estado):\n",
        "            raise ValueError(f\"Estado {estado} fora dos limites do labirinto.\")\n",
        "\n",
        "        self.agent_pos = tuple(estado)      # mantém tupla imutável\n",
        "\n",
        "        return self.agent_pos\n",
        "\n",
        "\n",
        "    def is_bad(self, state: Union[int, Tuple[int, int]]) -> bool:\n",
        "        \"\"\"Retorna True se o estado for um bad_state.\"\"\"\n",
        "        if isinstance(state, int):\n",
        "            state = self.index_to_state(state)\n",
        "        return state in self.bad_states\n",
        "\n",
        "\n",
        "    def is_target(self, state: Union[int, Tuple[int, int]]) -> bool:\n",
        "        \"\"\"Retorna True se o estado for um target_state.\"\"\"\n",
        "        if isinstance(state, int):\n",
        "            state = self.index_to_state(state)\n",
        "        return state in self.target_states\n",
        "\n",
        "\n",
        "    def state_to_index(self, estado: Tuple[int, int]) -> int:\n",
        "        \"\"\"\n",
        "        Converte coordenada (linha, coluna) para índice linear no intervalo [0, n_states-1].\n",
        "        \"\"\"\n",
        "        linha, coluna = estado\n",
        "        return linha * self.n_cols + coluna\n",
        "\n",
        "\n",
        "    def index_to_state(self, indice: int) -> Tuple[int, int]:\n",
        "        \"\"\"\n",
        "        Converte índice linear  no intervalo [0, n_states-1] para coordenada (linha, coluna).\n",
        "        \"\"\"\n",
        "        return divmod(indice, self.n_cols)  # (linha, coluna) = (indice // self.n_cols, indice % self.n_cols)\n",
        "\n",
        "\n",
        "    def enumerate_states(self) -> List[int]:\n",
        "        \"\"\"Retorna a lista de índices lineares de todos os estados [0, ..., n_states - 1].\"\"\"\n",
        "        return list(range(self.n_states))\n",
        "\n",
        "\n",
        "    def enumerate_actions(self) -> List[int]:\n",
        "        \"\"\"Retorna a lista de índices das ações disponíveis [0, ..., n_actions - 1].\"\"\"\n",
        "        return list(self.action_space.keys())\n",
        "\n",
        "\n",
        "    def render(\n",
        "        self,\n",
        "        *,\n",
        "        as_string: bool = True,\n",
        "        show_coords: bool = False,\n",
        "        legend: bool = True,\n",
        "        chars: dict | None = None\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Renderização ASCII do grid.\n",
        "        - A: agente, B: bad, T: target, .: vazio\n",
        "        - show_coords: mostra índices de linha/coluna\n",
        "        - legend: inclui legenda ao final\n",
        "        - chars: permite customizar símbolos (keys: 'agent','bad','target','empty')\n",
        "        \"\"\"\n",
        "        if chars is None:\n",
        "            chars = {\"agent\": \"A\", \"bad\": \"B\", \"target\": \"T\", \"empty\": \".\"}\n",
        "\n",
        "        linhas = []\n",
        "\n",
        "        # cabeçalho de colunas\n",
        "        if show_coords:\n",
        "            header = \"    \" + \" \".join(f\"{c:2d}\" for c in range(self.n_cols))\n",
        "            linhas.append(header)\n",
        "            linhas.append(\"    \" + \"--\" * self.n_cols)\n",
        "\n",
        "        # monta o grid linha a linha\n",
        "        for r in range(self.n_rows):\n",
        "            row_syms = []\n",
        "            for c in range(self.n_cols):\n",
        "                sym = chars[\"empty\"]\n",
        "                if (r, c) in self.bad_states:\n",
        "                    sym = chars[\"bad\"]\n",
        "                if (r, c) in self.target_states:\n",
        "                    sym = chars[\"target\"]\n",
        "                if self.agent_pos == (r, c):\n",
        "                    sym = chars[\"agent\"]\n",
        "                row_syms.append(sym)\n",
        "\n",
        "            linha_str = \" \".join(row_syms)\n",
        "            if show_coords:\n",
        "                linhas.append(f\"{r:2d} | {linha_str}\")\n",
        "            else:\n",
        "                linhas.append(linha_str)\n",
        "\n",
        "        # legenda\n",
        "        if legend:\n",
        "            linhas.append(\"\")\n",
        "            linhas.append(f\"Legenda: {chars['agent']}=agente, {chars['bad']}=bad, \"\n",
        "                        f\"{chars['target']}=target, {chars['empty']}=vazio\")\n",
        "\n",
        "        out = \"\\n\".join(linhas)\n",
        "        return out if as_string else print(out)\n",
        "\n",
        "\n",
        "    def plot_labirinto(self, ax=None, titulo: str = \"Visualização do Labirinto\", cbar: bool = False):\n",
        "        \"\"\"\n",
        "        Visualiza o labirinto.\n",
        "\n",
        "        Representa:\n",
        "        - Estado neutro: branco\n",
        "        - Estado ruim: vermelho\n",
        "        - Estado alvo: verde\n",
        "\n",
        "        Parâmetros\n",
        "        ----------\n",
        "        ax : matplotlib.axes.Axes, opcional\n",
        "            Eixo onde desenhar. Se None, cria uma nova figura.\n",
        "        titulo : str, opcional\n",
        "            Título do gráfico.\n",
        "        cbar : bool, opcional (default=False)\n",
        "            Exibe (True) ou oculta (False) a barra de cores.\n",
        "\n",
        "        Retorna\n",
        "        -------\n",
        "        ax : matplotlib.axes.Axes\n",
        "            Eixo com o heatmap.\n",
        "        \"\"\"\n",
        "        # Cria matriz com valores padrão (0 = neutro)\n",
        "        matriz = np.zeros((self.n_rows, self.n_cols), dtype=int)\n",
        "\n",
        "        # marca estados\n",
        "        for (r, c) in self.bad_states:\n",
        "            matriz[r, c] = 1   # ruim\n",
        "        for (r, c) in self.target_states:\n",
        "            matriz[r, c] = 2   # alvo\n",
        "\n",
        "        # cores: branco=neutro, vermelho=ruim, verde=alvo\n",
        "        cmap = ListedColormap([\"white\", \"red\", \"green\"])\n",
        "\n",
        "        fig = None\n",
        "        if ax is None:\n",
        "            fig, ax = plt.subplots(figsize=(self.n_cols, self.n_rows))\n",
        "\n",
        "        ax = sns.heatmap(\n",
        "            matriz,\n",
        "            cmap=cmap,\n",
        "            cbar=cbar,\n",
        "            linewidths=0.5,\n",
        "            linecolor=\"gray\",\n",
        "            square=True,\n",
        "            ax=ax\n",
        "        )\n",
        "\n",
        "        # remove ticks/labels\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.set_xticklabels([]); ax.set_yticklabels([])\n",
        "\n",
        "        # bordas externas\n",
        "        for side in (\"left\", \"right\", \"top\", \"bottom\"):\n",
        "            ax.spines[side].set_visible(True)\n",
        "            ax.spines[side].set_linewidth(0.5)\n",
        "            ax.spines[side].set_edgecolor(\"gray\")\n",
        "\n",
        "        ax.set_title(titulo)\n",
        "\n",
        "        if fig is not None:\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def _init_dynamics(self):\n",
        "        \"\"\"\n",
        "        Preenche as matrizes de transição e recompensa com base na estrutura do ambiente e regras de movimentação.\n",
        "        \"\"\"\n",
        "\n",
        "        self.recompensas_imediatas.fill(0.0)\n",
        "        self.transicao_de_estados.fill(0)\n",
        "        self.state_transition_probabilities.fill(0.0)\n",
        "        self.reward_probabilities.fill(0.0)\n",
        "\n",
        "        for s in self.enumerate_states():                                   # percorre todos os estados (índices lineares)\n",
        "            estado_atual = self.index_to_state(s)                           # converte índice para (linha, coluna)\n",
        "            for a in self.enumerate_actions():                              # percorre todas as ações disponíveis\n",
        "                proposta = self._proposta(estado_atual, a)                  # calcula a posição proposta pela ação\n",
        "                r = self._compute_reward(proposta)                          # avalia a recompensa da tentativa de movimento\n",
        "                destino = self._destino_final(estado_atual, a)              # obtém destino após aplicar regras de rebote\n",
        "                s_next = self.state_to_index(destino)                       # converte destino para índice linear\n",
        "\n",
        "                self.recompensas_imediatas[s, a] = r                        # registra r(s, a) na matriz de recompensas imediatas\n",
        "                self.transicao_de_estados[s, a] = s_next                    # registra T(s, a) = s' na matriz de transições\n",
        "\n",
        "                self.state_transition_probabilities[s_next, s, a] = 1.0     # define P(s'|s, a) = 1 (ambiente determinístico)\n",
        "                self.reward_probabilities[self.reward_map[r], s, a] = 1.0   # define P(r |s, a) = 1 (ambiente determinístico)\n",
        "\n",
        "\n",
        "    def _proposta(self, state: Tuple[int, int], acao: int) -> Tuple[int, int]:\n",
        "        \"\"\"Retorna a posição proposta (antes de qualquer rebote).\"\"\"\n",
        "        dl, dc = self.action_space[acao]\n",
        "        return (state[0] + dl, state[1] + dc)\n",
        "\n",
        "\n",
        "    def _destino_final(self, state: Tuple[int, int], acao: int) -> Tuple[int, int]:\n",
        "        \"\"\"\n",
        "        Aplica as regras de rebote: fora da grade => rebote;\n",
        "        bad_state com allow_bad_entry=False => rebote; caso contrário segue para a proposta.\n",
        "        \"\"\"\n",
        "        proposta = self._proposta(state, acao)\n",
        "        if not self._in_bounds(proposta):\n",
        "            return state\n",
        "        if (not self.allow_bad_entry) and self.is_bad(proposta):\n",
        "            return state\n",
        "        return proposta\n",
        "\n",
        "\n",
        "    def _in_bounds(self, posicao: Tuple[int, int]) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se uma posição está dentro dos limites do labirinto.\n",
        "\n",
        "        Parâmetros\n",
        "        ----------\n",
        "        posicao : (linha, coluna)\n",
        "\n",
        "        Retorna\n",
        "        -------\n",
        "        bool : True se dentro da grade, False caso contrário.\n",
        "        \"\"\"\n",
        "        linha, coluna = posicao\n",
        "        return 0 <= linha < self.n_rows and 0 <= coluna < self.n_cols\n",
        "\n",
        "\n",
        "    def _compute_reward(self, destino: Tuple[int, int]) -> float:\n",
        "        \"\"\"\n",
        "        Calcula a recompensa da tentativa de transição para 'destino'.\n",
        "\n",
        "        Regras\n",
        "        ------\n",
        "        - Se 'destino' está fora da grade: r_boundary\n",
        "        - Se 'destino' é bad_state: r_bad\n",
        "        - Se 'destino' é target_state: r_target\n",
        "        - Caso contrário: r_other\n",
        "        \"\"\"\n",
        "        if not self._in_bounds(destino):\n",
        "            return self.r_boundary\n",
        "        elif self.is_bad(destino):\n",
        "            return self.r_bad\n",
        "        elif self.is_target(destino):\n",
        "            return self.r_target\n",
        "        else:\n",
        "            return self.r_other"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e100bbcf",
      "metadata": {
        "id": "e100bbcf"
      },
      "source": [
        "## Funções auxiliares para visualização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eedf6090",
      "metadata": {
        "id": "eedf6090"
      },
      "outputs": [],
      "source": [
        "def _prepare_grid(env, ax=None, draw_cells=True):\n",
        "    \"\"\"\n",
        "    Configura o grid. Se 'ax' não for passado, cria 'fig, ax'; caso contrário retorna 'fig=None, ax'.\n",
        "    \"\"\"\n",
        "    fig = None\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(env.n_cols, env.n_rows))\n",
        "\n",
        "    ax.set_xlim(0, env.n_cols)\n",
        "    ax.set_ylim(0, env.n_rows)\n",
        "    ax.set_xticks(np.arange(0, env.n_cols + 1, 1))\n",
        "    ax.set_yticks(np.arange(0, env.n_rows + 1, 1))\n",
        "    ax.grid(True)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "    if draw_cells:\n",
        "        for r in range(env.n_rows):\n",
        "            for c in range(env.n_cols):\n",
        "                cell = (r, c)\n",
        "                if cell in env.bad_states:\n",
        "                    color = 'red'\n",
        "                elif cell in env.target_states:\n",
        "                    color = 'green'\n",
        "                else:\n",
        "                    color = 'white'\n",
        "                rect = patches.Rectangle((c, r), 1, 1, facecolor=color, edgecolor='gray')\n",
        "                ax.add_patch(rect)\n",
        "\n",
        "    return fig, ax\n",
        "\n",
        "def _coerce_policy(env, policy):\n",
        "    \"\"\"\n",
        "    Normaliza a política para o formato dict[(r,c)] -> ação (int).\n",
        "    Aceita:\n",
        "      - dict[(r,c)] -> ação\n",
        "      - dict[(r,c)] -> vetor de probabilidades\n",
        "      - Pi ndarray (n_estados, n_acoes)\n",
        "    \"\"\"\n",
        "    # caso 1: matriz Pi (ndarray)\n",
        "    if isinstance(policy, np.ndarray):\n",
        "        a_star = np.argmax(policy, axis=1)\n",
        "        return {env.index_to_state(s): int(a_star[s]) for s in range(env.n_states)}\n",
        "\n",
        "    # caso 2: dicionário\n",
        "    sample_val = next(iter(policy.values()))\n",
        "    if isinstance(sample_val, np.ndarray):\n",
        "        return {pos: int(np.argmax(probs)) for pos, probs in policy.items()}\n",
        "    else:\n",
        "        return policy\n",
        "\n",
        "def plot_policy(env, policy, ax=None, titulo=\"Política\"):\n",
        "    \"\"\"\n",
        "    Desenha setas/círculos de uma política. 'policy' pode ser:\n",
        "      - dict[(r,c)] -> ação\n",
        "      - dict[(r,c)] -> vetor de probabilidades\n",
        "      - ndarray com shape (n_estados, n_acoes)\n",
        "    \"\"\"\n",
        "    fig, ax = _prepare_grid(env, ax=ax)\n",
        "\n",
        "    policy_dict = _coerce_policy(env, policy)\n",
        "    color = 'black'\n",
        "    lw = 1.5\n",
        "\n",
        "    for (r, c), action in policy_dict.items():\n",
        "        x, y = c + 0.5, r + 0.5\n",
        "        if action == 0:      # cima\n",
        "            ax.arrow(x, y, dx=0, dy=-0.3, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
        "        elif action == 1:    # baixo\n",
        "            ax.arrow(x, y, dx=0, dy=0.3, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
        "        elif action == 2:    # esquerda\n",
        "            ax.arrow(x, y, dx=-0.3, dy=0, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
        "        elif action == 3:    # direita\n",
        "            ax.arrow(x, y, dx=0.3, dy=0, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n",
        "        elif action == 4:    # ficar\n",
        "            circ = patches.Circle((x, y), 0.1, edgecolor=color, facecolor='none', linewidth=lw)\n",
        "            ax.add_patch(circ)\n",
        "\n",
        "    ax.set_title(titulo)\n",
        "    if fig is not None:\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    return ax\n",
        "\n",
        "def plot_tabular(\n",
        "    data,\n",
        "    kind: str = \"Q\",          # \"Q\" (valores de ação), \"Pi\" (política), \"V\" (valores de estado), \"VISITAS\" (número de visitas)\n",
        "    ambiente=None,            # necessário quando kind=\"V\" para reshape\n",
        "    ax=None,\n",
        "    cbar: bool = True,\n",
        "    fmt: str = \".1f\",\n",
        "    center_zero: bool = True  # só relevante para \"Q\" e \"V\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Plota matrizes tabulares de RL em formato de heatmaps (mapas de calor).\n",
        "    Esta função cobre 3 casos:\n",
        "    1. kind=\"Q\": heatmap de Q(s, a) com ações nas linhas e estados nas colunas.\n",
        "    2. kind=\"Pi\": heatmap de Pi(a|s) (probabilidades) com ações nas linhas e estados nas colunas.\n",
        "    3. kind=\"V\": heatmap de V(s) no grid (n_rows x n_cols) do ambiente.\n",
        "    4. kind=\"VISITAS\": heatmap do número de visitas visitas(s,a) com ações nas linhas e estados nas colunas.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : ndarray\n",
        "        Dados a serem plotados.\n",
        "        - Para kind=\"Q\", \"Pi\" ou \"VISITAS\": array 2D com shape (n_estados, n_acoes).\n",
        "        - Para kind=\"V\": array 1D com shape (n_estados,) que será remodelado para (ambiente.n_rows, ambiente.n_cols).\n",
        "    kind : {\"Q\", \"Pi\", \"V\", \"VISITAS\"}, default=\"Q\"\n",
        "        Tipo do plot:\n",
        "        - \"Q\" usa paleta divergente centrada em zero.\n",
        "        - \"Pi\" usa paleta sequencial no intervalo [0, 1].\n",
        "        - \"V\" plota o valor de estado no grid do ambiente.\n",
        "    ambiente : object, optional\n",
        "        Necessário quando kind=\"V\". Deve expor n_rows e n_cols para o reshape.\n",
        "    ax : matplotlib.axes.Axes, optional\n",
        "        Eixo onde o heatmap será desenhado. Se None, uma nova figura/eixo é criado.\n",
        "    cbar : bool, default=True\n",
        "        Se True, exibe a barra de cores (colorbar).\n",
        "    fmt : str, default=\".1f\"\n",
        "        Formatação dos valores anotados em cada célula do heatmap.\n",
        "    center_zero : bool, default=True\n",
        "        Quando kind é \"Q\" ou \"V\", centraliza a escala de cores em zero (vmin=-absmax, vmax=absmax). Ignorado para \"Pi\".\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ax : matplotlib.axes.Axes\n",
        "        Eixo contendo o heatmap resultante.\n",
        "    \"\"\"\n",
        "    kind = kind.upper()\n",
        "\n",
        "    xlabel = {\"V\": \"Colunas\", \"PI\": \"Estados\", \"Q\": \"Estados\", \"VISITAS\": \"Estados\"}\n",
        "    ylabel = {\"V\": \"Linhas\", \"PI\": \"Ações\", \"Q\": \"Ações\", \"VISITAS\": \"Ações\"}\n",
        "    title  = {\"V\": \"Valores de Estado (V(s))\",\n",
        "              \"PI\": r\"Política ($\\pi(a|s)$ transposta)\",\n",
        "              \"Q\": \"Valores de ação (Q(s, a) transposta)\",\n",
        "              \"VISITAS\":  \"Número de visitas visitas(s, a) (transposta)\"}\n",
        "\n",
        "    fig = None\n",
        "\n",
        "    #  V(s): precisa do shape do grid\n",
        "    match kind:\n",
        "        case \"V\":\n",
        "\n",
        "            if ambiente is None:\n",
        "                raise ValueError(\"Para kind='V', passe 'ambiente' para reshape (n_rows, n_cols).\")\n",
        "\n",
        "            M = data.reshape(ambiente.n_rows, ambiente.n_cols)\n",
        "\n",
        "            if ax is None:\n",
        "                fig, ax = plt.subplots(figsize=(ambiente.n_cols, ambiente.n_rows))\n",
        "\n",
        "            if center_zero:\n",
        "                vmax = float(np.abs(M).max())\n",
        "                vmin = -vmax\n",
        "            else:\n",
        "                vmin = float(M.min())\n",
        "                vmax = float(M.max())\n",
        "\n",
        "            cmap, square = \"bwr\", True\n",
        "\n",
        "        case \"PI\" | \"Q\":\n",
        "\n",
        "            # Q(s,a) e Pi(a|s): ações nas linhas, estados nas colunas\n",
        "            M = data.T  # data: (n_estados, n_acoes) -> transposto para (n_acoes, n_estados)\n",
        "            n_acoes, n_estados = M.shape\n",
        "\n",
        "            if ax is None:\n",
        "                fig, ax = plt.subplots(figsize=(n_estados, n_acoes))\n",
        "\n",
        "            if kind == \"PI\":\n",
        "                cmap = \"Blues\";\n",
        "                vmin, vmax = 0.0, 1.0\n",
        "            else:  # \"Q\"\n",
        "                cmap = \"bwr\"\n",
        "                if center_zero:\n",
        "                    vmax = float(np.abs(M).max())\n",
        "                    vmin = -vmax\n",
        "                else:\n",
        "                    vmin = float(M.min())\n",
        "                    vmax = float(M.max())\n",
        "\n",
        "            square = False\n",
        "\n",
        "        case \"VISITAS\":\n",
        "            # visitas(s,a): visitas, ações nas linhas, estados nas colunas\n",
        "            M = data.T\n",
        "            n_acoes, n_estados = M.shape\n",
        "\n",
        "            if ax is None:\n",
        "                fig, ax = plt.subplots(figsize=(n_estados, n_acoes))\n",
        "\n",
        "            cmap = \"Oranges\"     # paleta sequencial para contagens\n",
        "            vmin = 0.0\n",
        "            vmax = float(M.max()) if M.max() > 0 else 1.0\n",
        "            square = False\n",
        "\n",
        "        case _:\n",
        "            raise ValueError(f\"kind desconhecido: {kind!r} (use 'Q', 'Pi' ou 'V').\")\n",
        "\n",
        "\n",
        "    ax = sns.heatmap(\n",
        "        data=M,\n",
        "        annot=True,\n",
        "        fmt=fmt,\n",
        "        cmap=cmap,\n",
        "        vmin=vmin,\n",
        "        vmax=vmax,\n",
        "        cbar=cbar,\n",
        "        square=square,\n",
        "        linewidths=0.5,\n",
        "        linecolor=\"gray\",\n",
        "        ax=ax\n",
        "    )\n",
        "\n",
        "    ax.set_xlabel(xlabel[kind])\n",
        "    ax.set_ylabel(ylabel[kind])\n",
        "    ax.set_title(title[kind])\n",
        "\n",
        "    # bordas externas\n",
        "    for side in (\"left\", \"right\", \"top\", \"bottom\"):\n",
        "        ax.spines[side].set_visible(True)\n",
        "        ax.spines[side].set_linewidth(0.5)\n",
        "        ax.spines[side].set_edgecolor(\"gray\")\n",
        "\n",
        "    # rótulos\n",
        "    if kind in (\"Q\", \"PI\", \"VISITAS\"):\n",
        "        ax.set_xticks(np.arange(n_estados) + 0.5)\n",
        "        ax.set_xticklabels([f\"s{i}\" for i in range(n_estados)], rotation=0)\n",
        "        ax.set_yticks(np.arange(n_acoes) + 0.5)\n",
        "        ax.set_yticklabels([f\"a{i}\" for i in range(n_acoes)], rotation=0)\n",
        "\n",
        "    if fig is not None:\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def plot_visitas_log(n_visitas):\n",
        "    \"\"\"\n",
        "    Gera um gráfico de dispersão com escala logarítmica no eixo y\n",
        "    mostrando o número de visitas para cada par (s,a).\n",
        "\n",
        "    Parâmetros\n",
        "    ----------\n",
        "    n_visitas : np.ndarray\n",
        "        Matriz de número de visitas de shape (n_states, n_actions).\n",
        "    \"\"\"\n",
        "    n_states, n_actions = n_visitas.shape\n",
        "    x = np.arange(n_states * n_actions)  # índice linear do par (s,a)\n",
        "    y = n_visitas.flatten()              # número de visitas\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.scatter(x, y, s=10, alpha=0.7)\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel(\"Índice linear do par (s,a)\")\n",
        "    plt.ylabel(\"Número de visitas ao par (s,a)\")\n",
        "    plt.title(\"Frequência de visitas (escala log)\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cb6fb51",
      "metadata": {
        "id": "0cb6fb51"
      },
      "source": [
        "## Ambiente: nova instância"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fe0fe5c",
      "metadata": {
        "id": "3fe0fe5c"
      },
      "outputs": [],
      "source": [
        "ambiente = AmbienteNavegacaoLabirinto(\n",
        "        world_size=(5, 5),\n",
        "        bad_states=[(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)],\n",
        "        target_states=[(3, 2)],\n",
        "        allow_bad_entry=True,\n",
        "        rewards=[-1, -10, 1, 0]\n",
        "    )\n",
        "ambiente.plot_labirinto()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "933b9c6e",
      "metadata": {
        "id": "933b9c6e"
      },
      "source": [
        "## Algoritmo: MC com inícios exploratórios"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e879cc53",
      "metadata": {
        "id": "e879cc53"
      },
      "source": [
        "### Episódio Monte-Carlo com horizonte fixo T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df1900ce",
      "metadata": {
        "id": "df1900ce"
      },
      "outputs": [],
      "source": [
        "def gerar_episodio(\n",
        "    ambiente,\n",
        "    estado_inicial: int,\n",
        "    acao_inicial: int,\n",
        "    Pi: np.ndarray,\n",
        "    T: int,\n",
        ") -> List[Tuple[int, int, float]]:\n",
        "    \"\"\"\n",
        "    Gera um episódio de comprimento fixo T atendendo à condição\n",
        "    de inícios exploratórios: o primeiro par (estado_inicial, acao_inicial)=(s0, a0) é imposto.\n",
        "    A partir do segundo passo segue a política determinística Pi.\n",
        "    Cada passo armazenado na trajetória contém a tupla (s_t, a_t, r_{t+1}).\n",
        "\n",
        "    Parâmetros\n",
        "    ----------\n",
        "    ambiente : AmbienteNavegacaoLabirinto\n",
        "        Instância do gridworld.\n",
        "    estado_inicial : int\n",
        "        Índice linear do estado em que o episódio começa (s0).\n",
        "    acao_inicial : int\n",
        "        Ação forçada a0 em s0.\n",
        "    Pi : np.ndarray, shape (n_estados, n_acoes)\n",
        "        Política determinística one-hot (argmax define a ação).\n",
        "    T : int\n",
        "        Horizonte fixo do episódio - número de passos do episódio (>=1).\n",
        "\n",
        "    Retorna\n",
        "    -------\n",
        "    trajetoria : List[Tuple[int, int, float]]\n",
        "        Lista de tamanho T com os triplos (s_t, a_t, r_{t+1}).\n",
        "    \"\"\"\n",
        "\n",
        "    if T < 1:\n",
        "        raise ValueError(\"T deve ser >= 1.\")\n",
        "\n",
        "    # Preparação: \"Teleporta\" o agente para o estado inicial.\n",
        "    ambiente.reset_to_state(estado_inicial)\n",
        "\n",
        "    # Inicialização\n",
        "    trajetoria: List[Tuple[int, int, float]] = []\n",
        "\n",
        "    # Passo 0: ação forçada (inícios exploratórios)\n",
        "\n",
        "    # Código aqui\n",
        "\n",
        "    # Demais passos: seguem a política Pi\n",
        "\n",
        "    # Código aqui\n",
        "\n",
        "    return trajetoria"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c1bec59",
      "metadata": {
        "id": "4c1bec59"
      },
      "source": [
        "### MC com inícios exploratórios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "711ec72e",
      "metadata": {
        "id": "711ec72e"
      },
      "outputs": [],
      "source": [
        "def mc_inicios_exploratorios(\n",
        "    ambiente,\n",
        "    gamma: float = 0.9,\n",
        "    N: int = 10_000,\n",
        "    T: int = 50,\n",
        "    seed: int = 42\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    Monte Carlo com Inícios Exploratórios (ES) no gridworld.\n",
        "\n",
        "    Loop principal (por episódio):\n",
        "      1) Escolhe uniformemente (s0, a0)  [condição ES];\n",
        "      2) Gera trajetória de comprimento fixo T começando com (s0, a0) e depois seguindo Pi;\n",
        "      3) Varredura reversa acumulando o retorno G e atualizando Q(s,a) por média (todas as visitas);\n",
        "      4) Melhoria de política incremental após cada atualização de Q(s_t, . ), isto é, executa a etapa de melhoria de política (determinística e gulosa).\n",
        "\n",
        "    Parâmetros\n",
        "    ----------\n",
        "    ambiente : AmbienteNavegacaoLabirinto\n",
        "        Instância do gridworld.\n",
        "    gamma : float\n",
        "        Fator de desconto.\n",
        "    N : int\n",
        "        Número total de episódios (ciclos) a executar.\n",
        "    T : int\n",
        "        Horizonte fixo (número de passos) de cada episódio.\n",
        "\n",
        "    Retorna\n",
        "    -------\n",
        "    Q   : np.ndarray, shape (n_estados, n_acoes)\n",
        "        Estimativas finais de Q(s,a) obtidas por média de retornos.\n",
        "    Pi  : np.ndarray, shape (n_estados, n_acoes)\n",
        "        Política determinística resultante.\n",
        "    numero_de_visitas : np.ndarray, shape (n_estados, n_acoes)\n",
        "        Matriz com com o número de visitas por par (s,a).\n",
        "    k   : int\n",
        "        Número de episódios efetivamente executados.\n",
        "    \"\"\"\n",
        "\n",
        "    # Reprodutibilidade\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Atalhos do ambiente (shapes)\n",
        "    n_estados     = ambiente.n_states   # int\n",
        "    n_acoes       = ambiente.n_actions  # int\n",
        "\n",
        "    # Inicializações\n",
        "    Q = np.zeros((n_estados, n_acoes), dtype=float)\n",
        "    numero_de_visitas = np.zeros((n_estados, n_acoes), dtype=float)      # contagem de visitas\n",
        "    soma_dos_retornos = np.zeros((n_estados, n_acoes), dtype=float)      # soma de retornos (para média empírica)\n",
        "\n",
        "    # Política inicial determinística (ex.: ação 0 em todos os estados)\n",
        "    Pi = np.zeros((n_estados, n_acoes), dtype=float)\n",
        "    Pi[np.arange(n_estados), rng.integers(n_acoes, size=n_estados)] = 1.0\n",
        "\n",
        "    for k in tqdm(range(1, N + 1), desc=\"Episódios (MC ES)\"):\n",
        "\n",
        "        # 1) CONDIÇÃO DE INÍCIOS EXPLORATÓRIOS: Escolha uniforme de (s0, a0)\n",
        "\n",
        "        # Código aqui\n",
        "\n",
        "        # 2) Gera trajetória com início exploratório\n",
        "\n",
        "        # Código aqui\n",
        "\n",
        "        # 3) Varredura reversa:\n",
        "        # acumula as somas dos retornos G\n",
        "        # atualiza Q por média de visitas\n",
        "        # melhoria de política (gulosa em relação a Q no estado s_t)\n",
        "\n",
        "        # Código aqui\n",
        "\n",
        "    return Q, Pi, numero_de_visitas, k"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b80e3daf",
      "metadata": {
        "id": "b80e3daf"
      },
      "source": [
        "## Experimento"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9wpp2NZ3rx2",
      "metadata": {
        "id": "b9wpp2NZ3rx2"
      },
      "source": [
        "### Simulação\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34567e4c",
      "metadata": {
        "id": "34567e4c"
      },
      "outputs": [],
      "source": [
        "Q, Pi, numero_de_visitas, k = mc_inicios_exploratorios(\n",
        "    ambiente,   # gridworld\n",
        "    gamma=0.9,  # fator de desconto\n",
        "    N=10_000,   # número total de episódios\n",
        "    T=100,      # comprimento fixo de cada episódio\n",
        "    seed=0      # semente para a reprodutibilidade\n",
        ")\n",
        "\n",
        "# Derivar V a partir de Q:\n",
        "# V = np.max(Q, axis=1)\n",
        "V = np.sum(Pi * Q, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd5lMgpr3tdV",
      "metadata": {
        "id": "dd5lMgpr3tdV"
      },
      "source": [
        "### Visualização"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pfg9_po8ivZ7",
      "metadata": {
        "id": "Pfg9_po8ivZ7"
      },
      "outputs": [],
      "source": [
        "# Q: ndarray (n_estados, n_acoes)\n",
        "plot_tabular(Q, kind=\"Q\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d167395",
      "metadata": {
        "id": "3d167395"
      },
      "outputs": [],
      "source": [
        "# Pi: ndarray (n_estados, n_acoes)\n",
        "plot_tabular(Pi, kind=\"Pi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b8baad",
      "metadata": {
        "id": "64b8baad"
      },
      "outputs": [],
      "source": [
        "# Número de visitas por par (s,a)\n",
        "plot_tabular(numero_de_visitas, kind=\"VISITAS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e2180ea",
      "metadata": {
        "id": "3e2180ea"
      },
      "outputs": [],
      "source": [
        "# Número de visitas por par (s,a)\n",
        "plot_visitas_log(numero_de_visitas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c79cc9b",
      "metadata": {
        "id": "0c79cc9b"
      },
      "outputs": [],
      "source": [
        "# V: ndarray (n_estados,)\n",
        "plot_tabular(V, kind=\"V\", ambiente=ambiente, center_zero=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "532bc8f6",
      "metadata": {
        "id": "532bc8f6"
      },
      "outputs": [],
      "source": [
        "# Política (setas) sobre o ambiente\n",
        "_ = plot_policy(ambiente, Pi)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f17e2a3",
      "metadata": {
        "id": "3f17e2a3"
      },
      "source": [
        "# Tarefa\n",
        "1. Implemente o algoritmo **MC com inícios exploratórios**.\n",
        "\n",
        "2. Analise o impacto do número de episódios (`N`):\n",
        "\n",
        "- Fixe o comprimento do episódio (`T=100`) e o fator de desconto ($\\gamma=0.9$).\n",
        "\n",
        "- Varie ($N \\in \\{10, 100, 1000, 10000\\}$).\n",
        "\n",
        "3. Analise o impacto o comprimento do episódio (`T`):\n",
        "\n",
        "- Fixe o número de episódios (`N=10000`) e o fator de desconto ($\\gamma=0.9$).\n",
        "\n",
        "- Varie o comprimento do episódio (`T` $\\in \\{1, 10, 50, 100\\}$).\n",
        "\n",
        "4. Analise o impacto do fator de desconto ($\\gamma$):\n",
        "\n",
        "- Fixe o comprimento do episódio (`T=100`) e o número de episódios (`N=10000`).\n",
        "\n",
        "- Varie o fator de desconto ($\\gamma \\in \\{0.0, 0.5, 0.9, 0.95, 0.99\\}$).\n",
        "\n",
        "**Configuração base:**\n",
        "\n",
        "- `world_size = (5, 5)`\n",
        "- `target_states = [(3, 2)]`\n",
        "- `bad_states = [(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)]`\n",
        "- `allow_bad_entry = True`\n",
        "- recompensas base: $[\\,r_{\\text{boundary}},\\ r_{\\text{bad}},\\ r_{\\text{target}},\\ r_{\\text{other}}\\,] = [-1,\\ -10,\\ 1,\\ 0]$\n",
        "- `max_iter=20`\n",
        "\n",
        "**Em todos os experimentos mostrar:**\n",
        "\n",
        "1. **Figuras**:\n",
        "   - heatmap de $V(s)$ (função `plot_tabular`);\n",
        "   - heatmap de $Q(s,a)$ (função `plot_tabular`);\n",
        "   - heatmap de $\\pi(a\\mid s)$ (função `plot_tabular`);\n",
        "   - política aprendida (função `plot_policy`)\n",
        "   - número de visitas por par (s,a) (função `plot_visitas_log` ou `plot_tabular`)\n",
        "2. **Discussão**: texto breve (3-6 linhas) por experimento.\n",
        "\n",
        "**Entregáveis:**\n",
        "\n",
        "2. **Código** (notebook `.ipynb`)\n",
        "1. **Relatório** (`.pdf`).\n",
        "- O PDF deve conter:\n",
        "  - **Setup** (parâmetros usados).\n",
        "  - **Resultados** (figuras e tabelas organizadas por experimento).\n",
        "  - **Análises curtas** por experimento.\n",
        "- O PDF **NÃO** deve conter:\n",
        "    - Códigos."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4298bf0e",
        "e8610559",
        "e100bbcf",
        "0cb6fb51"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}