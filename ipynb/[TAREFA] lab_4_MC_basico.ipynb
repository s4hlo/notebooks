{"cells":[{"cell_type":"markdown","id":"8441f52a","metadata":{"id":"8441f52a"},"source":["# Laboratório 4: MC Básico"]},{"cell_type":"markdown","id":"4298bf0e","metadata":{"id":"4298bf0e"},"source":["## Importações"]},{"cell_type":"code","execution_count":null,"id":"4786f23b","metadata":{"id":"4786f23b"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from matplotlib.colors import ListedColormap\n","import seaborn as sns\n","from typing import Dict, Tuple, List, Union, Optional, Set\n","from numpy import linalg as LA\n","from collections import defaultdict\n","from tqdm.auto import tqdm"]},{"cell_type":"markdown","id":"e8610559","metadata":{"id":"e8610559"},"source":["## Ambiente: Navegação no Labirinto (gridworld)"]},{"cell_type":"code","execution_count":null,"id":"2766bd79","metadata":{"id":"2766bd79"},"outputs":[],"source":["# Ambiente: Navegação no Labirinto (gridworld)\n","class AmbienteNavegacaoLabirinto:\n","    \"\"\"\n","    Navegação no Labirinto (gridworld) determinístico para experimentos de aprendizado por reforço.\n","\n","    Política de recompensa:\n","    - A recompensa é calculada com base na tentativa de movimento (s, a -> posição proposta), antes de aplicar rebote por sair da grade ou por entrada proibida em bad_states.\n","    - Se a tentativa mira fora da grade: r_boundary\n","    - Se mira um bad_state: r_bad\n","    - Se mira um target_state: r_target\n","    - Caso contrário: r_other\n","\n","    Terminação:\n","    - Por padrão, o ambiente não encerra episódios automaticamente (target_states/bad_states não são estados terminais).\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        world_size: Tuple[int, int],\n","        bad_states: List[Tuple[int, int]],\n","        target_states: List[Tuple[int, int]],\n","        allow_bad_entry: bool = False,\n","        rewards: Optional[List[float]] = None\n","    ) -> None:\n","        \"\"\"\n","        Inicializa o ambiente de navegação em labirinto.\n","\n","        Parâmetros\n","        ----------\n","        world_size : (n_linhas, n_colunas)\n","            Dimensões da grade.\n","        bad_states : lista de (linha, coluna)\n","            Coordenadas dos estados com penalidade.\n","        target_states : lista de (linha, coluna)\n","            Coordenadas dos estados-alvo.\n","        allow_bad_entry : bool\n","            Se False, impede entrada em bad_states (rebote para o estado atual).\n","        rewards : [r_boundary, r_bad, r_target, r_other]\n","            Recompensas possíveis. Se None, usa [-1, -1, 1, 0].\n","        \"\"\"\n","        if rewards is None:\n","            rewards = [-1, -1, 1, 0]\n","\n","        self.n_rows, self.n_cols = world_size   # dimensões da grade do labirinto\n","        self.bad_states = set(bad_states)       # estados com penalidade alta\n","        self.target_states = set(target_states) # estados com recompensa alta\n","        self.allow_bad_entry = allow_bad_entry  # se o agente pode entrar em estados ruins\n","\n","        # Validações\n","        for st in self.bad_states | self.target_states:\n","            if not (0 <= st[0] < self.n_rows and 0 <= st[1] < self.n_cols):\n","                raise ValueError(f\"Estado {st} fora dos limites.\")\n","        if self.bad_states & self.target_states:\n","            raise ValueError(\"bad_states e target_states devem ser disjuntos.\")\n","\n","        # Recompensas definidas para cada tipo de transição\n","        self.r_boundary = rewards[0]    # tentar sair da grade\n","        self.r_bad      = rewards[1]    # transição para estado ruim\n","        self.r_target   = rewards[2]    # transição para estado alvo\n","        self.r_other    = rewards[3]    # demais transições\n","\n","        # Espaço de ações: dicionário com deslocamentos (linha, coluna)\n","        self.action_space = {\n","            0: (-1, 0),  # cima\n","            1: (1, 0),   # baixo\n","            2: (0, -1),  # esquerda\n","            3: (0, 1),   # direita\n","            4: (0, 0)    # permanecer no mesmo estado\n","        }\n","\n","        # Espaço de recompensas: lista de recompensas possíveis\n","        self.recompensas_possiveis = np.array(sorted(set(rewards)))\n","        self.reward_map = {r: i for i, r in enumerate(self.recompensas_possiveis)}\n","\n","        # número total de estados\n","        self.n_states = self.n_rows * self.n_cols\n","\n","        # número total de ações\n","        self.n_actions = len(self.action_space)\n","\n","        # número total de recompensas possíveis\n","        self.n_rewards = self.recompensas_possiveis.shape[0]\n","\n","        # Tensor de probabilidades de transição: P(s'|s,a)\n","        self.state_transition_probabilities = np.zeros((self.n_states, self.n_states, self.n_actions))\n","\n","        # Tensor de probabilidade de recompensas: P(r|s,a)\n","        self.reward_probabilities = np.zeros((self.n_rewards, self.n_states, self.n_actions))\n","\n","        # Matriz de recompensa imediata (determinística): recompensa[s, a] = r\n","        self.recompensas_imediatas = np.zeros((self.n_states, self.n_actions))\n","\n","        # Matriz de transição de estados (determinística): transicao[s, a] = s'\n","        self.transicao_de_estados = np.zeros((self.n_states, self.n_actions), dtype=int)\n","\n","        self.agent_pos = (0, 0)  # posição inicial do agente\n","\n","        self._init_dynamics()  # inicializa as dinâmicas de transição e recompensa\n","\n","\n","    def __repr__(self) -> str:\n","        return (f\"AmbienteNavegacaoLabirinto({self.n_rows}x{self.n_cols}, \"\n","                f\"bad={len(self.bad_states)}, target={len(self.target_states)}, \"\n","                f\"allow_bad_entry={self.allow_bad_entry}, agent_pos={self.agent_pos})\")\n","\n","\n","    def __str__(self) -> str:\n","        return self.render(as_string=True)\n","\n","\n","    def reset(self) -> Tuple[int, int]:\n","        \"\"\"\n","        Reinicia a posição do agente para o estado inicial (0, 0).\n","\n","        Retorna\n","        -------\n","        (linha, coluna) : posição inicial do agente.\n","        \"\"\"\n","        self.agent_pos = (0, 0)\n","        return self.agent_pos\n","\n","\n","    def step(self, acao: int, *, linear: bool = False) -> Tuple[Union[int, Tuple[int, int]], float]:\n","        \"\"\"\n","        Executa uma ação, atualiza a posição do agente e devolve o próximo estado.\n","\n","        Parâmetros\n","        ----------\n","        acao : int\n","            Índice da ação (0-4).\n","        linear : bool, keyword-only (default=False)\n","            Se True, retorna o estado como índice linear {0,...,self.n_states-1}; caso contrário, retorna tupla (linha, coluna).\n","\n","        Retorna\n","        -------\n","        proximo_estado : int | (int, int)\n","            Estado observado após a transição (com possível rebote).\n","        recompensa : float\n","            Recompensa imediata recebida.\n","        \"\"\"\n","        estado_atual = self.agent_pos                                           # armazena o estado atual do agente\n","        proposta = self._proposta(estado_atual, acao)                           # calcula a posição proposta pela ação\n","        recompensa = self._compute_reward(proposta)                             # avalia a recompensa com base na tentativa\n","        destino = self._destino_final(estado_atual, acao)                       # aplica regras e obtém o destino após possíveis rebotes\n","        self.agent_pos = destino                                                # atualiza a posição interna do agente\n","        proximo_estado = self.state_to_index(destino) if linear else destino    # escolhe o formato da observação de saída\n","        return proximo_estado, recompensa                                       # retorna observação e recompensa\n","\n","\n","    def reset_to_state(self, estado: Union[Tuple[int, int], int], verificar_validade_estado: bool = True) -> Tuple[int, int]:\n","        \"\"\"\n","        Teleporta o agente para `estado` sem reiniciar o episódio completo.\n","\n","        Parâmetros\n","        ----------\n","        estado : (linha, coluna) | int\n","            Tupla de coordenadas (linha, coluna) ou índice linear (int).\n","        verificar_validade_estado : bool\n","            Se True, lança ValueError se o estado for inválido.\n","\n","        Retorna\n","        -------\n","        (linha, coluna) : nova posição do agente.\n","        \"\"\"\n","        # Converte índice -> tupla, se necessário\n","        if isinstance(estado, int):\n","            estado = self.index_to_state(estado)\n","\n","        if verificar_validade_estado and not self._in_bounds(estado):\n","            raise ValueError(f\"Estado {estado} fora dos limites do labirinto.\")\n","\n","        self.agent_pos = tuple(estado)      # mantém tupla imutável\n","\n","        return self.agent_pos\n","\n","\n","    def is_bad(self, state: Union[int, Tuple[int, int]]) -> bool:\n","        \"\"\"Retorna True se o estado for um bad_state.\"\"\"\n","        if isinstance(state, int):\n","            state = self.index_to_state(state)\n","        return state in self.bad_states\n","\n","\n","    def is_target(self, state: Union[int, Tuple[int, int]]) -> bool:\n","        \"\"\"Retorna True se o estado for um target_state.\"\"\"\n","        if isinstance(state, int):\n","            state = self.index_to_state(state)\n","        return state in self.target_states\n","\n","\n","    def state_to_index(self, estado: Tuple[int, int]) -> int:\n","        \"\"\"\n","        Converte coordenada (linha, coluna) para índice linear no intervalo [0, n_states-1].\n","        \"\"\"\n","        linha, coluna = estado\n","        return linha * self.n_cols + coluna\n","\n","\n","    def index_to_state(self, indice: int) -> Tuple[int, int]:\n","        \"\"\"\n","        Converte índice linear  no intervalo [0, n_states-1] para coordenada (linha, coluna).\n","        \"\"\"\n","        return divmod(indice, self.n_cols)  # (linha, coluna) = (indice // self.n_cols, indice % self.n_cols)\n","\n","\n","    def enumerate_states(self) -> List[int]:\n","        \"\"\"Retorna a lista de índices lineares de todos os estados [0, ..., n_states - 1].\"\"\"\n","        return list(range(self.n_states))\n","\n","\n","    def enumerate_actions(self) -> List[int]:\n","        \"\"\"Retorna a lista de índices das ações disponíveis [0, ..., n_actions - 1].\"\"\"\n","        return list(self.action_space.keys())\n","\n","\n","    def render(\n","        self,\n","        *,\n","        as_string: bool = True,\n","        show_coords: bool = False,\n","        legend: bool = True,\n","        chars: dict | None = None\n","    ) -> str:\n","        \"\"\"\n","        Renderização ASCII do grid.\n","        - A: agente, B: bad, T: target, .: vazio\n","        - show_coords: mostra índices de linha/coluna\n","        - legend: inclui legenda ao final\n","        - chars: permite customizar símbolos (keys: 'agent','bad','target','empty')\n","        \"\"\"\n","        if chars is None:\n","            chars = {\"agent\": \"A\", \"bad\": \"B\", \"target\": \"T\", \"empty\": \".\"}\n","\n","        linhas = []\n","\n","        # cabeçalho de colunas\n","        if show_coords:\n","            header = \"    \" + \" \".join(f\"{c:2d}\" for c in range(self.n_cols))\n","            linhas.append(header)\n","            linhas.append(\"    \" + \"--\" * self.n_cols)\n","\n","        # monta o grid linha a linha\n","        for r in range(self.n_rows):\n","            row_syms = []\n","            for c in range(self.n_cols):\n","                sym = chars[\"empty\"]\n","                if (r, c) in self.bad_states:\n","                    sym = chars[\"bad\"]\n","                if (r, c) in self.target_states:\n","                    sym = chars[\"target\"]\n","                if self.agent_pos == (r, c):\n","                    sym = chars[\"agent\"]\n","                row_syms.append(sym)\n","\n","            linha_str = \" \".join(row_syms)\n","            if show_coords:\n","                linhas.append(f\"{r:2d} | {linha_str}\")\n","            else:\n","                linhas.append(linha_str)\n","\n","        # legenda\n","        if legend:\n","            linhas.append(\"\")\n","            linhas.append(f\"Legenda: {chars['agent']}=agente, {chars['bad']}=bad, \"\n","                        f\"{chars['target']}=target, {chars['empty']}=vazio\")\n","\n","        out = \"\\n\".join(linhas)\n","        return out if as_string else print(out)\n","\n","\n","    def plot_labirinto(self, ax=None, titulo: str = \"Visualização do Labirinto\", cbar: bool = False):\n","        \"\"\"\n","        Visualiza o labirinto.\n","\n","        Representa:\n","        - Estado neutro: branco\n","        - Estado ruim: vermelho\n","        - Estado alvo: verde\n","\n","        Parâmetros\n","        ----------\n","        ax : matplotlib.axes.Axes, opcional\n","            Eixo onde desenhar. Se None, cria uma nova figura.\n","        titulo : str, opcional\n","            Título do gráfico.\n","        cbar : bool, opcional (default=False)\n","            Exibe (True) ou oculta (False) a barra de cores.\n","\n","        Retorna\n","        -------\n","        ax : matplotlib.axes.Axes\n","            Eixo com o heatmap.\n","        \"\"\"\n","        # Cria matriz com valores padrão (0 = neutro)\n","        matriz = np.zeros((self.n_rows, self.n_cols), dtype=int)\n","\n","        # marca estados\n","        for (r, c) in self.bad_states:\n","            matriz[r, c] = 1   # ruim\n","        for (r, c) in self.target_states:\n","            matriz[r, c] = 2   # alvo\n","\n","        # cores: branco=neutro, vermelho=ruim, verde=alvo\n","        cmap = ListedColormap([\"white\", \"red\", \"green\"])\n","\n","        fig = None\n","        if ax is None:\n","            fig, ax = plt.subplots(figsize=(self.n_cols, self.n_rows))\n","\n","        ax = sns.heatmap(\n","            matriz,\n","            cmap=cmap,\n","            cbar=cbar,\n","            linewidths=0.5,\n","            linecolor=\"gray\",\n","            square=True,\n","            ax=ax\n","        )\n","\n","        # remove ticks/labels\n","        ax.set_xticks([]); ax.set_yticks([])\n","        ax.set_xticklabels([]); ax.set_yticklabels([])\n","\n","        # bordas externas\n","        for side in (\"left\", \"right\", \"top\", \"bottom\"):\n","            ax.spines[side].set_visible(True)\n","            ax.spines[side].set_linewidth(0.5)\n","            ax.spines[side].set_edgecolor(\"gray\")\n","\n","        ax.set_title(titulo)\n","\n","        if fig is not None:\n","            plt.tight_layout()\n","            plt.show()\n","\n","        return\n","\n","\n","    def _init_dynamics(self):\n","        \"\"\"\n","        Preenche as matrizes de transição e recompensa com base na estrutura do ambiente e regras de movimentação.\n","        \"\"\"\n","\n","        self.recompensas_imediatas.fill(0.0)\n","        self.transicao_de_estados.fill(0)\n","        self.state_transition_probabilities.fill(0.0)\n","        self.reward_probabilities.fill(0.0)\n","\n","        for s in self.enumerate_states():                                   # percorre todos os estados (índices lineares)\n","            estado_atual = self.index_to_state(s)                           # converte índice para (linha, coluna)\n","            for a in self.enumerate_actions():                              # percorre todas as ações disponíveis\n","                proposta = self._proposta(estado_atual, a)                  # calcula a posição proposta pela ação\n","                r = self._compute_reward(proposta)                          # avalia a recompensa da tentativa de movimento\n","                destino = self._destino_final(estado_atual, a)              # obtém destino após aplicar regras de rebote\n","                s_next = self.state_to_index(destino)                       # converte destino para índice linear\n","\n","                self.recompensas_imediatas[s, a] = r                        # registra r(s, a) na matriz de recompensas imediatas\n","                self.transicao_de_estados[s, a] = s_next                    # registra T(s, a) = s' na matriz de transições\n","\n","                self.state_transition_probabilities[s_next, s, a] = 1.0     # define P(s'|s, a) = 1 (ambiente determinístico)\n","                self.reward_probabilities[self.reward_map[r], s, a] = 1.0   # define P(r |s, a) = 1 (ambiente determinístico)\n","\n","\n","    def _proposta(self, state: Tuple[int, int], acao: int) -> Tuple[int, int]:\n","        \"\"\"Retorna a posição proposta (antes de qualquer rebote).\"\"\"\n","        dl, dc = self.action_space[acao]\n","        return (state[0] + dl, state[1] + dc)\n","\n","\n","    def _destino_final(self, state: Tuple[int, int], acao: int) -> Tuple[int, int]:\n","        \"\"\"\n","        Aplica as regras de rebote: fora da grade => rebote;\n","        bad_state com allow_bad_entry=False => rebote; caso contrário segue para a proposta.\n","        \"\"\"\n","        proposta = self._proposta(state, acao)\n","        if not self._in_bounds(proposta):\n","            return state\n","        if (not self.allow_bad_entry) and self.is_bad(proposta):\n","            return state\n","        return proposta\n","\n","\n","    def _in_bounds(self, posicao: Tuple[int, int]) -> bool:\n","        \"\"\"\n","        Verifica se uma posição está dentro dos limites do labirinto.\n","\n","        Parâmetros\n","        ----------\n","        posicao : (linha, coluna)\n","\n","        Retorna\n","        -------\n","        bool : True se dentro da grade, False caso contrário.\n","        \"\"\"\n","        linha, coluna = posicao\n","        return 0 <= linha < self.n_rows and 0 <= coluna < self.n_cols\n","\n","\n","    def _compute_reward(self, destino: Tuple[int, int]) -> float:\n","        \"\"\"\n","        Calcula a recompensa da tentativa de transição para 'destino'.\n","\n","        Regras\n","        ------\n","        - Se 'destino' está fora da grade: r_boundary\n","        - Se 'destino' é bad_state: r_bad\n","        - Se 'destino' é target_state: r_target\n","        - Caso contrário: r_other\n","        \"\"\"\n","        if not self._in_bounds(destino):\n","            return self.r_boundary\n","        elif self.is_bad(destino):\n","            return self.r_bad\n","        elif self.is_target(destino):\n","            return self.r_target\n","        else:\n","            return self.r_other"]},{"cell_type":"markdown","id":"e100bbcf","metadata":{"id":"e100bbcf"},"source":["## Funções auxiliares para visualização"]},{"cell_type":"code","execution_count":null,"id":"eedf6090","metadata":{"id":"eedf6090"},"outputs":[],"source":["def _prepare_grid(env, ax=None, draw_cells=True):\n","    \"\"\"\n","    Configura o grid. Se 'ax' não for passado, cria 'fig, ax'; caso contrário retorna 'fig=None, ax'.\n","    \"\"\"\n","    fig = None\n","    if ax is None:\n","        fig, ax = plt.subplots(figsize=(env.n_cols, env.n_rows))\n","\n","    ax.set_xlim(0, env.n_cols)\n","    ax.set_ylim(0, env.n_rows)\n","    ax.set_xticks(np.arange(0, env.n_cols + 1, 1))\n","    ax.set_yticks(np.arange(0, env.n_rows + 1, 1))\n","    ax.grid(True)\n","    ax.set_aspect('equal')\n","    ax.invert_yaxis()\n","\n","    if draw_cells:\n","        for r in range(env.n_rows):\n","            for c in range(env.n_cols):\n","                cell = (r, c)\n","                if cell in env.bad_states:\n","                    color = 'red'\n","                elif cell in env.target_states:\n","                    color = 'green'\n","                else:\n","                    color = 'white'\n","                rect = patches.Rectangle((c, r), 1, 1, facecolor=color, edgecolor='gray')\n","                ax.add_patch(rect)\n","\n","    return fig, ax\n","\n","def _coerce_policy(env, policy):\n","    \"\"\"\n","    Normaliza a política para o formato dict[(r,c)] -> ação (int).\n","    Aceita:\n","      - dict[(r,c)] -> ação\n","      - dict[(r,c)] -> vetor de probabilidades\n","      - Pi ndarray (n_estados, n_acoes)\n","    \"\"\"\n","    # caso 1: matriz Pi (ndarray)\n","    if isinstance(policy, np.ndarray):\n","        a_star = np.argmax(policy, axis=1)\n","        return {env.index_to_state(s): int(a_star[s]) for s in range(env.n_states)}\n","\n","    # caso 2: dicionário\n","    sample_val = next(iter(policy.values()))\n","    if isinstance(sample_val, np.ndarray):\n","        return {pos: int(np.argmax(probs)) for pos, probs in policy.items()}\n","    else:\n","        return policy\n","\n","def plot_policy(env, policy, ax=None, titulo=\"Política\"):\n","    \"\"\"\n","    Desenha setas/círculos de uma política. 'policy' pode ser:\n","      - dict[(r,c)] -> ação\n","      - dict[(r,c)] -> vetor de probabilidades\n","      - ndarray com shape (n_estados, n_acoes)\n","    \"\"\"\n","    fig, ax = _prepare_grid(env, ax=ax)\n","\n","    policy_dict = _coerce_policy(env, policy)\n","    color = 'black'\n","    lw = 1.5\n","\n","    for (r, c), action in policy_dict.items():\n","        x, y = c + 0.5, r + 0.5\n","        if action == 0:      # cima\n","            ax.arrow(x, y, dx=0, dy=-0.3, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n","        elif action == 1:    # baixo\n","            ax.arrow(x, y, dx=0, dy=0.3, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n","        elif action == 2:    # esquerda\n","            ax.arrow(x, y, dx=-0.3, dy=0, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n","        elif action == 3:    # direita\n","            ax.arrow(x, y, dx=0.3, dy=0, head_width=0.2, head_length=0.2, fc=color, ec=color, linewidth=lw)\n","        elif action == 4:    # ficar\n","            circ = patches.Circle((x, y), 0.1, edgecolor=color, facecolor='none', linewidth=lw)\n","            ax.add_patch(circ)\n","\n","    ax.set_title(titulo)\n","    if fig is not None:\n","        plt.tight_layout()\n","        plt.show()\n","    return ax\n","\n","def plot_tabular(\n","    data,\n","    kind: str = \"Q\",          # \"Q\" (valores de ação), \"Pi\" (política), \"V\" (valores de estado)\n","    ambiente=None,            # necessário quando kind=\"V\" para reshape\n","    ax=None,\n","    cbar: bool = True,\n","    fmt: str = \".1f\",\n","    center_zero: bool = True  # só relevante para \"Q\" e \"V\"\n","):\n","    \"\"\"\n","    Plota matrizes tabulares de RL em formato de heatmaps (mapas de calor).\n","    Esta função cobre 3 casos:\n","    1. kind=\"Q\": heatmap de Q(s, a) com ações nas linhas e estados nas colunas.\n","    2. kind=\"Pi\": heatmap de Pi(a|s) (probabilidades) com ações nas linhas e estados nas colunas.\n","    3. kind=\"V\": heatmap de V(s) no grid (n_rows x n_cols) do ambiente .\n","\n","    Parameters\n","    ----------\n","    data : ndarray\n","        Dados a serem plotados.\n","        - Para kind=\"Q\" ou \"Pi\": array 2D com shape (n_estados, n_acoes).\n","        - Para kind=\"V\": array 1D com shape (n_estados,) que será remodelado para (ambiente.n_rows, ambiente.n_cols).\n","    kind : {\"Q\", \"Pi\", \"V\"}, default=\"Q\"\n","        Tipo do plot:\n","        - \"Q\" usa paleta divergente centrada em zero.\n","        - \"Pi\" usa paleta sequencial no intervalo [0, 1].\n","        - \"V\" plota o valor de estado no grid do ambiente.\n","    ambiente : object, optional\n","        Necessário quando kind=\"V\". Deve expor n_rows e n_cols para o reshape.\n","    ax : matplotlib.axes.Axes, optional\n","        Eixo onde o heatmap será desenhado. Se None, uma nova figura/eixo é criado.\n","    cbar : bool, default=True\n","        Se True, exibe a barra de cores (colorbar).\n","    fmt : str, default=\".1f\"\n","        Formatação dos valores anotados em cada célula do heatmap.\n","    center_zero : bool, default=True\n","        Quando kind é \"Q\" ou \"V\", centraliza a escala de cores em zero (vmin=-absmax, vmax=absmax). Ignorado para \"Pi\".\n","\n","    Returns\n","    -------\n","    ax : matplotlib.axes.Axes\n","        Eixo contendo o heatmap resultante.\n","    \"\"\"\n","    kind = kind.upper()\n","\n","    xlabel = {\"V\": \"Colunas\", \"PI\": \"Estados\", \"Q\": \"Estados\"}\n","    ylabel = {\"V\": \"Linhas\", \"PI\": \"Ações\", \"Q\": \"Ações\" }\n","    title  = {\"V\": \"Valores de Estado (V(s))\", \"PI\": r\"Política ($\\pi(a|s)$ transposta)\", \"Q\": \"Valores de ação (Q(s, a) transposta)\"}\n","\n","    fig = None\n","\n","    #  V(s): precisa do shape do grid\n","    match kind:\n","        case \"V\":\n","\n","            if ambiente is None:\n","                raise ValueError(\"Para kind='V', passe 'ambiente' para reshape (n_rows, n_cols).\")\n","\n","            M = data.reshape(ambiente.n_rows, ambiente.n_cols)\n","\n","            if ax is None:\n","                fig, ax = plt.subplots(figsize=(ambiente.n_cols, ambiente.n_rows))\n","\n","            if center_zero:\n","                vmax = float(np.abs(M).max())\n","                vmin = -vmax\n","            else:\n","                vmin = float(M.min())\n","                vmax = float(M.max())\n","\n","            cmap, square = \"bwr\", True\n","\n","        case \"PI\" | \"Q\":\n","\n","            # Q(s,a) e Pi(a|s): ações nas linhas, estados nas colunas\n","            M = data.T  # data: (n_estados, n_acoes) -> transposto para (n_acoes, n_estados)\n","            n_acoes, n_estados = M.shape\n","\n","            if ax is None:\n","                fig, ax = plt.subplots(figsize=(n_estados, n_acoes))\n","\n","            if kind == \"PI\":\n","                cmap = \"Blues\";\n","                vmin, vmax = 0.0, 1.0\n","            else:  # \"Q\"\n","                cmap = \"bwr\"\n","                if center_zero:\n","                    vmax = float(np.abs(M).max())\n","                    vmin = -vmax\n","                else:\n","                    vmin = float(M.min())\n","                    vmax = float(M.max())\n","\n","            square = False\n","\n","        case _:\n","            raise ValueError(f\"kind desconhecido: {kind!r} (use 'Q', 'Pi' ou 'V').\")\n","\n","\n","    ax = sns.heatmap(\n","        data=M,\n","        annot=True,\n","        fmt=fmt,\n","        cmap=cmap,\n","        vmin=vmin,\n","        vmax=vmax,\n","        cbar=cbar,\n","        square=square,\n","        linewidths=0.5,\n","        linecolor=\"gray\",\n","        ax=ax\n","    )\n","\n","    ax.set_xlabel(xlabel[kind])\n","    ax.set_ylabel(ylabel[kind])\n","    ax.set_title(title[kind])\n","\n","    # bordas externas\n","    for side in (\"left\", \"right\", \"top\", \"bottom\"):\n","        ax.spines[side].set_visible(True)\n","        ax.spines[side].set_linewidth(0.5)\n","        ax.spines[side].set_edgecolor(\"gray\")\n","\n","    # rótulos\n","    if kind in (\"Q\", \"PI\"):\n","        ax.set_xticks(np.arange(n_estados) + 0.5)\n","        ax.set_xticklabels([f\"s{i}\" for i in range(n_estados)], rotation=0)\n","        ax.set_yticks(np.arange(n_acoes) + 0.5)\n","        ax.set_yticklabels([f\"a{i}\" for i in range(n_acoes)], rotation=0)\n","\n","    if fig is not None:\n","        plt.tight_layout()\n","        plt.show()\n","\n","    return"]},{"cell_type":"markdown","id":"0cb6fb51","metadata":{"id":"0cb6fb51"},"source":["## Ambiente: nova instância"]},{"cell_type":"code","execution_count":null,"id":"3fe0fe5c","metadata":{"id":"3fe0fe5c"},"outputs":[],"source":["ambiente = AmbienteNavegacaoLabirinto(\n","        world_size=(5, 5),\n","        bad_states=[(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)],\n","        target_states=[(3, 2)],\n","        allow_bad_entry=True,\n","        rewards=[-1, -10, 1, 0]\n","    )\n","ambiente.plot_labirinto()"]},{"cell_type":"markdown","id":"933b9c6e","metadata":{"id":"933b9c6e"},"source":["## Algoritmo: MC Básico"]},{"cell_type":"markdown","id":"e879cc53","metadata":{"id":"e879cc53"},"source":["### Episódio Monte-Carlo com horizonte fixo T"]},{"cell_type":"code","execution_count":null,"id":"df1900ce","metadata":{"id":"df1900ce"},"outputs":[],"source":["def gerar_episodio(\n","    ambiente,\n","    estado_inicial: int,\n","    acao_inicial: int,\n","    Pi: np.ndarray,\n","    T: int,\n","    gamma: float,\n",") -> float:\n","    \"\"\"\n","    Executa um episódio Monte Carlo de comprimento fixo T. O primeiro passo usa a ação forçada \"acao_inicial\", os demais seguem a política fornecida (Pi).\n","\n","    Parâmetros\n","    ----------\n","    ambiente : AmbienteNavegacaoLabirinto\n","    estado_inicial : int\n","        Índice linear do estado inicial.\n","    acao_inicial : int\n","        Ação forçada no primeiro passo -> necessária para cobrir todo par (s,a).\n","    Pi : np.ndarray, shape (n_estados, n_acoes)\n","        Política seguida a partir do segundo passo.\n","    T : int\n","        Comprimento fixo do episódio.\n","    gamma : float\n","        Fator de desconto.\n","\n","    Retorna\n","    -------\n","    G : float\n","        Retorno acumulado.\n","    \"\"\"\n","\n","    # Código aqui\n","    # Dica: Utilize os métodos reset_to_state e step do ambiente\n","\n","    return G"]},{"cell_type":"markdown","id":"825ee5c3","metadata":{"id":"825ee5c3"},"source":["### Avaliação Monte Carlo de uma política"]},{"cell_type":"code","execution_count":null,"id":"5c425788","metadata":{"id":"5c425788"},"outputs":[],"source":["def avaliar_politica_mc(\n","    ambiente,\n","    Pi: np.ndarray,\n","    gamma: float,\n","    T: int,\n","    N: int,\n",") -> np.ndarray:\n","    \"\"\"\n","    Estima Q(s,a) por Monte Carlo com episódios de comprimento fixo T.\n","    Para cada par (s,a), gera N episódios e faz a média dos retornos.\n","\n","    Parâmetros\n","    ----------\n","    ambiente : AmbienteNavegacaoLabirinto\n","        Instância do gridworld.\n","    Pi : np.ndarray, shape (n_estados, n_acoes)\n","        Política a ser avaliada.\n","    gamma : float\n","        Fator de desconto.\n","    T : int\n","        Horizonte fixo (número de passos por episódio).\n","    N : int\n","        Número de episódios gerados para cada par (s,a).\n","    \"\"\"\n","\n","    # Atalhos do ambiente (shapes)\n","    n_estados     = ambiente.n_states                           # int\n","    n_acoes       = ambiente.n_actions                          # int\n","\n","    # Inicializações\n","    Q           = np.zeros((n_estados, n_acoes), dtype=float)   # armazena as médias\n","    retornos    = np.zeros((n_estados, n_acoes), dtype=float)   # acumula os retornos\n","\n","    # Código aqui\n","\n","    return Q"]},{"cell_type":"markdown","id":"6d99ba6b","metadata":{"id":"6d99ba6b"},"source":["### Melhoria de política"]},{"cell_type":"code","execution_count":null,"id":"e2b281e5","metadata":{"id":"e2b281e5"},"outputs":[],"source":["def melhorar_politica(\n","    ambiente,\n","    Q: np.ndarray,\n",") -> np.ndarray:\n","    \"\"\"\n","    Gera a política gulosa determinística em relação aos valores de ação Q.\n","\n","    Parâmetros\n","    ----------\n","    ambiente : AmbienteNavegacaoLabirinto\n","        Instância do gridworld.\n","    Q : np.ndarray, shape (n_estados, n_acoes)\n","        Valores-ação estimados.\n","\n","    Retorna\n","    -------\n","    Pi_nova : np.ndarray, shape (n_estados, n_acoes)\n","        Política gulosa determinística.\n","    \"\"\"\n","\n","    # Atalhos do ambiente (shapes)\n","    n_estados     = ambiente.n_states   # int\n","    n_acoes       = ambiente.n_actions  # int\n","\n","    # Código aqui\n","\n","    return Pi_nova"]},{"cell_type":"markdown","id":"4c1bec59","metadata":{"id":"4c1bec59"},"source":["### MC Básico"]},{"cell_type":"code","execution_count":null,"id":"711ec72e","metadata":{"id":"711ec72e"},"outputs":[],"source":["def mc_basico(\n","    ambiente,\n","    gamma: float = 0.9,\n","    N: int = 1,          # nº de episódios por (s,a)\n","    max_iter: int = 50,  # nº de ciclos (avaliação + melhoria)\n","    T: int = 50,         # horizonte do episódio\n",") -> Tuple[np.ndarray, np.ndarray, int]:\n","    \"\"\"\n","    Monte Carlo Básico (variante sem modelo da iteração de política) no gridworld.\n","\n","    O método alterna:\n","      1) Avaliação de política (MC): Q(s,a) = média de N retornos gerados com\n","         episódios de comprimento fixo T, usando ação inicial forçada a em s.\n","      2) Melhoria de política: Torna a política determinística e gulosa em relação às estimativas obtidas de Q.\n","\n","\n","    Parâmetros\n","    ----------\n","    ambiente : AmbienteNavegacaoLabirinto\n","        Instância do gridworld.\n","    gamma : float\n","        Fator de desconto.\n","    N : int\n","        Número de episódios por par (s,a) na avaliação de política.\n","    max_iter : int\n","        Número de ciclos (avaliação+melhoria) a executar.\n","    T : int\n","        Horizonte (número de passos) de cada episódio MC.\n","\n","    Retorna\n","    -------\n","    Q  : np.ndarray, shape (n_estados, n_acoes)\n","        Estimativa de Q(s,a) ao final do último ciclo.\n","    Pi : np.ndarray, shape (n_estados, n_acoes)\n","        Política determinística resultante.\n","    k  : int\n","        Número de ciclos efetivamente executados (1..max_iter).\n","    \"\"\"\n","\n","    # Atalhos do ambiente (shapes)\n","    n_estados     = ambiente.n_states   # int\n","    n_acoes       = ambiente.n_actions  # int\n","\n","    # Inicializações\n","    Pi = np.zeros((n_estados, n_acoes), dtype=float)    # Política inicial determinística:\n","    Pi[:, 0] = 1.0                                      # ação 0 em todos os estados\n","    Q = np.zeros((n_estados, n_acoes), dtype=float)\n","\n","    for k in tqdm(range(1, max_iter + 1), desc=\"Iterações (MC Básico)\"):\n","        # 1) Avaliação de política por MC (média de N episódios por (s,a))\n","        Q = avaliar_politica_mc(ambiente, Pi, gamma, T, N)\n","\n","        # 2) Melhoria de política (gulosa determinística)\n","        Pi = melhorar_politica(ambiente, Q)\n","\n","    return Q, Pi, k"]},{"cell_type":"markdown","id":"b80e3daf","metadata":{"id":"b80e3daf"},"source":["## Experimento"]},{"cell_type":"markdown","id":"b9wpp2NZ3rx2","metadata":{"id":"b9wpp2NZ3rx2"},"source":["### Simulação\n"]},{"cell_type":"code","execution_count":null,"id":"34567e4c","metadata":{"id":"34567e4c"},"outputs":[],"source":["Q, Pi, k = mc_basico(\n","    ambiente,     # gridworld\n","    gamma=0.9,    # fator de desconto\n","    N=1,          # número de episódios por (s,a)\n","    max_iter=20,  # número de ciclos (avaliação + melhoria) a executar\n","    T=100         # horizonte fixo por episódio\n",")\n","\n","# Derivar V a partir de Q:\n","V = np.sum(Pi * Q, axis=1)"]},{"cell_type":"markdown","id":"dd5lMgpr3tdV","metadata":{"id":"dd5lMgpr3tdV"},"source":["### Visualização"]},{"cell_type":"code","execution_count":null,"id":"Pfg9_po8ivZ7","metadata":{"id":"Pfg9_po8ivZ7"},"outputs":[],"source":["# Q: ndarray (n_estados, n_acoes)\n","plot_tabular(Q, kind=\"Q\")"]},{"cell_type":"code","execution_count":null,"id":"3d167395","metadata":{"id":"3d167395"},"outputs":[],"source":["# Pi: ndarray (n_estados, n_acoes)\n","plot_tabular(Pi, kind=\"Pi\")"]},{"cell_type":"code","execution_count":null,"id":"0c79cc9b","metadata":{"id":"0c79cc9b"},"outputs":[],"source":["# V: ndarray (n_estados,)\n","plot_tabular(V, kind=\"V\", ambiente=ambiente, center_zero=False)"]},{"cell_type":"code","execution_count":null,"id":"532bc8f6","metadata":{"id":"532bc8f6"},"outputs":[],"source":["# Política (setas) sobre o ambiente\n","_ = plot_policy(ambiente, Pi)"]},{"cell_type":"markdown","source":["# Tarefa\n","1. Implemente o algoritmo **MC básico**.\n","\n","2. Analise o impacto o comprimento do episódio (`T`):\n","\n","- Fixe o número de episódios (`N=1`) e o fator de desconto ($\\gamma=0.9$).\n","\n","- Varie o comprimento do episódio (`T` $\\in \\{1, 5, 10, 15, 30\\}$).\n","\n","2. Analise o impacto do fator de desconto ($\\gamma$):\n","\n","- Fixe o comprimento do episódio (`T=30`) e o número de episódios (`N=1`).\n","\n","- Varie o fator de desconto ($\\gamma \\in \\{0.0, 0.5, 0.9, 0.95, 0.99\\}$).\n","\n","**Configuração base (baseline)**\n","\n","- `world_size = (5, 5)`\n","- `target_states = [(3, 2)]`\n","- `bad_states = [(1, 1), (1, 2), (2, 2), (3, 1), (3, 3), (4, 1)]`\n","- `allow_bad_entry = True`\n","- recompensas base: $[\\,r_{\\text{boundary}},\\ r_{\\text{bad}},\\ r_{\\text{target}},\\ r_{\\text{other}}\\,] = [-1,\\ -10,\\ 1,\\ 0]$\n","- `max_iter=20`\n","\n","**Em todos os experimentos mostrar:**\n","\n","1. **Figuras**:\n","   - heatmap de $V(s)$ (função `plot_tabular`);\n","   - heatmap de $Q(s,a)$ (função `plot_tabular`);\n","   - heatmap de $\\pi(a\\mid s)$ (função `plot_tabular`);\n","   - politica aprendida (função `plot_policy`)\n","2. **Discussão**: texto breve (3-6 linhas) por experimento.\n","\n","**Entregáveis:**\n","\n","2. **Código** (notebook `.ipynb`)\n","1. **Relatório** (`.pdf`).\n","- O PDF deve conter:\n","  - **Setup** (parâmetros usados).\n","  - **Resultados** (figuras e tabelas organizadas por experimento).\n","  - **Análises curtas** por experimento.\n","- O PDF **NÃO** deve conter:\n","    - Códigos."],"metadata":{"id":"n7L-NAD9slX7"},"id":"n7L-NAD9slX7"}],"metadata":{"colab":{"collapsed_sections":["4298bf0e","e8610559","e100bbcf","0cb6fb51","933b9c6e","e879cc53"],"provenance":[]},"kernelspec":{"display_name":"ai","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.5"}},"nbformat":4,"nbformat_minor":5}