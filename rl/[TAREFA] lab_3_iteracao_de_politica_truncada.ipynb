{"cells":[{"cell_type":"markdown","metadata":{"id":"gYCGU2TiXK8x"},"source":["# Laboratório 3: Iteração de política truncada"]},{"cell_type":"markdown","metadata":{"id":"ekQmchLcXK8y"},"source":["## Importações"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtRypzNFEb3t"},"outputs":[],"source":["# In_estadostala os pacotes necessários:\n","# - gymnasium[toy-text]: inclui ambientes simples como FrozenLake, Taxi, etc.\n","# - imageio[ffmpeg]: permite salvar vídeos e GIFs (formato .mp4 ou .gif)\n","!pip install gymnasium[toy-text] imageio[ffmpeg]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FTh_QK47EfwM"},"outputs":[],"source":["# Importa as bibliotecas principais\n","import gymnasium as gym               # Biblioteca de simulações de ambientes para RL\n","import imageio                        # Usada para salvar a sequência de frames como GIF\n","from IPython.display import Image     # Para exibir a imagem (GIF) diretamente no notebook\n","import numpy as np                    # Importa o pacote NumPy, amplamente utilizado para manipulação de arrays e operações numéricas\n","from numpy import linalg as LA        # Rotinas de álgebra linear do NumPy (ex.: normas, autovalores, decomposições)\n","import matplotlib.pyplot as plt       # Biblioteca para criação de gráficos estáticos em Python (parte do matplotlib)\n","import seaborn as sns                 # Biblioteca baseada em matplotlib para gráficos estatísticos com visualização mais bonita (usada aqui para heatmaps)\n","from typing import Dict, Tuple, Optional, List  # Importa ferramentas de tipagem estática do Python"]},{"cell_type":"markdown","metadata":{"id":"uckJF0ntXK8z"},"source":["## Ambiente: nova instância  do FrozenLake"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TgPsVmo_XK80"},"outputs":[],"source":["map_name = '4x4'        # tamanho do mapa (pode ser '4x4' ou '8x8')\n","render_mode=\"rgb_array\" # render_mode=\"rgb_array\": retorna imagen_estados do ambiente como arrays de pixels\n","is_slippery=False       # is_slippery=True: torna o ambiente estocástico (com escorregões)\n","\n","env = gym.make(\"FrozenLake-v1\",\n","               map_name=map_name,\n","               render_mode=render_mode,\n","               is_slippery=is_slippery)\n","env = env.unwrapped     # ESSENCIAL para acessar env.P\n","\n","################################################################################\n","# Estrutura de env.P\n","################################################################################\n","# env.P: Dict[int, Dict[int, List[Tuple[float, int, float, bool]]]]\n","# env.P = {\n","#     estado_0: {\n","#         acao_0: [(p, s', r, done), ...],\n","#         acao_1: [(p, s', r, done), ...],\n","#         ...\n","#     },\n","#     estado_1: {\n","#         acao_0: [(p, s', r, done), ...],\n","#         ...\n","#     },\n","#     ...\n","# }\n","# (p, s', r, done) = (probabilidade, proximo_estado, recompensa, finalizado)\n","# probabilidade = p(s',r|s,a)\n","################################################################################"]},{"cell_type":"markdown","metadata":{"id":"cZzYmyGzXK80"},"source":["## Funções auxiliares para visualização"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yc1S2u3YXK80"},"outputs":[],"source":["def visualizar_politica(\n","    Pi: np.ndarray,\n","    env,\n","    *,\n","    action_labels: Optional[List[str]] = None,   # default FrozenLake: [\"←\",\"↓\",\"→\",\"↑\"]\n","    destacar_gulosa: bool = True,\n","    suptitle: Optional[str] = \"Política (distribuições por estado)\",\n",") -> None:\n","    # Inferir grid do Gymnasium (FrozenLake)\n","    if not (hasattr(env, \"unwrapped\") and hasattr(env.unwrapped, \"desc\")):\n","        raise ValueError(\"Passe um ambiente Gymnasium com 'env.unwrapped.desc' (ex.: FrozenLake-v1).\")\n","    desc = env.unwrapped.desc\n","    desc_str = np.char.decode(desc, \"utf-8\") if getattr(desc.dtype, \"kind\", \"\") == \"S\" else desc.astype(str)\n","\n","    n_rows, n_cols = desc_str.shape\n","    n_estados, n_acoes = Pi.shape\n","    if n_rows * n_cols != n_estados:\n","        raise ValueError(f\"Incompatibilidade: grid {n_rows}x{n_cols} != n_estados={n_estados}.\")\n","\n","    # Máscaras de terminais\n","    holes = (desc_str == \"H\")\n","    goal  = (desc_str == \"G\")\n","\n","    # Rótulos das ações (FrozenLake: LEFT, DOWN, RIGHT, UP)\n","    if action_labels is None:\n","        action_labels = [\"←\", \"↓\", \"→\", \"↑\"]\n","    if len(action_labels) != n_acoes:\n","        action_labels = [f\"a{i}\" for i in range(n_acoes)]\n","\n","    # Figura\n","    fig, axs = plt.subplots(n_rows, n_cols, figsize=(3.0 * n_cols, 2.2 * n_rows))\n","    axs = np.array(axs).reshape(-1)\n","\n","    for s in range(n_estados):\n","        r, c = divmod(s, n_cols)\n","        ax = axs[s]\n","\n","        # Estados terminais: só fundo colorido, sem barras\n","        if holes[r, c] or goal[r, c]:\n","            if holes[r, c]:\n","                ax.set_facecolor((1.0, 0.0, 0.0, 0.15))  # vermelho translúcido\n","                ax.set_title(f\"Estado {s} (H)\")\n","            else:\n","                ax.set_facecolor((0.0, 1.0, 0.0, 0.15))  # verde translúcido\n","                ax.set_title(f\"Estado {s} (G)\")\n","            # esconder eixos e ticks\n","            ax.set_xticks([]); ax.set_yticks([])\n","            for spine in ax.spines.values():\n","                spine.set_visible(True)\n","            continue\n","\n","        # Estados não-terminais: barras\n","        pi = Pi[s].astype(float)\n","        tot = pi.sum()\n","        if tot > 0:\n","            pi /= tot  # normalização defensiva\n","\n","        acoes = np.arange(n_acoes)\n","        colors = [\"gray\"] * n_acoes\n","        if destacar_gulosa:\n","            colors[int(np.argmax(pi))] = \"dimgray\"\n","\n","        ax.bar(acoes, pi, color=colors)\n","        ax.set_ylim(0, 1.05)\n","        ax.set_xticks(acoes)\n","        ax.set_xticklabels(action_labels)\n","        ax.set_yticks([0, 0.5, 1.0])\n","        ax.set_title(f\"Estado {s}\")\n","\n","    if suptitle:\n","        fig.suptitle(suptitle, y=1.02, fontsize=12)\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","def plot_tabular(\n","    data,\n","    kind: str = \"Q\",          # \"Q\" (valores de ação), \"Pi\" (política), \"V\" (valores de estado)\n","    ambiente=None,            # necessário quando kind=\"V\" para reshape\n","    ax=None,\n","    cbar: bool = True,\n","    fmt: str = \".1f\",\n","    center_zero: bool = True  # só relevante para \"Q\" e \"V\"\n","):\n","    \"\"\"\n","    Plota matrizes tabulares de RL em formato de heatmaps (mapas de calor).\n","    Esta função cobre 3 casos:\n","    1. kind=\"Q\": heatmap de Q(s, a) com ações nas linhas e estados nas colunas.\n","    2. kind=\"Pi\": heatmap de Pi(a|s) (probabilidades) com ações nas linhas e estados nas colunas.\n","    3. kind=\"V\": heatmap de V(s) no grid (n_rows x n_cols) do ambiente .\n","\n","    Parameters\n","    ----------\n","    data : ndarray\n","        Dados a serem plotados.\n","        - Para kind=\"Q\" ou \"Pi\": array 2D com shape (n_estados, n_acoes).\n","        - Para kind=\"V\": array 1D com shape (n_estados,) que será remodelado para (ambiente.n_rows, ambiente.n_cols).\n","    kind : {\"Q\", \"Pi\", \"V\"}, default=\"Q\"\n","        Tipo do plot:\n","        - \"Q\" usa paleta divergente centrada em zero.\n","        - \"Pi\" usa paleta sequencial no intervalo [0, 1].\n","        - \"V\" plota o valor de estado no grid do ambiente.\n","    ambiente : object, optional\n","        Necessário quando kind=\"V\". Deve expor n_rows e n_cols para o reshape.\n","    ax : matplotlib.axes.Axes, optional\n","        Eixo onde o heatmap será desenhado. Se None, uma nova figura/eixo é criado.\n","    cbar : bool, default=True\n","        Se True, exibe a barra de cores (colorbar).\n","    fmt : str, default=\".1f\"\n","        Formatação dos valores anotados em cada célula do heatmap.\n","    center_zero : bool, default=True\n","        Quando kind é \"Q\" ou \"V\", centraliza a escala de cores em zero (vmin=-absmax, vmax=absmax). Ignorado para \"Pi\".\n","\n","    Returns\n","    -------\n","    ax : matplotlib.axes.Axes\n","        Eixo contendo o heatmap resultante.\n","    \"\"\"\n","    kind = kind.upper()\n","\n","    xlabel = {\"V\": \"Colunas\", \"PI\": \"Estados\", \"Q\": \"Estados\"}\n","    ylabel = {\"V\": \"Linhas\", \"PI\": \"Ações\", \"Q\": \"Ações\" }\n","    title  = {\"V\": \"Valores de Estado (V(s))\", \"PI\": r\"Política ($\\pi(a|s)$ transposta)\", \"Q\": \"Valores de ação (Q(s, a) transposta)\"}\n","\n","    fig = None\n","\n","    #  V(s): precisa do shape do grid\n","    match kind:\n","        case \"V\":\n","\n","            if ambiente is None:\n","                raise ValueError(\"Para kind='V', passe 'ambiente' para reshape (n_rows, n_cols).\")\n","\n","            if hasattr(ambiente, \"n_rows\") and hasattr(ambiente, \"n_cols\"):\n","                n_rows, n_cols = ambiente.n_rows, ambiente.n_cols\n","            elif hasattr(ambiente, \"unwrapped\") and hasattr(ambiente.unwrapped, \"desc\"):\n","                # ex.: FrozenLake-v1 (Gymnasium)\n","                n_rows, n_cols = ambiente.unwrapped.desc.shape\n","            else:\n","                raise ValueError(\n","                    \"Passe um objeto com n_rows/n_cols ou um env Gymnasium com .unwrapped.desc.\"\n","                )\n","\n","            M = data.reshape(n_rows, n_cols)\n","\n","            if ax is None:\n","                fig, ax = plt.subplots(figsize=(n_cols, n_rows))\n","\n","            if center_zero:\n","                vmax = float(np.abs(M).max())\n","                vmin = -vmax\n","            else:\n","                vmin = float(M.min())\n","                vmax = float(M.max())\n","\n","            cmap, square = \"bwr\", True\n","\n","        case \"PI\" | \"Q\":\n","\n","            # Q(s,a) e Pi(a|s): ações nas linhas, estados nas colunas\n","            M = data.T  # data: (n_estados, n_acoes) -> transposto para (n_acoes, n_estados)\n","            n_acoes, n_estados = M.shape\n","\n","            if ax is None:\n","                fig, ax = plt.subplots(figsize=(n_estados, n_acoes))\n","\n","            if kind == \"PI\":\n","                cmap = \"Blues\";\n","                vmin, vmax = 0.0, 1.0\n","            else:  # \"Q\"\n","                cmap = \"bwr\"\n","                if center_zero:\n","                    vmax = float(np.abs(M).max())\n","                    vmin = -vmax\n","                else:\n","                    vmin = float(M.min())\n","                    vmax = float(M.max())\n","\n","            square = False\n","\n","        case _:\n","            raise ValueError(f\"kind desconhecido: {kind!r} (use 'Q', 'Pi' ou 'V').\")\n","\n","\n","    ax = sns.heatmap(\n","        data=M,\n","        annot=True,\n","        fmt=fmt,\n","        cmap=cmap,\n","        vmin=vmin,\n","        vmax=vmax,\n","        cbar=cbar,\n","        square=square,\n","        linewidths=0.5,\n","        linecolor=\"gray\",\n","        ax=ax\n","    )\n","\n","    ax.set_xlabel(xlabel[kind])\n","    ax.set_ylabel(ylabel[kind])\n","    ax.set_title(title[kind])\n","\n","    # bordas externas\n","    for side in (\"left\", \"right\", \"top\", \"bottom\"):\n","        ax.spines[side].set_visible(True)\n","        ax.spines[side].set_linewidth(0.5)\n","        ax.spines[side].set_edgecolor(\"gray\")\n","\n","    # rótulos\n","    if kind in (\"Q\", \"PI\"):\n","        ax.set_xticks(np.arange(n_estados) + 0.5)\n","        ax.set_xticklabels([f\"s{i}\" for i in range(n_estados)], rotation=0)\n","        ax.set_yticks(np.arange(n_acoes) + 0.5)\n","        ax.set_yticklabels([f\"a{i}\" for i in range(n_acoes)], rotation=0)\n","\n","    if fig is not None:\n","        plt.tight_layout()\n","        plt.show()\n","\n","    return"]},{"cell_type":"markdown","metadata":{"id":"ncWDixZ6XK81"},"source":["## Algoritmo: Iteração de política"]},{"cell_type":"markdown","metadata":{"id":"iKzZGMreXK81"},"source":["### Avaliar política"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cPpOKQMeXK81"},"outputs":[],"source":["def avaliar_politica_truncada(\n","    env,\n","    Pi: np.ndarray,\n","    j_truncado: int,\n","    V: np.ndarray | None = None,\n","    gamma: float = 0.9,\n",") -> np.ndarray:\n","    \"\"\"\n","    Avaliação de política truncada.\n","\n","    Executa exatamente j_truncado varreduras do algoritmo de avaliação de política (Jacobi).\n","\n","    Atualização:\n","        V_{new}(s) = sum_a Pi[a|s] * sum_{(p,s',r,done) in env.P[s][a]} p * [ r + gamma * V_old[s'] ]\n","\n","    Parâmetros\n","    ----------\n","    env : gym.Env (unwrapped)\n","        Ambiente com dicionário de transições env.P: Dict[s][a] -> List[(p, s', r, done)].\n","    Pi : np.ndarray, shape (n_estados, n_acoes)\n","        Política.\n","    j_truncado : int\n","        Número de iterações (varreduras de avaliação).\n","    V : np.ndarray | None, shape (n_estados,), opcional\n","        Valores de estado iniciais. Se None, começa em zeros.\n","    gamma : float, default=0.9\n","        Fator de desconto.\n","\n","    Retorna\n","    -------\n","    V : np.ndarray, shape (n_estados,)\n","        Valores de estado após j_truncado varreduras.\n","    \"\"\"\n","\n","    n_estados = env.observation_space.n\n","    n_acoes   = env.action_space.n\n","\n","    # Inicializações\n","    if V is None:\n","        V = np.zeros(n_estados, dtype=float)\n","\n","    ############################################################################################################\n","    # AVALIAÇÃO DA POLÍTICA ATUAL\n","    ############################################################################################################\n","    # Código aqui\n","\n","    ############################################################################################################\n","\n","    return V\n"]},{"cell_type":"markdown","metadata":{"id":"CX7YJUT-XK82"},"source":["### Melhorar política"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qf_3OcrFXK82"},"outputs":[],"source":["def melhorar_politica(\n","    env,\n","    V: np.ndarray,\n","    gamma: float = 0.9,\n",") -> Tuple[np.ndarray, np.ndarray]:\n","    \"\"\"\n","    Melhoria de política.\n","\n","    Dado V, calcula:\n","        Q(s,a) = sum_{(p,s',r,done) in P[s][a]} p * (r + gamma * V[s'])\n","    e retorna a política gulosa determinística.\n","\n","    Parâmetros\n","    ----------\n","    env : gym.Env (unwrapped)\n","        Ambiente com dicionário de transições env.P: Dict[s][a] -> List[(p, s', r, done)].\n","    V : np.ndarray, shape (n_estados,)\n","        Valores de estado.\n","    gamma : float, default=0.99\n","        Fator de desconto (0 <= gamma < 1).\n","\n","    Retorna\n","    -------\n","    Q : np.ndarray, shape (n_estados, n_acoes)\n","        Valores de ação.\n","    Pi_new : np.ndarray, shape (n_estados, n_acoes)\n","        Política gulosa determinística.\n","    \"\"\"\n","    # Dimensões e validações\n","\n","    n_estados = env.observation_space.n\n","    n_acoes   = env.action_space.n\n","\n","    # Inicializações\n","    Q      = np.zeros((n_estados, n_acoes), dtype=float)\n","    Pi_new = np.zeros((n_estados, n_acoes), dtype=float)\n","\n","    ############################################################################################################\n","    # MELHORIA DA POLÍTICA\n","    ############################################################################################################\n","    # Código aqui\n","\n","    ############################################################################################################\n","\n","    return Q, Pi_new"]},{"cell_type":"markdown","metadata":{"id":"IJPj11kAXK82"},"source":["### Algoritmo: iteração de política truncada"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BvaiYe-RXK82"},"outputs":[],"source":["def iteracao_de_politica_truncada(\n","    env,\n","    gamma: float = 0.99,\n","    j_truncado: int = 10,\n","    theta: float = 1e-8,\n",") -> Tuple[np.ndarray, np.ndarray, np.ndarray, int]:\n","    \"\"\"\n","    Iteração de Política truncada.\n","\n","    Loop externo (k):\n","      1) V <- avaliar_politica_truncada(env, Pi_k, j_truncado, V, gamma)\n","      2) (Q, Pi_{k+1}) <- melhorar_politica(env, V, gamma)\n","      3) parar quando ||V - V_prev||_inf < theta\n","\n","    Parâmetros\n","    ----------\n","    env : gym.Env (unwrapped)\n","        Ambiente com dicionário de transições env.P: Dict[s][a] -> List[(p, s', r, done)].\n","    gamma : float, default=0.9\n","        Fator de desconto (0 <= gamma < 1).\n","    j_truncado : int, default=10\n","        Número de varreduras na avaliação de política truncada.\n","    theta : float, default=1e-8\n","        Critério de parada baseado em V entre iterações externas (convergência).\n","\n","    Retorna\n","    -------\n","    V  : np.ndarray, shape (n_estados,)\n","        Valores de estado.\n","    Q  : np.ndarray, shape (n_estados, n_acoes)\n","        Valores de ação da última melhoria.\n","    Pi : np.ndarray, shape (n_estados, n_acoes)\n","        Política determinística.\n","    k  : int\n","        Número de iterações externas executadas.\n","    \"\"\"\n","    # Dimensões e validações básicas\n","    n_estados = env.observation_space.n\n","    n_acoes   = env.action_space.n\n","\n","    # Inicializações\n","    V  = np.zeros(n_estados, dtype=float)\n","    Q  = np.zeros((n_estados, n_acoes), dtype=float)\n","    Pi = np.full((n_estados, n_acoes), 1.0 / n_acoes, dtype=float)  # política uniforme\n","\n","    # Laço externo\n","    k = 0\n","    while True:\n","\n","        k += 1\n","\n","        V_prev = V.copy()\n","\n","        # 1) Avaliação de política (truncada)\n","        V = avaliar_politica_truncada(\n","            env=env,\n","            Pi=Pi,\n","            j_truncado=j_truncado,\n","            V=V,\n","            gamma=gamma,\n","        )\n","\n","        # 2) Melhoria de política (gulosa)\n","        Q, Pi_new = melhorar_politica(\n","            env=env,\n","            V=V,\n","            gamma=gamma,\n","        )\n","        Pi = Pi_new\n","\n","        # 3) Critério de parada baseado apenas em V\n","        if np.linalg.norm(V - V_prev, ord=np.inf) < theta:\n","            break\n","\n","    return V, Q, Pi, k\n"]},{"cell_type":"markdown","metadata":{"id":"zlSmbozrXK83"},"source":["## Experimento"]},{"cell_type":"markdown","metadata":{"id":"6hDB8mS6XK83"},"source":["### Simulação"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CwV8tBNfceSh"},"outputs":[],"source":["V, Q, Pi, k = iteracao_de_politica_truncada(\n","    env,                 # ambiente FrozenLake-v1 (unwrapped)\n","    gamma=0.95,          # fator de desconto\n","    j_truncado=100,      # número de varreduras por iteração externa (avaliação truncada)\n","    theta=1e-8           # critério de parada externo: ||V - V_prev||_inf < theta\n",")\n","\n","# V  -> (n_estados,)         valores de estado após convergência do laço externo\n","# Q  -> (n_estados,n_acoes)  valores de ação da última melhoria de política\n","# Pi -> (n_estados,n_acoes)  política determinística final\n","# k  -> int                  número de iterações externas executadas"]},{"cell_type":"markdown","metadata":{"id":"ibN6frzZXK83"},"source":["### Visualização"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJLb-35ygg-2"},"outputs":[],"source":["# Q: ndarray (n_estados, n_acoes)\n","plot_tabular(Q, kind=\"Q\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1DO60LcXXK83"},"outputs":[],"source":["# Pi: ndarray (n_estados, n_acoes)\n","plot_tabular(Pi, kind=\"Pi\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9e-q5O9eXK83"},"outputs":[],"source":["# V: ndarray (n_estados,)\n","plot_tabular(V, kind=\"V\", ambiente=env, center_zero=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_C3QA0LXK83"},"outputs":[],"source":["# Política (setas) sobre o ambiente\n","visualizar_politica(Pi, env=env)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJR2eCWZEuZe"},"outputs":[],"source":["env = gym.make(\"FrozenLake-v1\", map_name=map_name, render_mode=render_mode, is_slippery=is_slippery)    # Cria o ambiente FrozenLake\n","frames = []                                                                                             # Lista que armazenará todos os frames (imagen_estados) do episódio\n","n_episodios = 5                                                                                         # Número de episódios\n","for ep in range(n_episodios):\n","  observation, info = env.reset()                                                                       # Reinicia o ambiente e obtém o primeiro estado (observation)\n","  for _ in range(100):                                                                                  # Executa um episódio de até 100 passos\n","      action = int(np.argmax(Pi[observation]))                                                          # Seleciona a ação da política ótima\n","      observation, reward, terminated, truncated, info = env.step(action)                               # Aplica a ação no ambiente\n","      frames.append(env.render())                                                                       # Captura a imagem do ambiente após a ação\n","      if terminated or truncated:                                                                       # Verifica se o episódio acabou (chegou no objetivo ou caiu no buraco)\n","          break\n","env.close()                                                                                             # Encerra o ambiente corretamente"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TO2p215TEuhP"},"outputs":[],"source":["# Salva os frames coletados como um arquivo GIF animado\n","gif_path=\"frozenlake.gif\"\n","imageio.mimsave(gif_path, frames, format=\"GIF\", fps=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wxGGJ8zExzH"},"outputs":[],"source":["# Exibe o GIF diretamente no notebook\n","Image(filename=gif_path)"]},{"cell_type":"markdown","metadata":{"id":"LxYI7Z8SXK84"},"source":["# Tarefa\n","\n","1. Implemente o algoritmo **iteração de política truncada**.\n","2. Gere um **gráfico de dispersão** em que cada ponto (x,y) corresponde à (valor do j_truncado, iteração em que a condição de convergência foi satisfeita para este j_truncado).\n","\n","** Utilize a seguinte configuração do ambiente FrozenLake para os experimentos**\n","\n","- `map_name = '8x8'` e `map_name = '4x4'`      \n","- `render_mode=\"rgb_array\"`\n","- `is_slippery=True`\n","\n","**No experimento com configuração `map_name = '4x4'` mostrar:**\n","\n","1. **Figuras**:\n","   - heatmap de $V(s)$ (função `plot_tabular`);\n","   - heatmap de $Q(s,a)$ (função `plot_tabular`);\n","   - heatmap de $\\pi(a\\mid s)$ (função `plot_tabular`);\n","   - gráficos de barras de $\\pi(a\\mid s)$ (função `visualizar_politica`).\n","   - gráfico de dispersão\n","\n","**No experimento com configuração `map_name = '8x8'` mostrar:**\n","\n","1. **Figura**:\n","   - gráfico de dispersão\n","\n","**Entregáveis:**\n","\n","2. **Código** (notebook `.ipynb`)\n","1. **Relatório** (`.pdf`).\n","- O PDF deve conter:\n","  - **Setup** (parâmetros usados).\n","  - **Resultados** (figuras e tabelas organizadas por experimento).\n","  - **Análises curtas** por experimento.\n","- O PDF **NÃO** deve conter:\n","    - Códigos."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"ai","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.5"}},"nbformat":4,"nbformat_minor":0}